\documentclass[12pt]{article}
\usepackage{myVignette}
\usepackage[authoryear,round]{natbib}
\bibliographystyle{plainnat}
%%\VignetteIndexEntry{Sparse matrix representations of linear mixed models}
%%\VignetteDepends{Matrix}
%%\VignetteDepends{mlmrev}
\begin{document}
\SweaveOpts{engine=R,eps=FALSE,pdf=TRUE,width=5,height=3,strip.white=TRUE}
\setkeys{Gin}{width=\textwidth}
\title{Spare Matrix Representations of Linear Mixed Models}
\author{Douglas Bates\\R Development Core Team\\\email{Douglas.Bates@R-project.org}}
\date{\today}
\maketitle
\begin{abstract}
  We describe a representation of linear mixed-effects models using
  positive semidefinite, symmetric, compressed, column-oriented,
  sparse matrices.  This representation provides for efficient
  evaluation of the profiled log-likelihood or profiled restricted
  log-likelihood of the model, given the relative precision parameters
  of the random effects.  The evaluation is based upon the Cholesky
  decomposition of the augmented sparse representation.  Additionally,
  we can use information from this representation to evaluate ECME
  updates and the gradient and Hessian of the objective criterion.
  
  The ordering of the columns (and, correspondingly, the rows) of a
  positive semidefinite, symmetric sparse matrix can have a
  substantial effect on the amount of fill-in generated by the
  Cholesky decomposition.  For the particular matrices considered here
  the ordering will only become important when random effects are
  associated with more that one grouping factor and the grouping
  factors are neither nested nor fully crossed.  We say that such
  factors are partially crossed, a situation that is very common in
  observational data.

  Several methods for determining favorable orderings have been
  proposed but these generally reorder all the columns.  In our case
  the columns are grouped.  We show that we can reorder the columns
  while preserving the grouping and still attain acceptable levels of
  fill-in. 
\end{abstract}
<<preliminaries, echo=FALSE>>=
options(width=75)
@

\section{Introduction}
\label{sec:Intro}

This report is directed at two audiences: sparse matrix researchers
and mixed-effects model researchers.  To make the ideas accessible to
both audiences I will need to introduce a formulation of mixed models
for the sparse matrix researchers and also to introduce sparse matrix
representation for the mixed model researchers.

Linear mixed-effects models are described in detail in
\citet{pinh:bate:2000}.  In chapter 3 of that book we provide details
of computational methods suitable for mixed models with a single level
of random effects or with multiple, nested levels of random effects.
(Detailed descriptions of terms like ``levels'' of ``random effects''
and ``nesting'' are given later.)  Those methods were based on
orthogonal-triangular (also called QR) decompositions.  Later, in
\citet{bate:debr:2004} we generalized these computational methods and
showed that all of the important results can be calculated from the
Cholesky decomposition of a large, sparse, positive definite,
symmetric matrix.  We also provide additional computational results for
the specific cases considered in \citet[ch.~3]{pinh:bate:2000}.  

In this report I formulate a general approach to linear mixed model
calculations using sparse matrix techniques.  For an important class
of mixed model problems, those with partially crossed grouping
factors, the sparse matrix methods will be competitive with, and
probably quite superior to, any existing methods.  I believe that even
for the problems with simpler structure, either a single grouping
factor or nested grouping factors, an efficient implementation of the
sparse-matrix-based methods will be competitive with the best
current methods.  

Opportunities for efficiency exist because we must repeatedly perform
Cholesky decompositions of matrices with the same pattern of non-zero
entries (and somewhat different values of those non-zero entries) and
because the non-zero entries occur in dense blocks.  Many methods for
sparse matrices have both a symbolic phase, where the pattern of the
non-zero entries in the result is determined, and a numeric phase,
where the actual results are determined.  For decompositions part of
the symbolic phase is determination of permutation of the rows and
columns that reduces fill-in for the decomposition.  In our problem
the symbolic phase need only be done once and it can be performed
based the positions of the blocks.  The matrix giving the positions of
the blocks is frequently much smaller and easier to manipulate than
the matrix that is to be decomposed.  Furthermore, the blocks provide
a natural way of using multifrontal techniques, that is, using dense
matrix building blocks in a sparse matrix calculation, for this
problem.

In the next section I introduce a general form of linear mixed-models
and the notation I will use.  I also introduce three sample data sets
and models.  These include a very simple example that can be used to
illustrate details of the calculations, a moderate-sized example that has
been used to illustrate other methods, and a very large example that
can show the savings available with sparse matrix methods.  In section
In \S\ref{sec:Examples} I introduce sparse matrix storage schemes and
computational methods and show how they can be applied to the examples.
Throughout we illustrate these methods using
the \code{Matrix} package for R~\citep{R-1.9.0}, which provides an
implementation of sparse matrix methods for linear mixed-effects
models.  This R package is based on code from the LDL, TAUCS, Metis,
and UMFPACK packages.


\section{Linear mixed models}
\label{sec:LinearMixed}

As described in \citet{bate:debr:2004} a linear mixed-effects model
can be written as
\begin{equation}
  \label{eq:lmeGeneral}
  \by=\bX\bbeta+\bZ\bb+\beps\quad
  \beps\sim\mathcal{N}(\bzer,\sigma^2\bI),
  \bb\sim\mathcal{N}(\bzer,\sigma^2\bOmega^{-1}),
  \beps\perp\bb
\end{equation}
where $\by$ is the $n$-dimensional response vector, $\bX$ is an
$n\times p$ model matrix for the $p$-dimensional fixed-effects vector
$\bbeta$, $\bZ$ is the $n\times q$ model matrix for the
$q$-dimensional random-effects vector $\bb$ that has a Gaussian
distribution with mean $\bzer$ and relative precision matrix $\bOmega$
(i.e., $\bOmega$ is the precision of $\bb$ relative to the precision
of $\beps$), and $\beps$ is the random noise assumed to have a
spherical Gaussian distribution.  The symbol $\perp$ indicates
independence of random variables.  We assume that $\bX$ has full
column rank and that $\bOmega$ is positive definite.  Furthermore
$\bOmega$ is a function of an (unconstrained) parameter vector $\btheta$.

Generally $p$, the dimension of $\bbeta$, is moderate but $q$, the
dimension of $\bb$ can be huge.  Using the response and the model
matrices, $\by$, $\bX$ and $\bZ$, which are evaluated from the
observed data, we determine the estimates of the model parameters;
$\bbeta$, $\btheta$, and $\sigma^2$, as those values that optimize the
likelihood function or, more commonly, a variant called the restricted
likelihood.  Both the likelihood and the restricted likelihood must be
positive and their logarithms, called the log-likelihood and
the log-restricted-likelihood, are generally easier to evaluate and
provide a better quadratic approximation for optimization than the
original functions, so we work on the log-likelihood scale.

The dimension of $\btheta$ is typically very small.  Even in complex,
``real-world'' models applied to data sets with millions of
observations the dimension of $\btheta$ can be as small as two or
three.  Conditional on a value of $\btheta$, the optimal values of
$\bbeta$ and $\sigma^2$ can be determined from a penalized least
squares problem based on $\by$, $\bX$, $\bZ$, and $\bOmega$
\begin{equation}
  \label{eq:bPhi}
  \min_{\bb,\bbeta}\left\|\tilde{\by}-\bPhi(\btheta)
    \begin{bmatrix}
      \bb\\\bbeta
    \end{bmatrix}\right\|^2\text{ where }
  \bPhi(\btheta)=
  \begin{bmatrix}
    \bZ              & \bX \\
    \bDelta(\btheta) & \bzer 
  \end{bmatrix},
  \tilde{\by}=
  \begin{bmatrix}
    \by \\ \bzer
  \end{bmatrix},
\end{equation}
and $\bDelta(\btheta)\trans\bDelta(\btheta)=\bOmega$.
One way to solve problem~(\ref{eq:bPhi}) is form
$\bL_e$ and $\bD_e$, the
LDL form of the Cholesky decomposition,
\begin{equation}
  \label{eq:CrossProdGen}
  \begin{bmatrix}
    \bZ\trans\bZ+\bOmega & \bZ\trans\bX  & \bZ\trans\by \\
    \bX\trans\bZ         & \bX\trans\bX  & \bX\trans\by \\
    \by\trans\bZ         & \by\trans\bX  & \by\trans\by
  \end{bmatrix}=
  \bL\bD\bL\trans\text{ where }
  \bL=
  \begin{bmatrix}
    \LZZ & \bzer & \bzer \\
    \LXZ & \LXX  & \bzer \\
    \lyZ & \lyZ  & 1
  \end{bmatrix}
\end{equation}
and $\bD$ is diagonal with non-negative diagonal elements.  The matrix
$\bL$ is unit lower triangular (i.e.{} it is lower triangular and its
diagonal elements are all unity).

We divide the diagonal of $\bD$ into three components; $\dZ$ of length
$q$, $\dX$ of length $p$, and the scalar $\dy$.  Our assumptions that
$\bOmega$ be positive definite and that $\bX$ have full column rank ensure
that the elements of $\dZ$ and $\dX$ are positive.  Because the only
cases where $\dy=0$ are trivial cases with exact fits to the response,
we will assume that $\dy>0$.

In \citet{bate:debr:2004} we wrote the Cholesky decomposition
(\ref{eq:CrossProdGen}) as $\bR\trans\bR$ where $\bR$ is upper
triangular and, from that, derived expressions for the profiled
log-likelihood $\tilde{\ell}(\btheta)$ and the profiled
log-restricted-likelihood, $\tilde{\ell}_R(\btheta)$.  These
``profiled'' functions are
functions of $\btheta$ only.  They are the values of the corresponding
objective function at the conditionally optimal values of the other
parameters, $\bbeta$ and $\sigma^2$.  Translating from the
representation in \citet{bate:debr:2004} to the
representation used here provides
\begin{align}
  \label{eq:profiledLogLik}
  -2\tilde{\ell}(\btheta)&
  = \log\left(\frac{\left|\bZ\trans\bZ+\bOmega\right|}
    {\left|\bOmega\right|}\right)
  + n\left[1+\log\left(\frac{2\pi\dy}{n}\right)\right]\\
  -2\tilde{\ell}_R(\btheta)&
  = \log\left(\frac{\left|\bZ\trans\bZ+\bOmega\right|\left|\LXX\right|^2}
    {\left|\bOmega\right|}\right)
  + (n-p)\left[1+\log\left(\frac{2\pi\dy}{n-p}\right)\right]
\end{align}

As discussed later, $\bOmega$ has a block diagonal structure and its
determinant is easily evaluated.  The determinant
$\left|\bZ\trans\bZ+\bOmega\right|$ is the product of the elements of
$\dZ$ and $\left|\LXX\right|^2$ is the product of the elements of $\dX$.

The other results given in \citet{bate:debr:2004} can be calculated
from $\bL$ and $\bD$.  To make all this feasible the structure and, in
particular, the sparsity of $\bZ$ and $\bOmega$ must be exploited.

Sparsity in $\bZ$ (and $\bOmega$) occurs when the random effects
vector $\bb$ is divided into small components associated with one or
more factors that group the observations.  It is easiest to illustrate
this with some examples.


\section{Examples of mixed-effects models}
\label{sec:examples}


\subsection{A simple variance components model}
\label{sec:varianceComponents}

In \citet[\S1.1]{pinh:bate:2000} we discuss measurements of the travel
time of a certain type of ultrasonic wave in six different
railway rails.  Each rail was tested three times yielding a total of
18 observations.  Each observation denotes the rail and the observed
travel time. A simple data plot (e.g.{} Fig. 1.1 in
\cite{pinh:bate:2000}) shows that the variation between responses on
different rails is much greater than the variation between responses on
the same rail.  We model this as
\begin{equation}
  \label{eq:varianceComp}
  y_{ij}=\mu+b_i+\epsilon_{ij}\quad
  i=1,\dots,6,\;j=1,\dots,3\quad
  b_i\sim\mathcal{N}(0,\sigma^2_b),\;\epsilon_{ij}\sim\mathcal{N}(0,\sigma^2)
\end{equation}
where $\sigma^2$ is the within-rail variance and $\sigma^2_b$ is the
between-rail variance.  These are called the \emph{variance components}.

The random variable $b_i$ is the deviation of the mean travel time for
rail $i$ from the overall mean travel time $\mu$.  These are called
the \emph{random effects} associated with the rails.

We can express model (\ref{eq:varianceComp}) in the form
(\ref{eq:lmeGeneral}) by setting
\begin{equation}
  \label{eq:varianceCompMod}
  \begin{aligned}
    \bb &=\left(b_1,b_2,\dots,b_6\right)\trans\\
    \bbeta &= \mu
    \bX\trans&=\left[1,1,\dots,1\right]\trans
  \end{aligned}
\end{equation}
and $\bZ$ to be the $18\times 6$ matrix of indicators of the rail.
The matrix 
\begin{equation}
  \label{eq:relativePrecision}
  \bOmega=\frac{\sigma^2}{\sigma_a^2}\bI_6
\end{equation}
where $\bI$ is the $6\times 6$ identity matrix.  The multiple
$\frac{\sigma^2}{\sigma_a^2}$ is the relative precision of the random
effects and the parameter $\theta$ is a scalar that determines this
multiple.  To obtain an unconstrained $\theta$ we could use the
logarithm of the ratio
\begin{equation}
  \label{eq:thetaDef}
  \theta = \log\left(\sigma^2\right)-\log\left(\sigma_a^2\right)
\end{equation}

The first few rows of $\bZ$, $\bX$, and $\by$ are
<<ZXy>>=
data(Rail, package = "nlme")
ZXy = cbind(model.matrix(~Rail-1,Rail), X = 1, y = Rail$travel)
ZXy[1:4,]
@ 
The matrix to be decomposed is obtained by adding $\bOmega$ to the
$6\times 6$ upper left submatrix of
<<crossprod>>=
crossprod(ZXy)
@ 
For example, if $e^\theta=0.1$ then the Cholesky decomposition is
<<chol1>>=
options(digits=5)
chol(crossprod(ZXy) + diag(c(rep(.1, 6), 0, 0)))
@ 

As seen in this example the cross-product matrix is sparse and only
only the diagonal and the last two rows need to be stored.  (It
happens that $\bZ\trans\bZ$ is a multiple of the identity, as is
$\bOmega$, and this could lead to further simplifications.  However,
this property depends on the fact that the data are balanced in the
sense that there are the same number of observations made on each
rail.  Although data from a designed experiment may be balanced,
observational data are almost never balanced so it is not worthwhile
trying to exploit this special structure.)

In general the trailing $p+1$ rows (and columns) of the cross-product
matrix will be dense.  The structure of the other 
summarized as


In \S\ref{sec:SparseM} we describe two of the popular sparse
matrix representations and formation of the Cholesky decomposition of
sparse, symmetric, positive semidefinite matrices.  The number of
non-zero entries in the Cholesky factor can depend on the ordering of
the columns (and, correspondingly, the rows) of the original matrix.
Various methods have been proposed to choose optimal or near-optimal
reorderings.  This is an example of symbolic analysis that can be used
before the numeric computation to reduce the amount of numeric
computation.  We describe others.


\section{Sparse matrix classes and methods in the Matrix package for R}
\label{sec:SparseM}

The simplest representation of a sparse matrix $\bX$ is to store a
triplet $(i,j,x_{ij})$ for each non-zero element.  If the triplets are
sorted, say by column order, the column indices will occur in blocks
of equal values.  In the \emph{compressed, sparse, column-oriented}
format the entries are sorted in increasing column order and a set of
pointers to the beginning of each column are used instead of the
column values themselves.  The \var{tripletMatrix} class in the
\var{Matrix} package provides the triplet format and the
\var{cscMatrix} class provides the compressed, sparse column-oriented
format.  In both these classes indices are 0-based (for compatibility
with the underlying C code) and not 1-based as is common in R.
<<Matintro>>=
library(Matrix)
mm = new("tripletMatrix", 
         i = as(c(0,2,3,1,2,0,3,4,3,4),"integer"),
         j = as(c(0,0,0,1,1,2,2,2,3,3),"integer"),
         x = (1:10)/10, Dim = as(c(5,4),"integer"))
m1 = as(mm,"cscMatrix")
str(m1)
as(m1, "matrix")
diff(m1@p)
@ 
We see that the \var{p} slot in a \var{cscMatrix} with 4 columns has 5
elements.  The first element is always zero and the successive differences
are the numbers of non-zero elements in each column.  The total number
of non-zero elements is the value of the last element of the \var{p}
slot.  This is also the length of the vector of row indices in the
\var{i} slot.

The validation method for the \var{cscMatrix} class ensures that the
row indices are increasing within columns and reorders the \var{i} and
\var{x} slots if necessary to achieve this.  Technically, the objects
in this class can be described as sorted, compressed, sparse,
column-oriented matrices.

Objects in the \var{tscMatrix} class represent triangular, sparse,
column-oriented matrices and those in the \var{sscMatrix} class
represent symmetric, sparse, column-oriented matrices.  Only the
upper triangle or the lower triangle, as indicated by \code{"U"} or
\code{"L"} in the \var{uplo} slot, of a symmetric matrix is stored.
The \var{crossprod} function applied to a \var{cscMatrix} produces an
\var{sscMatrix}.
<<crossprod>>=
class(m2 <- crossprod(m1))
as(m2, "matrix")
@

In Statistics we usually define the Cholesky decomposition of a
positive semidefinite, symmetric matrix $\bA$ as an upper triangular
matrix $\bR$ such that $\bA=\bR\trans\bR$ but it is also frequently
defined as a lower triangular matrix $\bL$ such that
$\bA=\bL\bL\trans$.  Naturally, $\bL$ and $\bR$ are transposes of each
other.  On occasion there are advantages to working with the left
factor $\bL$ instead of the right factor $\bR$.

For a sparse, symmetric, semidefinite matrix reordering the columns
(and, correspondingly, the rows) of $\bA$ can change the number of
non-zero elements in the Cholesky factor.  The number of elements in
the Cholesky factor is at least the number of non-zero elements in the
lower triangle of $\bA$.  Additional non-zeros can be generated during
the decomposition.  This process is called ``fill-in''.  Various
methods of determining a fill-minimizing order have been proposed.  We
use a graph-based method implemented in the Metis package.

The \var{chol} function generates the Cholesky decomposition.   When
applied to an \var{sscMatrix} object it defaults to generating a
fill-reducing permutation and the Cholesky factor of the permuted matrix.
<<chols>>=
m3 = chol(m2)
as(m3, "matrix")
m3@perm
@ 
If we set the optional argument \var{pivot} to \var{FALSE},
calculation of the fill-reducing permutation is suppressed.
<<chols2>>=
as(chol(m2, pivot = FALSE), "matrix")
@ 
In this example the fill-reducing permutation reverses the order of
the columns and rows of \var{m2} before taking the decomposition.  It
results in two fewer non-zero elements in the decomposition than when
we suppress the permutation.

\subsection{Symbolic versus numeric computation}
\label{sec:symbolic}

Calculation of the fill-reducing ordering is an example of a symbolic
computation on sparse matrices in that it is based only on the
positions of the non-zero elements, not upon their values.  Frequently
a sparse-matrix computation has both a symbolic phase, which typically
determines the number and positions of the non-zero entries in the
result, and a numeric phase that actually calculates these non-zero
elements.

Evaluation of the profiled log-likelihood or profiled
log-restricted-likelihood requires updating the diagonal blocks in
$\bZ\trans\bZ$ and taking the Cholesky decomposition of the resulting
matrix.  The symbolic phases, including calculation of a fill-reducing
ordering only need to be done once.

Recently Tim Davis has released the LDL package that provides a
concise Cholesky factorization of the form $\bA=\bL\bD\bL\trans$ where
$\bL$ is a unit lower triangular matrix (i.e. all the diagonal
elements are unity) and $\bD$ is diagonal and stored as a single
vector.  This representation is particularly convenient for us because
the diagonal elements (which must be non-zero when $\bA$ is positive
definite) often constitute a substantial portion of the total number
of non-zero elements in the Cholesky factor and, in this
representations, we do not encounter the extra indexing overhead when
accessing these elements.  Also the determinant of $\bA$ (or, for our
purposes, the determinants of leading diagonal submatrices of $\bA$)
can be calculated directly from the diagonal of $\bD$.

As shown in the examples in the next section we can determine
fill-reducing orderings and sizes of Cholesky factors of the matrices
that we wish to decompose by considering first the pairwise
cross-tabulations of the grouping factors.

\section{Some examples}
\label{sec:Examples}

Data on achievement scores of Scottish secondary school students is
described in \citet{Paterson:1991} and included as the data set
\var{ScotsSec} in the \var{mlmrev} package for R that provides the
data sets from the multilevel modelling software review.

<<ScotsSec>>=
data(ScotsSec, package = "Matrix")
dim(ScotsSec)
summary(ScotsSec)
@ 

These data contain the achievement scores (\var{attain}) of 3435
secondary school students in Scotland.  Along with demographic data
(sex and social class) and a verbal reasoning score based on tests
taken at entry to secondary school, the primary school and the
secondary school that the student attended are recorded.

There are 148 distinct primary schools and 19 distinct secondary
schools represented in these data and, in common models for such data
\citep{Rnews:Lockwood+Doran+Mccaffrey:2003}, there would be one or
more coefficients in $\bb$ associated with each school.  We say that
\var{primary} and \var{second} are \emph{grouping factors} for the
random effects.

When the random effects are grouped according to more than one
grouping factors, as here, it is important to determine if the
grouping factors are \emph{crossed} (every level of factor 1 occurs
with every level of factor 2) or nested (each level of factor 1 occurs
with only one level of factor 2) or partially crossed, which is how we
describe grouping factors that are neither (fully) crossed nor
strictly nested.

In this case the grouping factors \var{primary} and \var{second} are
partially crossed.  We can express this graphically through the image
of the cross-tabulation of the grouping factors.  We can generate such
a cross-tabulation as a symmetric, sparse, compressed column matrix
with the \var{sscCrosstab} function and use \var{image} to show the
non-zero elements graphically.  (Only the non-zero elements in the
lower triangle are shown.)
<<ctab>>=
ctab = sscCrosstab(ScotsSec[, c("primary", "second")])
str(ctab)
@ 

\bibliography{Matrix}
\end{document}
Yesterday I spoke with Deepayan Sarkar, a graduate student with whom I
work, on ways for structuring the calculations for linear
mixed-effects models using sparse matrix representations and Tim
Davis's LDL decomposition code.  I described to Deepayan a plan A and
a plan B.  On thinking about the calculations more yesterday evening I
formulated a plan AB, which is what I think I will use.

One purpose of writing this note is so I can get the steps clear in my
mind.

The critical part of the calculation is determining the Cholesky
decomposition of a matrix of the form

 Z'Z+W Z'X Z'y
  X'Z  X'X X'y   for different matrices W that depend upon parameters r
  y'Z  y'X y'y

Statistically the role of y is very different from X but, as far as
the calculation goes, I can work with the augmented matrix [X,y] as if
it were a single matrix, which, at the risk of some confusion, I will
henceforth call X.  We will not refer to y again.

Matrices Z and X are n by q and n by p respectively with n >= q.
Generally, q >> p, Z is sparse, W is block diagonal (and the blocks
are of small dimension) and X is dense.  The matrix Z is generated
from a set of k grouping factors, each of length n, and corresponding
model matrices M1, M2, ..., Mk where Mi is n by qi.  The i'th grouping
factors, gi, only assume values in the range 1,...,mi and takes each
value in that range at least once.

Generally the qi, i = 1,...,k are very small (values of 1 or 2 are
common) but the mi can be large.  In the Scottish secondary school
data the grouping factors are the primary school attended and the
secondary school attended with m1=148 and m2=19 levels respectively.
Each observation corresponds to a student. The total number of
observations is n = 3435.  We would usually start our modeling with a
so-called ``variance components'' model for which q1 = q2 = 1 and
M1(=M2) is the 3435 by 1 matrix, all of whose entries are 1.  The
corresponding 167 by 167 matrix W will consist of two diagonal blocks
of sizes 148 by 148 and 19 by 19 respectively where each of these
blocks is a (positive) multiple of an identity matrix.

The matrix X has at least 2 columns (recall that the response is the
rightmost column of this X).  In keeping with the design of the LDL
package we will store the upper triangle of Z'Z as a compressed,
sparse, column-oriented matrix and Z'X and X'X as dense matrices.

In the variance components model the matrix Z is the concatenation of
k groups of columns, where the i'th group of columns is the mi
indicator columns for grouping factor gi.  In the Scottish secondary
students data Z = [Z1, Z2] where Z1, of size 3435 by 148, is the
indicators of the primary school for each student and Z2, of size 3435 by
19, is the indicators of the secondary school.  Matrices Z1'Z1 and
Z2'Z2 are diagonal with non-negative integers (the number of students
who attended that school) on the diagonal.  The matrix Z1'Z2 can be sparse.

Nested grouping factors:

If the levels of g1 are nested within the levels of g2 then each
column of Z1'Z2 has exactly one non-zero entry.  That is, each level
of g1 occurs with exactly one level of g2.  If the grouping factors
form a nested sequence, in the sense that gi is nested within gi+1 for
i = 1,...,k-1, then Z'Z has an especially simple structure in that the
Cholesky decomposition does not generate any ``fill-in''.  That is,
the Cholesky decomposition of Z'Z+W can be calculated in place and the
Cholesky factor can be inverted in place.  Because nested sequences of
grouping factors do not generate any fill-in it is unnecessary to
search for a fill-minimizing permutation of the levels of the factors.

A single grouping factor trivially forms a nested sequence of grouping
factors.

We detect and exploit this structure when it is present.

Pairwise cross-tabulation:

We will refer to the Z'Z matrix for the variance components form of
the model as the pairwise cross-tabulation of the factors.  This
matrix can be used to check for nested grouping factors and to
calculate fill-reducing permutations for non-nested factors.  Even
when some of the model matrices associated with grouping factors have
multiple columns, the pattern of non-zero elements in Z'Z, and the
fill-reducing permutation of the levels within the groups can be
determined from the pairwise cross-tabulation.

Our general algorithm is:

  determine the pairwise cross-tabulation of the grouping factors
  check for a sequence of nested grouping factors (trivially
    satisfied by a single grouping factor)
  if (non-nested) {
     determine fill-reducing permutation
     separate the groups within this permutation
  }
  if (any qi > 1) {
     re-evaluate Z'Z
  }
  create Z'X and X'X
  use ldl_symbolic to determine Lp (and hence the size of Li and Lx)

  given the diagonal blocks of W
    form Z'Z+W from a copy of Z'Z
    ldl_symbolic of Z'Z+W
    solve LY = Z'X for Y
    Cholesky decomposition (dpotrf) of X'X-(Z'X)'D^{-1}Y
    save D^{-1}Y
