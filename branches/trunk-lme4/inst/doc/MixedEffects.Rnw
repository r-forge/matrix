\documentclass[12pt]{article}
\usepackage{Sweave}
\usepackage{myVignette}
\usepackage[authoryear,round]{natbib}
\bibliographystyle{plainnat}
\DefineVerbatimEnvironment{Sinput}{Verbatim}
{formatcom={\vspace{-2.5ex}},fontshape=sl,
  fontfamily=courier,fontseries=b, fontsize=\scriptsize}
\DefineVerbatimEnvironment{Soutput}{Verbatim}
{formatcom={\vspace{-2.5ex}},fontfamily=courier,fontseries=b,%
  fontsize=\scriptsize}
%%\VignetteIndexEntry{Sparse matrix representations of linear mixed models}
%%\VignetteDepends{Matrix}
%%\VignetteDepends{lme4}
\begin{document}
\SweaveOpts{engine=R,eps=FALSE,pdf=TRUE,width=5,height=3,strip.white=TRUE}
\setkeys{Gin}{width=\textwidth}
\title{Sparse Matrix Representations of Linear Mixed Models}
\author{Douglas Bates\\R Development Core Team\\\email{Douglas.Bates@R-project.org}}
\date{\today}
\maketitle
\begin{abstract}
  We describe a representation of linear mixed-effects models using
  positive semidefinite, symmetric, compressed, column-oriented,
  sparse matrices.  This representation provides for efficient
  evaluation of the profiled log-likelihood or profiled restricted
  log-likelihood of the model, given the relative precision parameters
  of the random effects.  The evaluation is based upon the
  $\bL\bD\bL\trans$ form of the Cholesky decomposition of the
  augmented sparse representation.  Additionally, we can use
  information from this representation to evaluate ECME updates and
  the gradient and Hessian of the criterion being optimized.
\end{abstract}
<<preliminaries,echo=FALSE,print=FALSE>>=
options(width=75)
library(lme4)
data(ScotsSec, package = "Matrix")
@

\section{Introduction}
\label{sec:Intro}

Linear mixed-effects models, described in \citet{pinh:bate:2000}, are
a widely-used class of statistical models.  In chapter 3 of that book
we provide details of computational methods suitable for mixed models
with a single grouping factor for the random effects or with multiple,
nested grouping factors.  Those methods are based on
orthogonal-triangular (or ``QR'') decompositions.  Later, in
\citet{bate:debr:2004} we generalized and extended these QR-based
methods to provide the gradient and the Hessian of the
criteria that are optimized by the parameter estimates.

In this report we formulate a general approach to linear mixed model
calculations using sparse matrix methods.  For an important class
of mixed model problems, those with partially crossed grouping
factors, the sparse matrix methods are superior to any existing
methods in terms of the amount of storage required and computational
speed.  Even for the problems with simpler structure, either a single
grouping factor or nested grouping factors, an efficient
implementation of the sparse-matrix-based methods is competitive with
the best current methods.

In \S\ref{sec:LinearMixed} we review some of the results from
\citet{bate:debr:2004} to establish the basic operations to be
performed and we introduce a moderate-sized example with partially
crossed random effects.  Details of the implementation are considered
in \S\ref{sec:Implementation} and opportunities for further
enhancements are considered in \S\ref{sec:Further}.  Throughout we
illustrate these methods with the implementation provided by the
\code{lme4} and \code{Matrix} packages for R~\citep{R-1.9.0}.

\section{Linear mixed models}
\label{sec:LinearMixed}

For ease of reference we will restate some of the results derived in
\citet{bate:debr:2004}.  The reader should refer to that paper for
details of the derivations.  

We consider linear mixed-effects models that can be written as
\begin{equation}
  \label{eq:lmeGeneral}
  \by=\bX\bbeta+\bZ\bb+\beps\quad
  \beps\sim\mathcal{N}(\bzer,\sigma^2\bI),
  \bb\sim\mathcal{N}(\bzer,\sigma^2\bOmega^{-1}),
  \beps\perp\bb
\end{equation}
where $\by$ is the $n$-dimensional response vector, $\bX$ is an
$n\times p$ model matrix for the $p$-dimensional fixed-effects vector
$\bbeta$, $\bZ$ is the $n\times q$ model matrix for the
$q$-dimensional random-effects vector $\bb$ that has a Gaussian
distribution with mean $\bzer$ and relative precision matrix $\bOmega$
(i.e., $\bOmega$ is the precision of $\bb$ relative to the precision
of $\beps$), and $\beps$ is the random noise assumed to have a
spherical Gaussian distribution.  The symbol $\perp$ indicates
independence of random variables.  We assume that $\bX$ has full
column rank and that $\bOmega$, which is a function of an
(unconstrained) parameter vector $\btheta$, is positive definite.

Given the observed responses $\by$ and the model matrices $\bX$ and
$\bZ$, we wish to determine either the maximum likelihood (ML) or the
restricted maximum likelihood (REML) estimates of the parameters
$\btheta$, $\bbeta$, and $\sigma^2$.  Because the conditional
estimates of $\bbeta$ and $\sigma^2$, given a value of $\btheta$, for
either criterion can be determined from a penalized least squares
problem we can reduce the optimization problem to one involving
$\btheta$ only.  This reduction of the dimension of the optimization
problem is called \emph{profiling}.

The conditional, penalized least squares problem can be solved using
the Cholesky decomposition
\begin{equation}
  \label{eq:CrossProdGen}
  \begin{bmatrix}
    \bZ\trans\bZ+\bOmega & \bZ\trans\bX  & \bZ\trans\by \\
    \bX\trans\bZ         & \bX\trans\bX  & \bX\trans\by \\
    \by\trans\bZ         & \by\trans\bX  & \by\trans\by
  \end{bmatrix}=\bR\trans\bR\quad\text{where}\quad\bR=
  \begin{bmatrix}
    \RZZ & \RZX & \rZy \\
    \bzer    & \RXX & \rXy \\
    \bzer    & \bzer    & \ryy
  \end{bmatrix} .
\end{equation}
The matrices $\RZZ$ and $\RXX$ are upper triangular of dimension
$q\times q$ and $p\times p$ respectively.  Because these matrices are
triangular it is easy to evaluate the squares of the determinants
$|\RZZ|^2$ and $|\RXX|^2$.  (Because the signs of the diagonal
elements of $\RZZ$ and $\RXX$ are not uniquely determined, $|\RZZ|$
and $|\RXX|$ are not well-defined.  However, their magnitudes, and
hence their squares, are well-defined.)  The corresponding vectors,
$\rZy$ and $\rXy$, are of dimension $q$ and $p$, and $\ryy$ is a
scalar.  The conditions that $\bOmega$ be positive definite and $\bX$
have full column rank ensure that $\RZZ$ and $\RXX$ are nonsingular.

The conditional estimates of $\bbeta$ satisfy
\begin{equation}
  \label{eq:betaHat}
  \RXX\widehat{\bbeta}(\btheta)=\rXy
\end{equation}
and the conditional modes of the random effects satisfy
\begin{equation}
  \label{eq:ConditionalExp}
  \RZZ\widehat{\bb}(\btheta)=\rZy-\RZX\widehat{\bbeta} .
\end{equation}
The conditional ML estimate of $\sigma^2$ is
$\widehat{\sigma^2}(\btheta)=\ryy^2/n$ and the conditional REML
estimate is $\widehat{\sigma^2}_R(\btheta)=\ryy^2/(n-p)$.

The profiled optimization problem, expressed in terms of the
deviance, is
\begin{equation}
  \label{eq:ProfiledLogLik}
  \begin{aligned}
    \widehat{\btheta}&=\arg\min_{\btheta} -2\tilde{\ell}(\btheta)\\
    &=\arg\min_{\btheta}\left\{\log\left(\frac{\left|\RZZ\right|^2}
      {\left|\bOmega\right|}\right)
    + n\left[1+\log\left(\frac{2\pi\ryy^2}{n}\right)\right]\right\}
  \end{aligned}
\end{equation}
\begin{equation}
  \label{eq:ProfiledLogRestLik}
  \begin{aligned}
    \widehat{\btheta_R}&=\arg\min_{\btheta} -2\tilde{\ell_R}(\btheta)\\
    &=\arg\min_{\btheta}\left\{\log\left(\frac{\left|\RZZ\right|^2\left|\RXX\right|^2}
      {\left|\bOmega\right|}\right)
    +  (n-p)\left[1+\log\left(\frac{2\pi\ryy^2}{n-p}\right)\right]\right\}
  \end{aligned}
\end{equation}
for ML and REML estimation, respectively.  The gradients of these
criteria are
\begin{align}
  \label{eq:gradDev}
  \nabla(-2\tilde\ell)&=\tr\left[\der\bOmega\left(
      (\bZ\trans\bZ+\bOmega)^{-1}-\bOmega^{-1}+
      \frac{\hat{\bb}}{\widehat{\sigma}}
      \frac{\hat{\bb}}{\widehat{\sigma}}\trans\right)\right]\\
  \label{eq:gradDevRest}
  \nabla(-2\tilde\ell_R)&=\tr\left[\der\bOmega\left(
      \vb-\bOmega^{-1}+
      \frac{\hat{\bb}}{\widehat{\sigma}_R}
      \frac{\hat{\bb}}{\widehat{\sigma}_R}\trans\right)\right]
\end{align}
where
\begin{equation}
  \label{eq:vbDef}
  \vb=\RZZ^{-1}\left(\bI+\RZX\RXX^{-1}\RXX\invtrans
    \RZX\trans\right)\RZZ\invtrans
\end{equation}
and $\der$ denotes the Frechet derivative.  Terms in the Hessian can
be expressed as
\begin{align}
  \label{eq:HessDev}
  \derj\deri(-2\tilde{\ell})&=\tr\left[\derj(\deri\bOmega)\left(
      (\bZ\trans\bZ+\bOmega)^{-1}-\bOmega^{-1}+
      \frac{\hat{\bb}}{\widehat{\sigma}}
      \frac{\hat{\bb}}{\widehat{\sigma}}\trans\right)\right]\\
  \nonumber
  &\quad-\tr\left[\derj\bOmega
    (\bZ\trans\bZ+\bOmega)^{-1}\deri\bOmega(\bZ\trans\bZ+
    \bOmega)^{-1}\right]\\
  \nonumber
  &\quad+\tr\left(\derj\bOmega\bOmega^{-1}\deri\bOmega\bOmega^{-1}\right)
  -2\frac{\hat{\bb}}{\widehat{\sigma}}\trans\derj\bOmega\vb\deri\bOmega
  \frac{\hat{\bb}}{\widehat{\sigma}}\\
  \nonumber
  &\quad-\frac{1}{n}
  \left(\frac{\hat{\bb}}{\widehat{\sigma}}\trans\derj\bOmega
    \frac{\hat{\bb}}{\widehat{\sigma}}\right)
  \left(\frac{\hat{\bb}}{\widehat{\sigma}}\trans\deri\bOmega
    \frac{\hat{\bb}}{\widehat{\sigma}}\right)\\
  \label{eq:HessDevRest}
  \derj\deri(-2\tilde{\ell_R})&=\tr\left[\derj(\deri\bOmega)\left(
      \vb-\bOmega^{-1}+
      \frac{\hat{\bb}}{\widehat{\sigma}_R}
      \frac{\hat{\bb}}{\widehat{\sigma}_R}\trans\right)\right]\\
  \nonumber
  &\quad-\tr\left[\derj\bOmega \vb\deri\bOmega\vb\right]\\
  \nonumber
  &\quad+\tr\left(\derj\bOmega\bOmega^{-1}\deri\bOmega\bOmega^{-1}\right)
  -2\frac{\hat{\bb}}{\widehat{\sigma}_R}\trans\derj\bOmega\vb\deri\bOmega
  \frac{\hat{\bb}}{\widehat{\sigma}_R}\\
  \nonumber
  &\quad-\frac{1}{n-p}
  \left(\frac{\hat{\bb}}{\widehat{\sigma}_R}\trans\derj\bOmega
    \frac{\hat{\bb}}{\widehat{\sigma}_R}\right)
  \left(\frac{\hat{\bb}}{\widehat{\sigma}_R}\trans\deri\bOmega
    \frac{\hat{\bb}}{\widehat{\sigma}_R}\right)
\end{align}
where $\deri$ and $\derj$
represent differentiation with respect to elements $i$ and $j$ of
$\btheta$, respectively.

If good starting estimates of $\btheta$ are not available, the initial
Newton iterations for (\ref{eq:ProfiledLogLik}) or
(\ref{eq:ProfiledLogRestLik}) can be unstable.  We can refine our
initial estimates with a moderate number of ECME steps
for which $\btheta_{i+1}$ satisfies
\begin{equation}
  \label{eq:theta1}
  \tr\left[\der\bOmega\left(
      \frac{\hat{\bb}(\btheta_i)}{\hat{\sigma}(\btheta_{i})}
      \frac{\hat{\bb}(\btheta_i)\trans}{\hat{\sigma}(\btheta_{i})}+
      \left(\bZ\trans\bZ+\bOmega(\btheta_{i})\right)^{-1}
      -\bOmega(\btheta_{i+1})^{-1}\right)\right]=\bzer
\end{equation}
for ML estimates or
\begin{equation}
  \label{eq:theta1R}
  \tr\left[\der\bOmega\left(
      \frac{\hat{\bb}(\btheta_{i})}{\hat{\sigma}_R(\btheta_i)}
      \frac{\hat{\bb}(\btheta_{i})\trans}{\hat{\sigma}_R(\btheta_i)}+
      \vb(\btheta_{i})
      -\bOmega(\btheta_{i+1})^{-1}\right)\right]=\bzer
\end{equation}
for REML.

At this point it is easy to formulate a general method of obtaining ML
or REML estimates for a linear mixed model:
\begin{enumerate}
\item Given the data $\by$ and the model matrices $\bX$ and $\bZ$,
  formulate initial estimates $\btheta_0$.  Some heuristics for doing
  so are given in \citet[ch.~3]{pinh:bate:2000}.
\item Use a moderate number of ECME steps, (\ref{eq:theta1}) or
  (\ref{eq:theta1R}), to refine these starting estimates.  Each ECME
  step requires evaluating $\bOmega(\btheta)$ followed by the
  decomposition (\ref{eq:CrossProdGen}) and the solutions to
  (\ref{eq:betaHat}) and (\ref{eq:ConditionalExp}).
\item Use a Newton method to optimize the criterion
  (\ref{eq:ProfiledLogLik}) or (\ref{eq:ProfiledLogRestLik}) with
  gradient (\ref{eq:gradDev}) or (\ref{eq:gradDevRest}) (and possibly
  using analytic Hessians defined by (\ref{eq:HessDev}) or
  (\ref{eq:HessDevRest})).  Each evaluation of the criterion
  requires  evaluating $\bOmega(\btheta)$ followed by the
  decomposition (\ref{eq:CrossProdGen}).  Gradient evaluations require
  the solutions to (\ref{eq:betaHat}) and (\ref{eq:ConditionalExp}).
\end{enumerate}
In \citet{bate:debr:2004} we show that the deviance forms of the
criteria are bounded below over the parameter space and,
furthermore, that the behavior of these criteria at the extremes of
the parameter space can be characterized.

\subsection{Sparsity in the large matrices}
\label{sec:Sparsity}

In terms of equations, the solution to the estimation problem can be
succinctly stated.  In terms of computation there is one complicating
factor --- in practice the matrices $\bZ\trans\bZ$ and $\bOmega$ are
sparse and patterned and can be extremely large.  The decomposition
(\ref{eq:CrossProdGen}) and the solution of the system
(\ref{eq:ConditionalExp}) must be done carefully.

Sparsity of $\bZ\trans\bZ$ and $\bOmega$ is induced by the association
of components of $\bb$ with grouping factors $\bff_i,i=1,\dots,k$ in
the data.  Each grouping factor is of length $n$, the same as the
length of $\by$.  The number of distinct values in $\bff_i$ is
$m_i,i=1,\dots,k$.  In the general form of the model, a model matrix
$\bZ_i$ of size $n\times q_i$ is associated with grouping factor
$\bff_i$, $i=1,\dots,k$.  Typically the $q_i$ are very small.  In
fact, in one common form of the model, called a \emph{variance
components} model, $q_1=q_2=\dots=q_k=1$ and each of the
$\bZ_i,i=1,\dots,k$ consist of a single column of 1's.

In the general form, the random effects vector $\bb$, of length
$q=\sum_{i=1}^k m_i q_i$, is partitioned into $k$ ``outer blocks''
where the i'th outer block is of size $m_i q_i,i=1,\dots,k$.  The
columns of $\bZ$ and the rows and columns of $\bZ\trans\bZ$ and
$\bOmega$ are similarly partitioned.  Each of the outer blocks is
further subdivided into $m_i$ inner blocks of size $q_i$.  The
grouping factors determine the outer blocks and the levels of each
grouping factor determine the inner blocks.

In the models that we will consider, the random effects associated
with different grouping factors are independent.  That is to say that
$\bOmega$ is block-diagonal consisting of $k$ diagonal blocks of sizes
$m_i q_i\times m_i q_i,i=1,\dots,k$.  Furthermore the random effects
associated with the levels of a given blocking factor are independent
and identically distributed.  That is, the $i$'th diagonal block in
$\bOmega$ is itself block diagonal and these diagonal blocks are $m_i$
repetitions of a $q_i\times q_i$ matrix $\bOmega_i$, $i=1,\dots,k$.

For a variance components model the matrices $\bOmega_i,i=1,\dots,k$
are $1\times 1$ positive definite matrices which we can consider as
positive scalars $\omega_i,i=1,\dots,k$.  The matrix $\bOmega$ is
block-diagonal of size $\sum_{i=1}^k m_i$ and the diagonal blocks are
$\omega_i\bI_{m_i}$ where $\bI_{m_i}$ is the $m_i\times m_i$ identity
matrix.  Thus $\left|\bOmega\right|=\sum_{i=1}^k m_i\omega_i$. The
$k$-dimensional vector $\btheta$ where
$\theta_i=\log\omega_i,i=1,\dots,k$ can be used as the unconstrained
parameter vector.

The columns of the matrix $\bZ$ are similarly divided into blocks.
For the variance components model the $i$th block is the set of
indicator columns for the $m_i$ levels of $\bff_i,i=1,\dots,k$.
Because each block is a set of indicators, the diagonal blocks of
$\bZ\trans\bZ$ are themselves diagonal.  However, unlike the
corresponding blocks in $\bOmega$, these blocks are not necessarily a
multiple of the identity.  The diagonal elements of the $i$th diagonal
block are the $m_i$ frequencies of occurence of each the levels of the
$i$th grouping factor in the data.  (Because all the elements of $\bZ$
are zero or one, the diagonals of $\bZ\trans\bZ$ are simply the counts
of the number of ones in the corresponding column of $\bZ$.)

The off-diagonal blocks of $\bZ\trans\bZ$ in a variance components
model are the pairwise cross-tabulations of the corresponding grouping
factors.

\subsection{Scottish secondary school student scores}
\label{sec:Scottish}

An example may help to clarify these descriptions.

Data on achievement scores of Scottish secondary school students are
described in \citet{Paterson:1991} and are analyzed in
\citet[ch.~18]{MLwiN:2002} and other references.  In the \code{Matrix}
package for \RR{} these data are available as the data set
\var{ScotsSec} containing the achievement scores (\var{attain}), some
demographic data (\var{sex} and \var{social} class), a \var{verbal}
reasoning score based on tests taken at entry to secondary school, and
the \var{primary} and secondary (\var{second}) schools attended by
3435 students.

The grouping factors for the random effects are \var{primary} (148
distinct schools) and \var{second} (19 distinct schools).  The
summary of these variables each give the counts for the six schools
with the greatest number of students in the study.  A cross-tabulation
of \var{primary} and \var{second} (we show only the first five rows),
shows that on some occasions students from the same primary school
attend different secondary schools.
<<ScotsSecsummary>>=
summary(ScotsSec[,c("primary", "second")])
xtabs(~primary + second, ScotsSec)[1:5,]
@ 

If all the students from a given primary school attended the same
secondary school we would say that \var{primary} is \emph{nested
  within} \var{second}.  That is not the case here.  There is a
moderate amount of \emph{crossing} of these two grouping factors.  If
there was at least one student in the study from each combination of
primary school and secondary school we would describe the grouping
factors \var{primary} and \var{second} as being \emph{fully crossed}.
Again, that is not the case here.  Grouping factors like these that
are neither nested nor fully crossed are called \emph{partially
  crossed}.

We begin with a variance components model that incorporates the
student's sex and their verbal pre-test score and the interaction 
of these two covariates in the fixed effects.
<<fm1>>=
fm1 = lme(attain~verbal*sex, data = ScotsSec, random = list(primary=~1,second=~1))
@ 

The locations of the non-zeros in the $167\times 167$ matrix
$\bZ\trans\bZ$ are shown in Figure~\ref{fig:ZtZ}.
\begin{figure}[tbp]
  \centering
\setkeys{Gin}{width=\textwidth}
<<FigZtZ,fig=TRUE,echo=FALSE,height=8,width=8>>=
print(image(as(fm1@rep, "sscMatrix"), aspect='xy', xlim = c(-0.5,166.5),
            ylim = c(166.5, -0.5), colorkey = FALSE))
@   
  \caption{Location of non-zero elements in
    $\bZ\trans\bZ$ for model \var{fm1} fit to the \var{ScotsSec} data.
    Darker squares indicate larger magnitudes. Rows and columns are
    numbered from zero. The levels of the grouping factors have been
    rearranged according to a fill-reducing permutation.}
  \label{fig:ZtZ}
\end{figure}
Darker greys indicate larger magnitudes.  As can be expected the
largest magnitudes occur on the diagonal of the secondary school block.
Figure~\ref{fig:ZtZ} corresponds to a re-ordering of the levels of the
\var{primary} and \var{second} grouping factors, as described in the
next section.

The matrix $\bZ\trans\bZ$ is stored in the compressed, sorted, sparse,
symmetric, column-oriented representation.  Only the upper triangle or
the lower triangle of this symmetric matrix need be stored.  Because
we will use the methods implemented in Tim Davis' LDL package
\citep{Davis:2004} which requires the upper triangle to be stored, we
do so.  There are a total of 404 non-zero elements in the upper
triangle.  We store these in increasing column order and, within each
column, in order of increasing row index.  In addition to the non-zero
value we must store the row indices and the column indices, but the
column indices are in increasing order and we can compress this vector
by storing only the information on where each column begins. This is a
standard representation for sparse matrices, as described in
\citet{Davis:2004}.

The matrix $\bOmega$ is implicitly defined by the number of levels in
each of the grouping factors and by the values of $\omega_1$ and
$\omega_2$ (shown below).  We store the matrix $\bZ\trans\bX$ and the
vector $\bZ\trans\by$ as a single dense matrix of size $167\times 5$.
Similarly $\bX\trans\bX$, $\bX\trans\by$ and $\by\trans\by$ are stored
in a single dense matrix of size $5\times 5$ (only the upper triangle
of this symmetric matrix is stored).
<<fm1Pieces>>=
unlist(fm1@rep@Omega)
dim(fm1@rep@ZtX)
fm1@rep@ZtX[1:4,]
fm1@rep@XtX
@ 

\subsection{Cholesky decomposition of the sparse matrices}
\label{sec:Fill-reducing}

For each value of $(\omega_1,\omega_2)$, code from the LDL package is
used to factor
\begin{equation}
  \label{eq:LDLdef}
  \bZ\trans\bZ+\bOmega=\bL\bD\bL\trans
\end{equation}
where $\bL$ is a unit, lower triangular $q\times q$ matrix and $\bD$
is diagonal with positive diagonal elements.  Because the diagonal
elements of the unit triangular matrix $\bL$ are, by definition,
unity, they are not explicitly stored.

The number of nonzero offdiagonal elements in $\bL$ depends on the
order of the rows and columns in $\bZ\trans\bZ+\bOmega$ because these
determine the amount of ``fill-in'' that occurs during the course of
the decomposition algorithm.  There are several different approaches to
determining a fill-reducing permutation of the rows and columns.  We
use an algorithm based on graph partitioning and implemented in
Metis~\citep{Metis}.  

A general fill-reducing permutation algorithm will not preserve
separation of the rows (and columns) associated with different
grouping factors.  Especially when working with more general
mixed-effects models for which the $q_i,i=1,\dots,k$ can be greater
than 1, there are substantial advantages in maintaining separation of
the grouping factors.  To avoid the undesirable intermingling of the
grouping factors, we take the permutation returned by Metis and
reorder it to separate the factors.

For the Scottish secondary school data the number of non-zero
off-diagonal elements in $\bL$ is 464.  The total amount of storage
required for $\bD$ and $\bL$ is 167 numeric elements in $\bD$ and 464
numeric elements in $\bL$.  We also need 632 integer elements for the
indices in $\bL$ (168 for the $q+1$ column pointers
and 464 for the row indices of the nonzero offdiagonals).

The positions of the nonzero elements in the last 19 rows of $\bL$ are
shown in Figure~\ref{fig:Lnonzero} (the first 148 rows of $\bL$ are an
identity matrix augmented with 19 columns of zeros)
\begin{figure}[tbp]
  \centering
\setkeys{Gin}{width=\textwidth}
<<FigLnonzero,fig=TRUE,echo=FALSE,height=2.5,width=8>>=
print(image(as(fm1@rep, "tscMatrix"),ylim=c(166.5,147.5),
            xlim=c(-0.5,166.5),colorkey=FALSE,aspect='xy'))
@   
  \caption{Location of non-zero elements in the last 19 rows of the
    unit lower triangular matrix $\bL$ for model \var{fm1} fit to the
    \var{ScotsSec} data.  Darker squares indicate larger magnitudes.
    Rows and columns are numbered from zero. The levels of the
    grouping factors have been rearranged according to a fill-reducing
    permutation.}
  \label{fig:Lnonzero}
\end{figure}

If the grouping factors are a nested sequence of factors there will be
no fill-in.  In fact, both $\bL$ and its inverse will have exactly the
same pattern of nonzeros as does the lower triangle of $\bZ\trans\bZ$.
We do not seek a fill-reducing permutation if the grouping factors
form a nested sequence.  The case $k=1$ (the random effects are
determined by a single grouping factor) is, trivially, a nested
sequence.

When the grouping factors are not in a strictly nested sequence, the
number of nonzeros in $\bL^{-1}$ can be much greater than the number
of nonzeros in $\bL$.  The positions of the nonzeros in the last 19
rows of $\bL^{-1}$ (again, the first 148 rows are an identity matrix
augmented with 19 columns of zeros) are shown in
Figure~\ref{fig:LInonz}.
\begin{figure}[tbp]
  \centering
\setkeys{Gin}{width=\textwidth}
<<FigLInonz,fig=TRUE,echo=FALSE,height=2.5,width=8>>=
fmr = fm1@rep
LI = as(new("tscMatrix", i = fmr@LIi, p = fmr@LIp, Dim=fmr@Dim,
         x = fmr@LIx, uplo = "L", diag = "U"), "tripletMatrix")
print(image(LI, ylim=c(166.5,147.5), xlim=c(-0.5,166.5),
            colorkey=FALSE,aspect='xy'))
@   
  \caption{Location of non-zero elements in the last 19 rows of the unit
    lower triangular matrix $\bL^{-1}$ for model \var{fm1} fit to the
    \var{ScotsSec} data. Darker squares indicate larger magnitudes.
    Rows and columns are numbered from zero. The levels of the
    grouping factors have been rearranged according to a fill-reducing
    permutation.}
  \label{fig:LInonz}
\end{figure}
There are 2200 nonzero off-diagonal elements in $\bL^{-1}$ for this
example.  However, even this count is less than the number of cells
required in MLwiN (2812, according to \citet{MLwiN:2002}) to
store the off-diagonal part of $\bZ\trans\bZ$.

The small overhead incurred for index storage with sparse matrices is
repaid many times in the savings from not storing elements that are zero.


\section{Implementation}
\label{sec:Implementation}



\section{Further enhancements}
\label{sec:Further}


\section{Acknowledgements}
\label{sec:Ack}

This work was supported by U.S.{} Army Medical Research and Materiel
Command under Contract No.{} DAMD17-02-C-0119.  The views, opinions
and/or findings contained in this report are those of the authors and
should not be construed as an official Department of the Army
position, policy or decision unless so designated by other
documentation. 

I thank Deepayan Sarkar and Tim Davis for helpful discussions and
suggestions and Harold Doran for his original suggestion to use sparse
matrix techniques for linear mixed-effects models.

The \code{Matrix} package for \RR{} is based on code from the
LDL~\citep{Davis:2004}, TAUCS~\citep{Taucs}, and Metis~\citep{Metis}
packages.

\bibliography{lme4}
\end{document}
Yesterday I spoke with Deepayan Sarkar, a graduate student with whom I
work, on ways for structuring the calculations for linear
mixed-effects models using sparse matrix representations and Tim
Davis's LDL decomposition code.  I described to Deepayan a plan A and
a plan B.  On thinking about the calculations more yesterday evening I
formulated a plan AB, which is what I think I will use.

One purpose of writing this note is so I can get the steps clear in my
mind.

The critical part of the calculation is determining the Cholesky
decomposition of a matrix of the form

 Z'Z+W Z'X Z'y
  X'Z  X'X X'y   for different matrices W that depend upon parameters r
  y'Z  y'X y'y

Statistically the role of y is very different from X but, as far as
the calculation goes, I can work with the augmented matrix [X,y] as if
it were a single matrix, which, at the risk of some confusion, I will
henceforth call X.  We will not refer to y again.

Matrices Z and X are n by q and n by p respectively with n >= q.
Generally, q >> p, Z is sparse, W is block diagonal (and the blocks
are of small dimension) and X is dense.  The matrix Z is generated
from a set of k grouping factors, each of length n, and corresponding
model matrices M1, M2, ..., Mk where Mi is n by qi.  The i'th grouping
factors, gi, only assume values in the range 1,...,mi and takes each
value in that range at least once.

Generally the qi, i = 1,...,k are very small (values of 1 or 2 are
common) but the mi can be large.  In the Scottish secondary school
data the grouping factors are the primary school attended and the
secondary school attended with m1=148 and m2=19 levels respectively.
Each observation corresponds to a student. The total number of
observations is n = 3435.  We would usually start our modeling with a
so-called ``variance components'' model for which q1 = q2 = 1 and
M1(=M2) is the 3435 by 1 matrix, all of whose entries are 1.  The
corresponding 167 by 167 matrix W will consist of two diagonal blocks
of sizes 148 by 148 and 19 by 19 respectively where each of these
blocks is a (positive) multiple of an identity matrix.

The matrix X has at least 2 columns (recall that the response is the
rightmost column of this X).  In keeping with the design of the LDL
package we will store the upper triangle of Z'Z as a compressed,
sparse, column-oriented matrix and Z'X and X'X as dense matrices.

In the variance components model the matrix Z is the concatenation of
k groups of columns, where the i'th group of columns is the mi
indicator columns for grouping factor gi.  In the Scottish secondary
students data Z = [Z1, Z2] where Z1, of size 3435 by 148, is the
indicators of the primary school for each student and Z2, of size 3435 by
19, is the indicators of the secondary school.  Matrices Z1'Z1 and
Z2'Z2 are diagonal with non-negative integers (the number of students
who attended that school) on the diagonal.  The matrix Z1'Z2 can be sparse.

Nested grouping factors:

If the levels of g1 are nested within the levels of g2 then each
column of Z1'Z2 has exactly one non-zero entry.  That is, each level
of g1 occurs with exactly one level of g2.  If the grouping factors
form a nested sequence, in the sense that gi is nested within gi+1 for
i = 1,...,k-1, then Z'Z has an especially simple structure in that the
Cholesky decomposition does not generate any ``fill-in''.  That is,
the Cholesky decomposition of Z'Z+W can be calculated in place and the
Cholesky factor can be inverted in place.  Because nested sequences of
grouping factors do not generate any fill-in it is unnecessary to
search for a fill-minimizing permutation of the levels of the factors.

A single grouping factor trivially forms a nested sequence of grouping
factors.

We detect and exploit this structure when it is present.

Pairwise cross-tabulation:

We will refer to the Z'Z matrix for the variance components form of
the model as the pairwise cross-tabulation of the factors.  This
matrix can be used to check for nested grouping factors and to
calculate fill-reducing permutations for non-nested factors.  Even
when some of the model matrices associated with grouping factors have
multiple columns, the pattern of non-zero elements in Z'Z, and the
fill-reducing permutation of the levels within the groups can be
determined from the pairwise cross-tabulation.

Our general algorithm is:

  determine the pairwise cross-tabulation of the grouping factors
  check for a sequence of nested grouping factors (trivially
    satisfied by a single grouping factor)
  if (non-nested) {
     determine fill-reducing permutation
     separate the groups within this permutation
  }
  if (any qi > 1) {
     re-evaluate Z'Z
  }
  create Z'X and X'X
  use ldl_symbolic to determine Lp (and hence the size of Li and Lx)

  given the diagonal blocks of W
    form Z'Z+W from a copy of Z'Z
    ldl_symbolic of Z'Z+W
    solve LY = Z'X for Y
    Cholesky decomposition (dpotrf) of X'X-(Z'X)'D^{-1}Y
    save D^{-1}Y


  
  The ordering of the columns (and, correspondingly, the rows) of a
  positive semidefinite, symmetric sparse matrix can have a
  substantial effect on the amount of fill-in generated by the
  Cholesky decomposition.  For the particular matrices considered here
  the ordering will only become important when random effects are
  associated with more that one grouping factor and the grouping
  factors are neither nested nor fully crossed.  We say that such
  factors are partially crossed, a situation that is very common in
  observational data.

  Several methods for determining favorable orderings have been
  proposed but these generally reorder all the columns.  In our case
  the columns are grouped.  We show that we can reorder the columns
  while preserving the grouping and still attain acceptable levels of
  fill-in. 


This report is directed at two audiences: sparse matrix researchers
and mixed-effects model researchers.  To make the ideas accessible to
both audiences I will need to introduce a formulation of mixed models
for the sparse matrix researchers and also to introduce sparse matrix
representation for the mixed model researchers.


Opportunities for efficiency exist because we must repeatedly perform
Cholesky decompositions of matrices with the same pattern of non-zero
entries (and somewhat different values of those non-zero entries) and
because the non-zero entries occur in dense blocks.  Many methods for
sparse matrices have both a symbolic phase, where the pattern of the
non-zero entries in the result is determined, and a numeric phase,
where the numerical results are determined.  For decompositions part
of the symbolic phase is determination of permutation of the rows and
columns that reduces fill-in for the decomposition.  In our problem
the symbolic phase need only be done once and it can be performed
based the positions of the blocks.  The matrix giving the positions of
the blocks is frequently much smaller and easier to manipulate than
the matrix that is to be decomposed.  Furthermore, the blocks provide
a natural way of using supernodes that employ dense matrix building
blocks in a sparse matrix calculation, for this problem.

In the next section I introduce a general form of linear mixed-models
and the notation I will use.  I also introduce three sample data sets
and models.  These include a very simple example that can be used to
illustrate details of the calculations, a moderate-sized example that
has been used to illustrate other methods, and a very large example
that can show the savings available with sparse matrix methods.  In
\S\ref{sec:Examples} I introduce sparse matrix storage schemes and
computational methods and show how they can be applied to the
examples. 


Using the response and the model
matrices, $\by$, $\bX$ and $\bZ$, which are evaluated from the
observed data, we determine the estimates of the model parameters;
$\bbeta$, $\btheta$, and $\sigma^2$, as those values that optimize the
likelihood function or, more commonly, a variant called the restricted
likelihood.  Both the likelihood and the restricted likelihood must be
positive and their logarithms, called the log-likelihood and
the log-restricted-likelihood, are generally easier to evaluate and
provide a better quadratic approximation for optimization than the
original functions, so we work on the log-likelihood scale.






When the random effects are grouped according to more than one
grouping factors, as here, it is important to determine if the
grouping factors are \emph{crossed} (every level of factor 1 occurs
with every level of factor 2) or nested (each level of factor 1 occurs
with only one level of factor 2) or partially crossed, which is how we
describe grouping factors that are neither (fully) crossed nor
strictly nested.

In this case the grouping factors \var{primary} and \var{second} are
partially crossed.  We can express this graphically through the image
of the cross-tabulation of the grouping factors.  We can generate such
a cross-tabulation as a symmetric, sparse, compressed column matrix
with the \var{sscCrosstab} function and use \var{image} to show the
non-zero elements graphically.  (Only the non-zero elements in the
lower triangle are shown.)
<<ctab>>=
ctab = sscCrosstab(ScotsSec[, c("primary", "second")])
str(ctab)
@ 


\subsection{A simple variance components model}
\label{sec:varianceComponents}

In \citet[\S1.1]{pinh:bate:2000} we discuss measurements of the travel
time of a certain type of ultrasonic wave in six different
railway rails.  Each rail was tested three times yielding a total of
18 observations.  Each observation denotes the rail and the observed
travel time. A simple data plot (e.g.{} Fig. 1.1 in
\cite{pinh:bate:2000}) shows that the variation between responses on
different rails is much greater than the variation between responses on
the same rail.  We model this as
\begin{equation}
  \label{eq:varianceComp}
  y_{ij}=\mu+b_i+\epsilon_{ij}\quad
  i=1,\dots,6,\;j=1,\dots,3\quad
  b_i\sim\mathcal{N}(0,\sigma^2_b),\;\epsilon_{ij}\sim\mathcal{N}(0,\sigma^2)
\end{equation}
where $\sigma^2$ is the within-rail variance and $\sigma^2_b$ is the
between-rail variance.  These are called the \emph{variance components}.

The random variable $b_i$ is the deviation of the mean travel time for
rail $i$ from the overall mean travel time $\mu$.  These are called
the \emph{random effects} associated with the rails.

We can express model (\ref{eq:varianceComp}) in the form
(\ref{eq:lmeGeneral}) by setting
\begin{equation}
  \label{eq:varianceCompMod}
  \begin{aligned}
    \bb &=\left(b_1,b_2,\dots,b_6\right)\trans\\
    \bbeta &= \mu
    \bX\trans&=\left[1,1,\dots,1\right]\trans
  \end{aligned}
\end{equation}
and $\bZ$ to be the $18\times 6$ matrix of indicators of the rail.
The matrix 
\begin{equation}
  \label{eq:relativePrecision}
  \bOmega=\frac{\sigma^2}{\sigma_a^2}\bI_6
\end{equation}
where $\bI$ is the $6\times 6$ identity matrix.  The multiple
$\frac{\sigma^2}{\sigma_a^2}$ is the relative precision of the random
effects and the parameter $\theta$ is a scalar that determines this
multiple.  To obtain an unconstrained $\theta$ we could use the
logarithm of the ratio
\begin{equation}
  \label{eq:thetaDef}
  \theta = \log\left(\sigma^2\right)-\log\left(\sigma_a^2\right)
\end{equation}

The first few rows of $\bZ$, $\bX$, and $\by$ are
<<ZXy>>=
data(Rail, package = "nlme")
ZXy = cbind(model.matrix(~Rail-1,Rail), X = 1, y = Rail$travel)
ZXy[1:4,]
@ 
The matrix to be decomposed is obtained by adding $\bOmega$ to the
$6\times 6$ upper left submatrix of
<<crossprod>>=
crossprod(ZXy)
@ 
For example, if $e^\theta=0.1$ then the Cholesky decomposition is
<<chol1>>=
options(digits=5)
chol(crossprod(ZXy) + diag(c(rep(.1, 6), 0, 0)))
@ 

As seen in this example the cross-product matrix is sparse and only
only the diagonal and the last two rows need to be stored.  (It
happens that $\bZ\trans\bZ$ is a multiple of the identity, as is
$\bOmega$, and this could lead to further simplifications.  However,
this property depends on the fact that the data are balanced in the
sense that there are the same number of observations made on each
rail.  Although data from a designed experiment may be balanced,
observational data are almost never balanced so it is not worthwhile
trying to exploit this special structure.)

In general the trailing $p+1$ rows (and columns) of the cross-product
matrix will be dense.  The structure of the other 
summarized as


In \S\ref{sec:SparseM} we describe two of the popular sparse
matrix representations and formation of the Cholesky decomposition of
sparse, symmetric, positive semidefinite matrices.  The number of
non-zero entries in the Cholesky factor can depend on the ordering of
the columns (and, correspondingly, the rows) of the original matrix.
Various methods have been proposed to choose optimal or near-optimal
reorderings.  This is an example of symbolic analysis that can be used
before the numeric computation to reduce the amount of numeric
computation.  We describe others.


\section{Sparse matrix classes and methods in the Matrix package for R}
\label{sec:SparseM}

The simplest representation of a sparse matrix $\bX$ is to store a
triplet $(i,j,x_{ij})$ for each non-zero element.  If the triplets are
sorted, say by column order, the column indices will occur in blocks
of equal values.  In the \emph{compressed, sparse, column-oriented}
format the entries are sorted in increasing column order and a set of
pointers to the beginning of each column are used instead of the
column values themselves.  The \var{tripletMatrix} class in the
\var{Matrix} package provides the triplet format and the
\var{cscMatrix} class provides the compressed, sparse column-oriented
format.  In both these classes indices are 0-based (for compatibility
with the underlying C code) and not 1-based as is common in R.
<<Matintro>>=
library(Matrix)
mm = new("tripletMatrix", 
         i = as(c(0,2,3,1,2,0,3,4,3,4),"integer"),
         j = as(c(0,0,0,1,1,2,2,2,3,3),"integer"),
         x = (1:10)/10, Dim = as(c(5,4),"integer"))
m1 = as(mm,"cscMatrix")
str(m1)
as(m1, "matrix")
diff(m1@p)
@ 
We see that the \var{p} slot in a \var{cscMatrix} with 4 columns has 5
elements.  The first element is always zero and the successive differences
are the numbers of non-zero elements in each column.  The total number
of non-zero elements is the value of the last element of the \var{p}
slot.  This is also the length of the vector of row indices in the
\var{i} slot.

The validation method for the \var{cscMatrix} class ensures that the
row indices are increasing within columns and reorders the \var{i} and
\var{x} slots if necessary to achieve this.  Technically, the objects
in this class can be described as sorted, compressed, sparse,
column-oriented matrices.

Objects in the \var{tscMatrix} class represent triangular, sparse,
column-oriented matrices and those in the \var{sscMatrix} class
represent symmetric, sparse, column-oriented matrices.  Only the
upper triangle or the lower triangle, as indicated by \code{"U"} or
\code{"L"} in the \var{uplo} slot, of a symmetric matrix is stored.
The \var{crossprod} function applied to a \var{cscMatrix} produces an
\var{sscMatrix}.
<<crossprod>>=
class(m2 <- crossprod(m1))
as(m2, "matrix")
@

In Statistics we usually define the Cholesky decomposition of a
positive semidefinite, symmetric matrix $\bA$ as an upper triangular
matrix $\bR$ such that $\bA=\bR\trans\bR$ but it is also frequently
defined as a lower triangular matrix $\bL$ such that
$\bA=\bL\bL\trans$.  Naturally, $\bL$ and $\bR$ are transposes of each
other.  On occasion there are advantages to working with the left
factor $\bL$ instead of the right factor $\bR$.

For a sparse, symmetric, semidefinite matrix reordering the columns
(and, correspondingly, the rows) of $\bA$ can change the number of
non-zero elements in the Cholesky factor.  The number of elements in
the Cholesky factor is at least the number of non-zero elements in the
lower triangle of $\bA$.  Additional non-zeros can be generated during
the decomposition.  This process is called ``fill-in''.  Various
methods of determining a fill-minimizing order have been proposed.  We
use a graph-based method implemented in the Metis package.

The \var{chol} function generates the Cholesky decomposition.   When
applied to an \var{sscMatrix} object it defaults to generating a
fill-reducing permutation and the Cholesky factor of the permuted matrix.
<<chols>>=
m3 = chol(m2)
as(m3, "matrix")
m3@perm
@ 
If we set the optional argument \var{pivot} to \var{FALSE},
calculation of the fill-reducing permutation is suppressed.
<<chols2>>=
as(chol(m2, pivot = FALSE), "matrix")
@ 
In this example the fill-reducing permutation reverses the order of
the columns and rows of \var{m2} before taking the decomposition.  It
results in two fewer non-zero elements in the decomposition than when
we suppress the permutation.

\subsection{Symbolic versus numeric computation}
\label{sec:symbolic}

Calculation of the fill-reducing ordering is an example of a symbolic
computation on sparse matrices in that it is based only on the
positions of the non-zero elements, not upon their values.  Frequently
a sparse-matrix computation has both a symbolic phase, which typically
determines the number and positions of the non-zero entries in the
result, and a numeric phase that actually calculates these non-zero
elements.

Evaluation of the profiled log-likelihood or profiled
log-restricted-likelihood requires updating the diagonal blocks in
$\bZ\trans\bZ$ and taking the Cholesky decomposition of the resulting
matrix.  The symbolic phases, including calculation of a fill-reducing
ordering only need to be done once.

Recently Tim Davis has released the LDL package that provides a
concise Cholesky factorization of the form $\bA=\bL\bD\bL\trans$ where
$\bL$ is a unit lower triangular matrix (i.e. all the diagonal
elements are unity) and $\bD$ is diagonal and stored as a single
vector.  This representation is particularly convenient for us because
the diagonal elements (which must be non-zero when $\bA$ is positive
definite) often constitute a substantial portion of the total number
of non-zero elements in the Cholesky factor and, in this
representations, we do not encounter the extra indexing overhead when
accessing these elements.  Also the determinant of $\bA$ (or, for our
purposes, the determinants of leading diagonal submatrices of $\bA$)
can be calculated directly from the diagonal of $\bD$.

As shown in the examples in the next section we can determine
fill-reducing orderings and sizes of Cholesky factors of the matrices
that we wish to decompose by considering first the pairwise
cross-tabulations of the grouping factors.


\begin{equation}
  \label{eq:qtot}
\end{equation}
Generally $p$, the dimension of $\bbeta$, is moderate but $q$, the
dimension of $\bb$, and $n$, the number of observations in $\by$,
which is also the number of rows in $bX$ and $\bZ$, can
be extremely large.  The $n\times q$ matrix $\bZ$ and the $q\times q$
matrices $\bZ\trans\bZ$ and $\bOmega$ can be prohibitively large but
they are sparse (i.e. most of the elements of these matrices are zero)
and, especially in the case of $\bOmega$, highly structured.

The dimension of $\btheta$ is typically very small.  Even in complex,
``real-world'' models applied to data sets where $n$ can be in the
millions, the dimension of $\btheta$ can be as small as three or less.
As described in \citet{bate:debr:2004}, the general problem we
consider is determining the values of the parameters $\bbeta$,
$\btheta$, and $\sigma^2$, in model (\ref{eq:lmeGeneral}) that
optimize the likelihood function, providing the maximum likelihood or
ML estimates, or, more commonly, a modified version called the
restricted likelihood, providing the REML estimates. 

Sparsity in $\bZ$, $\bZ\trans\bZ$ and $\bOmega$ occurs when the random
effects vector $\bb$ is divided into small components associated with
$k$ factors that group the observations and the random effects
associated with each grouping factor are independent between groups
and are i.i.d. (independent and identically distributed) within
groups.  The size of the components in each group is $q_i$ and the
number of distinct levels of each grouping factor is $m_i, i =
1,\dots,k$.  Then $\bOmega$ is block diagonal with $k$ blocks and
block $i$ is itself block diagonal with the diagonal consisting of
$m_i$ repetitions of a $q_i\times q_i$ matrix $\bOmega_i$.  


In common models for such data
\citep{Rnews:Lockwood+Doran+Mccaffrey:2003}, the random effects are
associated with the primary school and the secondary school that the
student attended, which is to say that \var{primary}, with 148
distinct levels, and \var{second}, with 19 distinct levels, are the
grouping factors.  


We see that $\Omega$ has a very simple structure.  The matrix
$\bZ\trans\bZ$ is sparse but not as simple.  To determine its
structure we must first examine $\bZ$ which has 3435 rows
(corresponding to students) and 167 columns.  The first 148 columns
are the indicators of the primary schools.  That is, if student $i$
attended primary school $j$ then
\begin{equation}
  \label{eq:bZik}
  \{\bZ\}_{ik}=
  \begin{cases}
    1&k=j\\
    0&k\ne j
  \end{cases}
\end{equation}
The last 19 columns are the indicators of the secondary schools.
The diagonal elements of $\bZ\trans\bZ$ are the tabulation of the
number of students attending each primary school followed by the
number of students attending each secondary school.  Part of this
tabulation is given in the output of the \var{summary} function above.
Here we give the full tabulation of the secondary schools and part of
the crosstabulation of the primary and secondary schools.
<<xtabs>>=
xtabs(~second, ScotsSec)
@ 

For the purposes of this study each student is classified as attending
only one primary school and one secondary school.  Primary school $i$
can occur with secondary school $j$ but not all of these combinations
do 
one prim
but one of these columns, the column corresponding to the primary school
There is a corresponding grouping of the columns of the matrix $\bZ$
into $m_1\times q_1$ columns associated with the first grouping factor
$\bff_1$, $m_2\times q_2$ columns associated with the second grouping
factor $\bff_2$, and so on.  A given observation, corresponding to a
row of $\bZ$, is associated with exactly one level of $\bff_1$, one
level of $\bff_2$, and so on and all the elements of that row of $\bZ$
will be zero except for the corresponding $q_1$ columns in the first
group, the corresponding $q_2$ columns in the second group, and so on.

This structure in $\bZ$ induces a special structure in $\bZ\trans\bZ$.
It is symmetric and can be divided into $k\times k$ ``outer blocks'',
corresponding to the $k$ grouping factors. The $(i,j)$ outer block, of
size $m_i q_i\times m_j q_j$, is further subdivided into $m_i\times
m_j$ inner blocks, each of size $q_i\times q_j$.  The $(m,n)$'th inner
block in the $(i,j)$'th outer block will be non-zero only if level $m$
of $\bff_i$ occurs with level $n$ of $\bff_j$.  In particular, a
diagonal outer block is block-diagonal with the diagonal consisting of
$m_i$ (possibly different) blocks of size $q_i\times q_i$.

As discussed later, $\bOmega$ has a block diagonal structure and its
determinant is easily evaluated.  The determinant
$\left|\bZ\trans\bZ+\bOmega\right|$ is the product of the elements of
$\dZ$ and $\left|\LXX\right|^2$ is the product of the elements of $\dX$.

The other results given in \citet{bate:debr:2004} can be calculated
from $\bL$ and $\bD$.  To make all this feasible the structure and, in
particular, the sparsity of $\bZ$ and $\bOmega$ must be exploited.

Sparsity in $\bZ$ (and $\bOmega$) occurs when the random effects
vector $\bb$ is divided into small components associated with one or
more factors that group the observations.  It is easiest to illustrate
this with some examples.

\subsection{Examples}
\label{ssec:Examples}

Data on achievement scores of Scottish secondary school students,
as described in \citet{Paterson:1991} and \citet[ch.~18]{MLwiN:2002},
are available as the data set \var{ScotsSec}. 

This data set contains the achievement scores (\var{attain}), some
demographic data (sex and social class), a verbal reasoning score
based on tests taken at entry to secondary school, and the primary and
secondary schools attended for 3435 students.  There are 148 distinct
primary schools and 19 distinct secondary schools represented in these
data.
<<ScotsSec>>=
data(ScotsSec, package = "Matrix")
dim(ScotsSec)
summary(ScotsSec[,c("attain", "verbal", "sex", "primary", "second")])
@ 

In common models for such data
\citep{Rnews:Lockwood+Doran+Mccaffrey:2003}, there would be one or
more coefficients in $\bb$ associated with each school (i.e.
\var{primary} and \var{second} are the grouping factors for the random
effects).  Let's start with a simple model in which the random effects
for the primary school and the random effects for the secondary school
are both simple additive effects.  This means that the first 148
columns of $\bZ$ are indicators of the primary school and the last 19
columns are indicators of the secondary school.  We incorporate the
student's verbal score and sex and their interaction in the fixed
effects part of the model (the model matrix $\bX$ and the coefficients
$\bbeta$).

<<fm1>>=
fm1 = lme(attain~verbal*sex,ScotsSec,list(primary=~1,second=~1))
@ 


                                Scottish                Dallas
Number of levels of factor 1       148                    13471
Number of levels of factor 2        19                     3722
Number of diagonals                167                   138435
Dense storage                     2812                501401786
Sparse ZtZ                         303                   238676
No permutation
 L
 LI
Metis permutation
 L
 LI
Separated Metis perm
 L                                 464 (16.50%)         3763597 (0.75\%)
 LI                               2200 (78.24%)       266944262 (53.24\%)
