\documentclass[12pt]{article}
\usepackage{Sweave,amsmath,amsfonts,bm}
\usepackage[authoryear,round]{natbib}
\bibliographystyle{plainnat}
\DefineVerbatimEnvironment{Sinput}{Verbatim}
{formatcom={\vspace{-1ex}},fontshape=sl,
  fontfamily=courier,fontseries=b, fontsize=\footnotesize}
\DefineVerbatimEnvironment{Soutput}{Verbatim}
{formatcom={\vspace{-1ex}},fontfamily=courier,fontseries=b,%
  fontsize=\footnotesize}
%%\VignetteIndexEntry{Implementation Details}
%%\VignetteDepends{lme4}
\title{Theory and computational methods for mixed models}
\author{Douglas Bates\\Department of Statistics\\%
  University of Wisconsin -- Madison}
\begin{document}
\SweaveOpts{engine=R,eps=FALSE,pdf=TRUE,strip.white=TRUE}
\SweaveOpts{include=FALSE}
\setkeys{Gin}{width=\textwidth}
\newcommand{\trans}{\ensuremath{^\prime}}
\newcommand{\code}[1]{\texttt{\small{#1}}}
\newcommand{\package}[1]{\textsf{\small{#1}}}
<<preliminaries,echo=FALSE,results=hide>>=
options(width=65,digits=5)
library(lme4)
@
\maketitle
\begin{abstract}
  The \code{lme4} package provides R functions to fit and analyze
  linear mixed models, generalized linear mixed models and nonlinear
  mixed models.  In this vignette we describe the formulation of these
  models and the computational approach used to evaluate or
  approximate the log-likelihood of a model/data/parameter value
  combination.
\end{abstract}



\section{Introduction}
\label{sec:intro}

The \code{lme4} package provides \code{R} functions to fit and analyze
linear mixed models, generalized linear mixed models and nonlinear
mixed models.  These models are called \emph{mixed-effects models} or,
more simply, \emph{mixed models} because they incorporate both
\emph{fixed-effects} parameters, which apply to an entire population
or to certain well-defined and repeatable subsets of a population, and
\emph{random effects}, which apply to the particular experimental
units or observational units in the study.  Such models are also
called \emph{multilevel} models because the random effects represent
levels of variation in addition to the per-observation noise term that
is incorporated in common statistical models such as linear
regression models, generalized linear models and nonlinear regression
models.

The three types of mixed models -- linear, generalized linear and
nonlinear -- share common characteristics in that the model is
specified in whole or in part by a \emph{mixed model formula} that
describes a \emph{linear predictor} and a variance-covariance
structure for the random effects.  In the next section we describe
the mixed model formula and the forms of these matrices.  The
following section presents a general formulation of the Laplace
approximation to the log-likelihood of a mixed model.

In subsequent sections we describe computational methods for specific
kinds of mixed models.  In particular, we should how a profiled
log-likelihood for linear mixed models, and for some nonlinear mixed
models, can be evaluated exactly.

\section{Mixed-model formulas}
\label{sec:formula}

The right-hand side of a mixed-model formula, as used in the
\code{lme4} package, consists of one or more random-effects terms and
zero or more fixed-effects terms separated by the `\code{+}' symbol.
The fixed-effects terms generate the fixed-effects model matrix,
$\bm X$, from the data.  The random-effects terms generate the
random-effects model matrix, $\bm{Z}$, and determine the structure of
the relative variance-covariance matrix, $\bm{\Sigma}$.

The model matrices $\bm X$ and $\bm{Z}$ are of size $m\times p$ and
$m\times q$ respectively. For linear and generalized linear mixed
models $m$, the number of rows in $\bm X$ and $\bm Z$, is equal to
$n$, the dimension of the response vector, $\bm y$.  For nonlinear
mixed models $m$ is a multiple of $n$, $m=ns$, where $s$ is the number
of parameters in the underlying nonlinear model.

The dimension of the fixed-effects parameter vector, $\bm{\beta}$, is
$p$ and the dimension of the random effects vector, $\bm{b}$, is $q$.
Together with the matrices $\bm X$ and $\bm{Z}$ these vectors
determine the \emph{linear predictor}
\begin{equation}
  \label{eq:linpred}
  \bm\eta_{\bm{b}}\left(\bm\beta,\bm{b}\right)=\bm X\bm\beta+\bm{Z}\bm{b} .
\end{equation}

The elements of $\bm\beta$ are parameters in the model.  Strictly
speaking, the elements of $\bm b$ are not parameters -- they are
unobserved random variables.  In the models we will consider
$\bm\beta$ and $\bm b$ determine the \emph{conditional mean},
$\bm\mu_{\bm y|\bm b}$, of the observed responses, $\bm y$, through
the linear predictor,
$\bm\eta_{\bm{b}}\left(\bm\beta,\bm{b}\right)$. That is,
\begin{equation}
  \label{eq:conditionalmean}
  \mathsf{E}[\bm y|\bm b]=
  \bm\mu_{\bm y|\bm b}\left(\bm\beta,\bm b\right)=
  \bm\mu(\bm\eta_{\bm{b}}\left(\bm\beta,\bm{b}\right))=
  \bm\mu(\bm X\bm\beta+\bm{Z}\bm{b}) .
\end{equation}

The notation in (\ref{eq:linpred}) and (\ref{eq:conditionalmean})
emphasizes the fact that $\bm\eta$ is being expressed as a function of
$\bm b$ (and $\bm\beta$).  In \S\ref{sec:orthogonal} we define
orthogonal random effects, $\bm u$, and the corresponding expression
$\bm\eta_{\bm u}$ for the linear predictor.  We will need to be able
to distinguish between these two expressions for $\bm\eta$.

The conditional distribution of $\bm y$ given $\bm b$ is completely
determined by the conditional mean $\bm\mu_{\bm y|\bm b}$ and,
possibly, a scale parameter, which we will write as $\sigma^2$.
Furthermore, the conditional distribution depends on $\bm\mu_{\bm
  y|\bm b}$ only through a \emph{discrepancy function}, $d(\bm\mu,\bm
y)$, which defines a ``squared distance'' between $\bm\mu$ and $\bm
y$.  For linear mixed models and for nonlinear mixed models
$d(\bm\mu,\bm y)$ is simply the square of the Euclidean distance
between $\bm\mu$ and $\bm y$, $d(\bm\mu,\bm y)=\left\|\bm
  y-\bm\mu\right\|^2$.  The more general form of the discrepancy
function for generalized linear mixed models is described in
\S\ref{sec:GLMM}.

The observed responses, $\bm y$, can be continuous random variables or
discrete random variables. In either case we will write the
conditional distribution as $f_{\bm y|\bm{b}}$, representing
the condition probability density, when $\bm y$ is
continuous, or the conditional probability mass function, when $\bm y$
is discrete.  It is of the form
\begin{equation}
  \label{eq:conddens}
    f_{\bm y|\bm b}(\bm y|\bm b,\bm\beta,\sigma^2)=
    k(\sigma^2,\bm y)e^{-d\left(\bm\mu_{\bm y|\bm b},\bm y\right)/\left(2\sigma^2\right)} .
\end{equation}

The scale parameter, $\sigma^2$, if it is used in the model, only
determines the variance-covariance of the conditional distribution of
$\bm y$; it does not affect the conditional mean.  Some mixed models,
such as generalized linear mixed models for which the conditional
distribution, $f_{\bm y|\bm b}$, is Bernoulli or binomial or Poisson, do
not have a scale parameter, because the mean of the conditional
distribution completely determines its variance.  In such cases the
conditional distribution can be written $f_{\bm y|\bm b}(\bm y|\bm
b,\bm\beta)= k(\bm y)e^{-d(\bm\mu_{\bm y|\bm b},\bm y)/2}$.

The normalization factor $k(\sigma^2,\bm y)$ depends only on the scale
parameter, $\sigma^2$, if it is used in the model, and the observed
response, $\bm y$.

The marginal distribution of $\bm{b}$ is modelled as a multivariate
Gaussian (or ``normal'') distribution of the form
\begin{equation}
  \label{eq:ranef}
  \bm{b}\sim\mathcal{N}\left(\bm{0},\sigma^2\bm{\Sigma}(\bm{\theta})\right)
\end{equation}
where $\sigma^2$ is the same scale parameter used in
(\ref{eq:conddens}) and the $q\times q$ symmetric,
positive-semidefinite matrix $\bm\Sigma(\bm\theta)$, called the
\emph{relative variance-covariance matrix} of the random effects,
$\bm{b}$, is a function of a parameter $\bm{\theta}$.  The form of
$\bm\theta$ and of $\bm\Sigma(\bm\theta)$ are described in
\S\ref{sec:relvarcov}. (The condition that $\bm\Sigma(\bm\theta)$ is
positive-semidefinite means that
$\bm{v}\trans\bm{\Sigma}(\bm\theta)\bm{v}\ge 0,
\forall\bm{v}\in\mathbb{R}^q$.)

For linear and generalized linear mixed models the model matrix
$\bm X$ is constructed from the data and the fixed-effects terms in
the model formula according to the usual rules for model matrices in
the S language~\citep[Chapter 2]{R:Chambers+Hastie:1992}.  For
nonlinear mixed models these rules are modified somewhat, as described
in \S\ref{sec:nlmmmodelmats}.

\subsection{Random-effects terms}
\label{sec:ranefterms}

A simple random-effects term is of the form
`\code{(}\emph{formula}\code{|}\emph{factor}\code{)}' where
\emph{formula} is a linear model formula and \emph{factor} is an
expression that can be evaluated as a factor.  This factor is called
the \emph{grouping factor} for the term because it partitions the
elements of the conditional mean, $\bm\mu_{\bm y|\bm b}$, into
non-overlapping groups and isolates the effect of certain elements of
the random effects vector, $\bm{b}$, to a specific group.

Typically a random-effects term is enclosed in parentheses so that the
extent of \emph{formula} is clearly defined. The presence of the
vertical bar, \code{|}, which is sometimes read as ``given'' or
``by'', is what distinguishes a random-effects term from a
fixed-effects term.

Let $k$ be the number of random-effects terms in the formula and
$n_i,i=1,\dots,k$ be the number of levels in the $i$th grouping
factor, $\bm{f}_i$.

The linear model formula in the $i$th random-effects term determines
the $m\times q_i$ model matrix $\bm{Z}_i$ according to the usual rules
for model matrices, in the case of linear or generalized linear
models, and according to slightly modified rules, as described in
\S\ref{sec:nlmmmodelmats}, for nonlinear mixed models.

Together $\bm{f}_i$ and $\bm{Z}_i$ determine an \emph{indicator
  interaction matrix} $\tilde{\bm{Z}}_i$, which is the horizontal
concatenation of $q_i$ matrices representing the interaction of a
column of $\bm{Z}_i$ with the matrix of indicators of the levels of
$\bm{f}_i$.  That is, the $m\times n_iq_i$ matrix $\tilde{\bm{Z}}_i$
consists of $q_i$ vertical blocks, each of size $m\times n_i$, whose
nonzeros are in the form of the indicator columns for $\bm{f}_i$.  The
values of the nonzeros in the $j$th vertical block in
$\tilde{\bm{Z}}_i$ are the $j$th column of $\bm{Z}_i$.

In the not-uncommon case that the linear model formula in a
random-effects term is `\code{1}' then $q_i=1$, $\bm{Z}_i=\bm{1}_m$, the
$m\times 1$ matrix all of whose elements are unity, and
$\tilde{\bm{Z}}_i$ is the $m\times n_i$ matrix of indicators of the
levels of $\bm{f}_i$.

Suppose, for example, that we wish to model data where three
observations have been recorded on each of four subjects.  A data
frame containing just the ``subject'' factor with rows ordered by
subject could be constructed as
<<subj>>=
str(dat <- data.frame(subj = gl(4, 3, labels = LETTERS[1:4])))
@
A term of the form \code{(1|subj)} generates the model matrix
$\bm{Z}_i=\bm{1}_{12}$
<<trivialMM>>=
str(Zi <- model.matrix(~1, dat))
@
from the formula \code{1} and the corresponding indicator interaction
matrix, $\tilde{\bm{Z}}_i$
<<extendedMM1,echo=FALSE>>=
with(dat, t(as(subj, "sparseMatrix")))
@
That is, $\tilde{\bm Z}_i$ is the matrix of indicators of the
subject factor.

In the \package{lme4} package it is the transposes of these sparse
model matrices that are stored as compressed column
matrices~\citep[Ch. 2]{davis06:csparse_book} of class
\code{"dgCMatrix"}.  When a matrix of this class is printed, the
systematic zeros are shown as `.'.  In the form of the transpose
<<extendedMM1t>>=
with(dat, as(subj, "sparseMatrix"))
@
the association between the rows and the levels of the grouping factor
is shown more clearly.

For a more general example, assume that each subject was observed at
times 1, 2 and 3 and set the \code{time} variable in the data frame
\code{dat} to be
<<Z1prep>>=
dat$time <- rep(1:3, 4)
@
The first few rows of the data frame are
<<dathead>>=
head(dat, n = 5)
@
The formula \code{time} in the term \code{(time|Subject)} generates
model matrix $\bm{Z}_1$ with $q_i=2$ columns.  Its first few rows are
<<Z1mat>>=
head(Z1 <- model.matrix(~ time, dat), n = 7)
@
and $\tilde{\bm{Z}}_i\trans$ is
<<Z1tilde,echo=FALSE>>=
tim <- ind <- as(dat$subj, "sparseMatrix")
tim@x <- as.numeric(dat$time)
rBind(ind, tim)
@

Finally, the $m\times q$ matrix $\bm{Z}$ is the horizontal concatenation of the
$\tilde{\bm{Z}}_i,i=1,\dots,k$. Thus $q$, the number of columns in $\bm Z$, is
\begin{equation}
  \label{eq:qdim}
  q = \sum_{i=1}^k n_iq_i .
\end{equation}

\subsection{The relative variance-covariance matrix}
\label{sec:relvarcov}

The elements of the random-effects vector $\bm{b}$ are partitioned
into groups in that same way that the columns of $\bm{Z}$ were
partitioned.  That is, they are divided into $k$ groups, corresponding
to the $k$ random-effects terms, and the $i$th such group is
subdivided into $q_i$ groups of $n_i$ elements.  The $q_i$ groups
correspond to the $q_i$ columns of $\bm Z_i$ and the $n_i$ elements in
each group correspond to the levels of the $i$th grouping factor.

This partitioning determines the structure of the variance-covariance
matrix of $\bm{b}$ because we assume that random effects corresponding
to different terms are uncorrelated, as are random effects
corresponding to different levels of the same term.  Furthermore, the
variance-covariance structures of the $n_i$ groups of size $q_i$ from
the $i$th term are identical.

Thus the relative variance-covariance matrix, $\bm\Sigma$, has the form
\begin{equation}
  \label{eq:relvarcov}
  \bm{\Sigma}=
  \begin{bmatrix}
    \tilde{\bm{\Sigma}}_1 & \bm{0} & \hdots & \bm{0}\\
    \bm{0} & \tilde{\bm{\Sigma}}_2 & \hdots & \bm{0}\\
    \vdots & \vdots & \ddots & \vdots \\
    \bm{0} & \bm{0} & \hdots & \tilde{\bm{\Sigma}}_k .
  \end{bmatrix}
\end{equation}
with the $i$th diagonal block, $\tilde{\bm{\Sigma}}_i$, of the form
\begin{equation}
  \label{eq:bSigma}
  \tilde{\bm{\Sigma}}_i =
  \begin{bmatrix}
    \sigma_{1,1}\bm{I}_{n_i}&\sigma_{1,2}\bm{I}_{n_i}& \hdots &\sigma_{1,q_i}\bm{I}_{n_i}\\
    \sigma_{1,2}\bm{I}_{n_i}&\sigma_{2,2}\bm{I}_{n_i}& \hdots &\sigma_{2,q_i}\bm{I}_{n_i}\\
    \vdots    & \vdots    & \ddots & \vdots\\
    \sigma_{1,q_i}\bm{I}_{n_i}&\sigma_{2,q_i}\bm{I}_{n_i}& \hdots &\sigma_{q_i,q_i}\bm{I}_{n_i}\\
  \end{bmatrix}=
  \bm{\Sigma}_i\otimes\bm{I}_{n_i}
\end{equation}
where
\begin{equation}
  \label{eq:Sigmai}
  \bm{\Sigma}_i=
  \begin{bmatrix}
    \sigma_{1,1}&\sigma_{1,2}&\hdots&\sigma_{1,q_i}\\
    \sigma_{1,2}&\sigma_{2,2}&\hdots&\sigma_{2,q_i}\\
    \vdots    & \vdots    & \ddots & \vdots\\
    \sigma_{1,q_i}&\sigma_{2,q_i}&\hdots&\sigma_{q_i,q_i}
  \end{bmatrix}
\end{equation}
is a $q_i\times q_i$ symmetric matrix. (The symbol
$\otimes$ denotes the Kronecker product of matrices, which is a
convenient shorthand for a structure like that shown in
(\ref{eq:bSigma}).)

The matrix $\bm{\Sigma}$ will be positive-semidefinite if all the
symmetric matrices $\bm{\Sigma}_i,i=1,\dots,k$ are
positive-semidefinite.  This occurs if and only if each of the
$\bm{\Sigma}_i$ has an Cholesky factorization of the ``LDL$\trans$'' form where
the left factor ``L'' is a unit lower triangular matrix and ``D'' is a
diagonal matrix with non-negative diagonal elements.

In the ``LDL$\trans$'' form of a variance-covariance matrix the
elements of ``D'' would be on the variance scale.  Because it will be
more convenient to work with elements on the standard deviation scale
we modify this form slightly and write the factorization as
\begin{equation}
  \label{eq:TSST}
  \bm{\Sigma}_i=\bm{T}_i\bm{S}_i\bm{S}_i\bm{T}_i\trans\quad i=1,\dots,k
\end{equation}
where $\bm{T}_i$ is a unit lower triangular matrix of size $q_i\times
q_i$ and $\bm{S}_i$ is a diagonal $q_i\times q_i$ matrix with
non-negative diagonal elements.

We parameterize $\bm{\Sigma}_i$ according to the factorization
(\ref{eq:TSST}) by defining $\bm{\theta}_i$ to be the vector of length
$q_i(q_i+1)/2$ consisting of the diagonal elements of $\bm{S}_i$
followed by the elements in the strict lower triangle of $\bm{T}_i$ in
row-major order.

Finally, let $\bm{\theta}$ be the concatenation of the $\bm{\theta}_i,i=1,\dots,k$.

The unit lower-triangular and non-negative diagonal factors,
$\bm{T}(\bm{\theta})$ and $\bm{S}(\bm{\theta})$, of
$\bm\Sigma(\bm\theta)$ are constructed from the $\bm{T}_i$, $\bm{S}_i$
and $n_i$, $i=1,\dots,k$ according to the pattern of $\bm{\Sigma}$
illustrated in (\ref{eq:relvarcov}) and (\ref{eq:bSigma}).  That is,
$\bm T(\bm\theta)$ (respectively $\bm S(\bm\theta)$) is block-diagonal
with $i$th diagonal block $\tilde{\bm T}_i(\bm\theta)=\bm T(\bm\theta)\otimes{\bm I}_{n_i}$
(respectively $\tilde{\bm S}_i(\bm\theta)=\bm S(\bm\theta)\otimes{\bm I}_{n_i}$).

Although the number of levels of the $i$th factor, $n_i$, can be very
large, the number of columns in $\bm Z_i$, $q_i$, is typically very
small.  Hence the dimension of the parameter $\bm\theta_i$, which
depends on $q_i$ but not on $n_i$, is also small and the structure of
$\bm T_i$ and $\bm S_i$ is often very simple.

Consider our example of three observations on each of four subjects.  For the
random effects term \code{(1|subj)} $q_i=1$ and $\bm{T}_i$, which is a
$1\times 1$ unit lower triangular matrix, must be $\bm I_1$, the $1\times
1$ identity matrix. Hence $\tilde{\bm{T}}_i=\bm{I}_{4}$ and the
factorization $\tilde{\bm\Sigma}_i=\tilde{\bm T}_i\tilde{\bm S}_i\tilde{\bm S}_i
\tilde{\bm T}_i\trans$ reduces to
$\tilde{\bm\Sigma}_i=\tilde{\bm S}_i\tilde{\bm S}_i$.
Furthermore, $\bm{S}_i$ is a $1\times 1$ matrix $[\theta_{i,1}]$,
subject to $\theta_{i,1}\ge 0$, so that
\begin{displaymath}
  \tilde{\bm S}_i=\theta_{i,1}\bm I_4
\end{displaymath}
and
\begin{displaymath}
  \tilde{\bm\Sigma}_i=\tilde{\bm S}_i\tilde{\bm S}_i=\theta_{i,1}^2\bm{I}_4 .
\end{displaymath}

We see that the standard deviations of the elements of $\bm{b}_i$, the
$i$th block of random effects, are all equal to $\theta_{i,1}\sigma$,
where $\sigma$ is the standard deviation of the conditional
distribution of elements of $\bm y$.  Similarly, the variance of the
elements of $\bm{b}_i$, relative to the conditional variance of
elements of $\bm y$, is $\theta_{i,1}^2$.

For the random-effects term \code{(time|subj)}, for which $q_i=2$, let us write
the $2(2+1)/2=3$-dimensional $\bm\theta_i$ as $[a,b,c]\trans$.  Then
\begin{displaymath}
  \bm{S}_i=
  \begin{bmatrix}
    a & 0 \\
    0 & b
  \end{bmatrix}
\end{displaymath}
so that
\begin{displaymath}
  \tilde{\bm S}_i=
  \begin{bmatrix}
    a\bm I_4 & \bm 0\\
    \bm 0 & b\bm I_4
  \end{bmatrix}
\end{displaymath}
and
\begin{displaymath}
  \bm{T}_i=
  \begin{bmatrix}
    1 & 0 \\
    c & 1
  \end{bmatrix}
\end{displaymath}
so that
\begin{displaymath}
  \tilde{\bm T}_i=
  \begin{bmatrix}
    \bm I_4 & \bm 0\\
    c\bm I_4 & \bm I_4
  \end{bmatrix} .
\end{displaymath}
The constraints on $\bm\theta_i$ are $a\ge 0$ and $b\ge 0$.

\subsection{The fill-reducing permutation matrix, $\bm P$}
\label{sec:permutation}

We saw in \S\ref{sec:ranefterms} that the model matrix, $\bm Z$, for the
random effects, $\bm b$, is typically quite sparse (i.e. it is mostly
zeros), as is the matrix
\begin{equation}
  \label{eq:Vthetadef}
  \bm V(\bm\theta)=\bm Z\bm T(\bm\theta)\bm S(\bm\theta) ,
\end{equation}
which is the model matrix for a vector of orthogonal random effects
defined in \S\ref{sec:orthogonal}. We store $\bm Z$ and $\bm
V(\bm\theta)$ as sparse matrices. The matrices $\bm T(\bm\theta)$,
$\bm S(\bm\theta)$ and $\bm\Sigma(\bm\theta)$ are so highly patterned
that there is no need to store them as sparse matrices; we only need
to store the very small dense matrices $\bm T(\bm\theta)_i$ and $\bm
S(\bm\theta)_i$ for $i=1,\dots,k$.

Effective algorithms for sparse matrix decompositions can be
considerably different from the corresponding algorithms for dense
matrices.  Algorithms for sparse matrices frequently
have a symbolic phase, in which the number and position of the nonzero
elements in the result are determined, followed by a numeric phase, in
which the actual numeric values at the nonzero positions are
calculated.  Naturally, the symbolic phase depends only on the pattern
of nonzeros in the matrix or matrices involved in the operation being
considered.

Careful examination of the patterns in $\bm Z$ and $\bm T(\bm\theta)$
shows that the number and locations of the nonzero elements in $\bm
Z\bm T(\bm\theta)$ are the same as those in $\bm Z$.  Multiplication
by $\bm S(\bm\theta)$ will not introduce nonzeros in $\bm
V(\bm\theta)$ where there are zeros in $\bm Z$.  When $\bm\theta$ is
on the boundary this multiplication will convert some of the nonzeros
in $\bm Z$ to zeros but, allowing for the general case, we can assume
the same pattern of nonzeros in $\bm V(\bm\theta)$ as in $\bm Z$.

The \emph{Cholesky decomposition} of sparse, symmetric matrices is one
of most highly optimized sparse matrix algorithms.  It is this
decomposition, as implemented in the CHOLMOD sparse matrix library by
Tim Davis, that forms the basis of the computational methods for mixed
models in the \package{lme4} package.  The key step in our methods of
determining estimates for the parameters in mixed models is creating a
Cholesky factorization of sparse matrices of the form $\bm
V(\bm\theta)\trans\bm W\bm V(\bm\theta) + \bm I$, where $\bm W$ is a
diagonal weight matrix.  It is possible that such factorizations could
be required hundreds or thousands of times during iterative
optimization of the parameters in the model.  Because the dimension,
$q$, of $\bm V(\bm\theta)\trans\bm W\bm V(\bm\theta) + \bm I$ can be
in the tens or hundreds of thousands for some models encountered in
practice, it is crucial that the factorization be performed
efficiently.

Permuting (i.e. reordering) the columns of $\bm V(\bm\theta)$ can
affect, sometimes dramatically, the number of nonzero elements in the
Cholesky factor and, hence, the time required to perform the
factorization.  The number of nonzeros in the factor will always be at
least as large as the number of nonzeros in the lower triangle of $\bm
V(\bm\theta)\trans\bm W\bm V(\bm\theta)+\bm I$ but it can be larger,
in which case we say that the factor has been ``filled-in'' relative
to the original symmetric matrix $\bm V(\bm\theta)\trans\bm W\bm
V(\bm\theta)+\bm I$.  Determining the optimal, fill-minimizing, column
permutation of $\bm V(\bm\theta)$ is an extremely difficult and
time-consuming operation when $q$ is large.  However, some heuristics,
such as the approximate minimal degree ordering algorithm
\citep{Davis:1996}, can be used to determine a near-optimal,
\emph{fill-reducing permutation} much more rapidly.

Because the pattern of the nonzeros in $\bm V(\bm\theta)\trans\bm W\bm
V(\bm\theta)+\bm I$ is the same as the pattern in $\bm Z\trans\bm Z$,
the symbolic analysis to determine the fill-reducing permutation can
be performed on $\bm Z$.  We express the resulting fill-reducing
permutation as the $q\times q$ permutation matrix $\bm P$, which is
formed by applying the permutation to the rows of the $q\times q$
identity matrix, and which has the property $\bm P\trans\bm P=\bm P\bm
P\trans=\bm I_q$.  The absolute value of the determinant of $\bm P$
is unity,
\begin{displaymath}
  1 = |\bm I|=|\bm P\bm P\trans|=|\bm P|\,|\bm P\trans|
  =|\bm P|\,|\bm P|=|\bm P|^2 .
\end{displaymath}
The transpose, $\bm P\trans$, is also a permutation matrix.  It
represents the inverse permutation of the permutation represented by
$\bm P$.

\subsection{Orthogonal random effects}
\label{sec:orthogonal}

For a fixed value of $\bm{\theta}$ we write $\bm{b}$ as
\begin{equation}
  \label{eq:TSPu}
  \bm{b}=\bm{T}(\bm{\theta})\bm{S}(\bm{\theta})\bm P\trans\bm{u}
\end{equation}
where $\bm{u}$ is a vector of \emph{orthogonal random effects} with
distribution
$\bm{u}\sim\mathcal{N}\left(\bm{0},\sigma^2\bm{I}\right)$.  These
random effects are orthogonal in the sense that the elements of $\bm
u$ are uncorrelated.

We note that (\ref{eq:TSPu}) provides the desired distribution for $\bm b$,
$\bm{b}\sim\mathcal{N}(\bm{0},\sigma^2\bm{\Sigma})$, because $\bm b$,
as a linear transformation of $\bm u$, has a multivariate Gaussian
distribution with mean
\begin{displaymath}
  \mathsf{E}[\bm{b}]=\mathsf{E}[\bm T(\bm\theta)\bm S(\bm\theta)\bm P\trans\bm{u}]=
  \bm T(\bm\theta)\bm S(\bm\theta)\bm P\trans\mathsf{E}[\bm{u}]=\bm{0}
\end{displaymath}
and variance-covariance matrix
\begin{displaymath}
  \begin{aligned}[t]
    \mathrm{Var}(\bm{b})=\mathsf{E}[\bm b\bm b\trans]
    &=\bm T(\bm\theta)\bm S(\bm\theta)\bm P\trans\mathsf{E}[\bm u\bm u\trans]\bm P\bm S(\bm\theta)\bm T(\bm\theta)\trans\\
    &=\bm T(\bm\theta)\bm S(\bm\theta)\bm P\trans\mathrm{Var}(\bm u)\bm P\bm S(\bm\theta)\bm T(\bm\theta)\trans\\
    &=\sigma^2\bm T(\bm\theta)\bm S(\bm\theta)\bm P\trans\bm P\bm S(\bm\theta)\bm T(\bm\theta)\trans\\
    &=\sigma^2\bm T(\bm\theta)\bm S(\bm\theta)\bm S(\bm\theta)\bm T(\bm\theta)\trans\\
    &=\sigma^2\bm\Sigma(\bm\theta) .
  \end{aligned}
\end{displaymath}
A similar calculation shows that $\bm P\trans\bm
u\sim\mathcal{N}\left(\bm{0},\sigma^2\bm{I}\right)$ so $\bm
P\trans\bm u$ is also a vector of orthogonal random effects.

Because $\bm T(\bm\theta)$ is a unit lower triangular matrix its
determinant, $|\bm T(\bm\theta)|$, which is the product of the
diagonal elements in the case of a triangular matrix, is unity.  Hence
$\bm T^{-1}(\bm\theta)$ always exists. When $\bm\theta$ is not on the
boundary of its constraint region, so that all the diagonal elements of
$\bm S(\bm\theta)$ are positive, then $\bm S^{-1}(\bm\theta)$ exists,
as does
\begin{equation}
  \label{eq:Sigmainv}
  \bm\Sigma^{-1}(\bm\theta)=
  \bm T^{-1}(\bm\theta)\trans\bm S^{-1}(\bm\theta)\bm
  S^{-1}(\bm\theta)\bm T^{-1}(\bm\theta) .
\end{equation}

We say that $\bm S(\bm\theta)$ and $\bm\Sigma(\bm\theta)$ are
\emph{non-singular} when $\bm\theta$ is not on the boundary.  When
$\bm\theta$ is on the boundary, meaning that one or more of the
diagonal elements of the $\bm S_i,i=1,\dots,k$ is zero,
$\bm\Sigma(\bm\theta)$ is said to be a singular, or degenerate,
variance-covariance matrix.  There will be non-trivial linear
combinations, $\bm v\trans\bm b$ where $\bm v\ne\bm 0$, such that
$\mathrm{Var}(\bm v\trans\bm b)=\sigma^2\bm v\trans\bm\Sigma\bm v=0$.

For a non-singular $\bm\Sigma(\bm\theta)$ we can express $\bm u$ as
\begin{equation}
  \label{eq:btou}
  \bm u=\bm P\bm S^{-1}(\bm\theta)\bm T^{-1}(\bm\theta)\bm b
\end{equation}
and the quadratic form, $\bm b\trans\bm\Sigma^{-1}(\bm\theta)\bm b$, in
the exponent of the multivariate Gaussian density function becomes
\begin{equation}
  \label{eq:bquad}
  \begin{aligned}[t]
  \bm b\trans\bm\Sigma^{-1}(\bm\theta)\bm b&=
  \bm b\trans\bm T^{-1}(\bm\theta)\trans\bm S^{-1}(\bm\theta)
  \bm S^{-1}(\bm\theta)\bm T^{-1}(\bm\theta)\bm b\\
  &=\bm b\trans\bm T^{-1}(\bm\theta)\trans\bm S^{-1}(\bm\theta)\bm P\trans
  \bm P\bm S^{-1}(\bm\theta)\bm T^{-1}(\bm\theta)\bm b\\
  &=\bm u\trans\bm u .
  \end{aligned}
\end{equation}

Because the conditional mean, $\bm\mu_{\bm y|\bm b}$, and the discrepancy
function, $d(\bm{\mu},\bm y)$, depend on $\bm{b}$ only through the
linear predictor and because we can rewrite the linear predictor as a
function of $\bm{\beta}$ and $\bm{u}$
\begin{equation}
  \label{eq:linpredu}
  \bm X\bm\beta+\bm{Z}\bm{b}=
  \bm X\bm\beta+\bm{Z}\bm{T}(\bm\theta)\bm{S}(\bm\theta)\bm P\trans\bm{u}=
  \bm X\bm\beta+\bm{V}(\bm\theta)\bm P\trans\bm{u}=
  \bm\eta_{\bm u}(\bm\beta,\bm\theta,\bm u)
\end{equation}
we can express the discrepancy as a function of $\bm{\beta}$ and
$\bm{u}$.  The two forms of the discrepancy are
\begin{equation}
  \label{eq:db}
  \begin{aligned}[t]
    d_{\bm{b}}(\bm{b},\bm\beta,\bm y)
    &=d(\bm\mu(\bm X\bm\beta+\bm{Z}\bm{b}),\bm y)\\
    &=d(\bm\mu(\bm\eta_{\bm b}(\bm\beta,\bm{b})),\bm y)
  \end{aligned}
\end{equation}
and
\begin{equation}
  \label{eq:db}
  \begin{aligned}
  d_{\bm{u}}(\bm{u},\bm\beta,\bm\theta,\bm y)
  &=d(\bm\mu(\bm X\bm\beta+\bm{V}(\bm\theta)\bm P\trans\bm{u}),\bm y)\\
  &=d(\bm\mu(\bm\eta_{\bm u}(\bm\beta,\bm\theta,\bm u)),\bm y) .
  \end{aligned}
\end{equation}
Note that $d_{\bm{u}}$ depends on $\bm\theta$ but $d_{\bm{b}}$ does not.

In the next section we will evaluate an integral with respect to
$\bm{b}$ by changing the variable of integration from $\bm b$ to
$\bm{u}$.  When performing the change of variable on an integral with
respect to a vector we must incorporate the absolute value of the determinant of the
Jacobian of the transformation.  For the transformation
$\bm{b}=\bm T(\bm\theta)\bm S(\bm\theta)\bm P\trans\bm{u}$ the
determinant of the Jacobian is
\begin{displaymath}
  \left|\frac{d\bm{b}}{d\bm{u}}\right|=
  \left|\bm{T}(\bm\theta)\bm{S}(\bm\theta)\bm P\trans\right|=
  \left|\bm{T}(\bm\theta)\right|\,\left|\bm{S}(\bm\theta)\right|\,\left|\bm
    P\trans\right|=\pm
  \left|\bm{S}(\bm\theta)\right|
\end{displaymath}
with absolute value $\left|\bm{S}(\bm\theta)\right|$.
This determinant also appears as
\begin{equation}
  \label{eq:Sigmadet}
  |\bm\Sigma(\bm\theta)|^{1/2}=
  \sqrt{|\bm T(\bm\theta)|^2|\bm S(\bm\theta)|^2}=
  |\bm{S}(\bm\theta)|
\end{equation}
in the multivariate Gaussian density of $\bm b$.


\section{Evaluating the likelihood}
\label{sec:log-likelihood}

If the distribution of $\bm y$ is continuous, the likelihood of the
parameters, $\bm{\beta}$, $\bm{\theta}$ and $\sigma^2$, given the
observed data, $\bm y$,
is equal to the
marginal density of $\bm y$ given the parameters.  If the
distribution of $\bm y$ is discrete, the likelihood is equal to the
marginal probability mass function of $\bm y$ given the parameters.

In either case we can write the likelihood as
\begin{equation}
  \label{eq:marginal}
  L(\bm{\beta},\bm{\theta},\sigma^2|\bm y)=\int_{\bm{b}}
  f_{\bm y|\bm{b}}(\bm y|\bm{b},\bm{\beta},\sigma^2)\,f_{\bm{b}}(\bm{b}|\bm\theta,\sigma^2)\;
  d\bm{b}
\end{equation}
where $f_{\bm y|\bm{b}}(\bm y|\bm{b},\bm{\beta},\sigma^2)$, defined
in (\ref{eq:conddens}), is the conditional density or the conditional
probability mass function of $\bm y$, as appropriate.

The unconditional distribution of $\bm{b}$,
$\bm{b}\sim\mathcal{N}\left(\bm{0},\sigma^2\bm\Sigma(\bm\theta)\right)$,
has density
\begin{equation}
  \label{eq:bdensity}
  f_{\bm{b}}(\bm{b}|\bm\theta,\sigma^2)=\frac{e^{-\bm{b}\trans\bm{\Sigma}^{-1}(\bm\theta)\bm{b}/(2\sigma^2)}}
  {\left(2\pi\sigma^2\right)^{q/2}|\bm\Sigma(\bm\theta)|^{1/2}} .
\end{equation}
Substituting (\ref{eq:conddens}) and (\ref{eq:bdensity}) into
(\ref{eq:marginal}) and changing the variable of integration
from $\bm b$ to $\bm u$ produces
\begin{equation}
  \label{eq:likelihoodb}
  \begin{aligned}
    L(\bm\beta,\bm\theta,\sigma^2|\bm y)
    &=\int_{\bm b}\frac{k(\sigma^2,\bm y)
    e^{-(d_{\bm b}(\bm b,\bm\beta,\bm y)+ \bm b\trans\bm\Sigma^{-1}\bm b)/(2\sigma^2)}}
    {\left(2\pi\sigma^2\right)^{q/2}|\bm{\Sigma}|^{1/2}}\,d\bm{b}\\
    &=\int_{\bm{u}}\frac{k(\sigma^2,\bm y)
    e^{-(d_{\bm{u}}(\bm{u},\bm\beta,\bm\theta,\bm y)+\bm u\trans\bm u)/(2\sigma^2)}}
    {\left(2\pi\sigma^2\right)^{q/2}}\,d\bm u\\
    &=\int_{\bm{u}}\frac{k(\sigma^2,\bm y)
    e^{-\delta(\bm{u}|\bm\beta,\bm\theta,\bm y)/(2\sigma^2)}}
    {\left(2\pi\sigma^2\right)^{q/2}}\,d\bm u .
  \end{aligned}
\end{equation}

The function
$\delta(\bm u|\bm\beta,\bm\theta,\bm y)=d_{\bm u}(\bm
u,\bm\beta,\bm\theta,\bm y)+\bm u\trans\bm u$ is called
the \emph{penalized discrepancy} function. It is composed of a
``squared distance'' between the observed data, $\bm y$, and the conditional
mean, $\bm\mu(\bm\eta_{\bm u}(\bm\beta,\bm\theta,\bm u))$ plus a ``penalty'', $\bm
u\trans\bm u$, on the size of $\bm u$.

Note that the likelihood (\ref{eq:likelihoodb}) can be evaluated even
when $\bm\theta$ on the boundary and $\bm\Sigma(\bm\theta)$ is
singular if it is expressed as the integral with respect to $\bm u$.

\subsection{The Laplace approximation}
\label{sec:Laplace}

In later sections we will see that, for the models that we are
considering, it is relatively straightforward to
determine the minimizer of the penalized discrepancy
\begin{equation}
  \label{eq:tildeu}
  \tilde{\bm{u}}(\bm\beta,\bm\theta,\bm y)=
  \arg\min_{\bm{u}}\delta(\bm{u}|\bm{\beta},\bm{\theta},\bm y) ,
\end{equation}
either directly, as the solution to a penalized linear least squares
problem, or through an iterative algorithm in which each iteration
requires the solution of a penalized linear least squares problem.
Because the value that minimizes the penalized discrepancy
will maximize the conditional density of $\bm u$ given $\bm y$,
\begin{displaymath}
  f_{\bm{u}|\bm y}(\bm{u}|\bm\beta,\bm\theta,\bm y)=
  \frac{k(\sigma^2,\bm y)e^{-\delta(\bm{u},\bm\beta,\bm\theta,\bm y)
      /(2\sigma^2)}}{(2\pi\sigma^2)^{q/2}} ,
\end{displaymath}
$\tilde{\bm{u}}(\bm\beta,\bm\theta,\bm y)$ is called the
\emph{conditional mode} of $\bm{u}$ given $\bm{\beta}$, $\bm{\theta}$
and $\bm y$.  (The conditional density depends on $\sigma^2$ as well
as the other parameters but the conditional mode does not.)

Near the conditional mode, the penalized discrepancy has a quadratic approximation
\begin{equation}
  \label{eq:quadapprox}
  \delta(\bm{u}|\bm{\beta},\bm{\theta},\bm y)\approx
  \delta(\tilde{\bm{u}}|\bm{\beta},\bm{\theta},\bm y)+
  \left(\bm u-\tilde{\bm u}\right)\trans
  \frac{\left.\nabla_{\bm{u}}^2 \delta(\bm u|\bm\beta,\bm\theta,\bm y)
  \right|_{\bm u=\tilde{\bm u}}}{2}
  \left(\bm{u}-\tilde{\bm{u}}\right)
\end{equation}
where $\nabla_{\bm{u}}^2\delta(\bm{u}|\bm{\beta},\bm{\theta},\bm y)$
denotes the symmetric $q\times q$ \emph{Hessian} matrix of the scalar
function $\delta(\bm u|\bm\beta,\bm\theta,\bm y)$.  The $(j,k)$th
element of
$\nabla_{\bm{u}}^2\delta(\bm{u}|\bm{\beta},\bm{\theta},\bm y)$ is
\begin{equation}
  \label{eq:Hessiandef}
  \frac{\partial^2\delta(\bm u|\bm\beta,\bm\theta,\bm y)}
  {\partial u_j\partial u_k} .
\end{equation}

One of the conditions for $\tilde{\bm{u}}(\bm\beta,\bm\theta,\bm y)$
to be the minimizer of the penalized discrepancy is that the Hessian
at $\tilde{\bm u}$ must be positive definite.  We can, therefore, evaluate the
Cholesky factor $\bm L(\bm\beta,\bm\theta,\bm y)$, which is the
$q\times q$ lower triangular matrix with positive diagonal elements
that satisfies
\begin{equation}
  \label{eq:CholeskyFactor}
  \bm L(\bm\beta,\bm\theta,\bm y)\bm L(\bm\beta,\bm\theta,\bm y)\trans =
  \frac{\left.\nabla_{\bm{u}}^2
      \delta(\bm{u}|\bm{\beta},\bm{\theta},\bm y)
    \right|_{\bm{u}=\tilde{\bm{u}}(\bm\beta,\bm\theta,\bm y)}}{2} .
\end{equation}

After substituting (\ref{eq:CholeskyFactor}) into the quadratic
approximation (\ref{eq:quadapprox}) and approximation
(\ref{eq:quadapprox}) into expression (\ref{eq:likelihoodb}) for the
likelihood, $L(\bm{\beta},\bm{\theta},\sigma^2|\bm y)$, the only part
of the integrand that will depend on $\bm{u}$ will be the quadratic
term in the exponent.  To evaluate an integral like this, say,
\begin{displaymath}
  I=\int_{\bm{u}}\frac{e^{-(\bm{u}-\tilde{\bm{u}})\trans\bm{L}
      \bm{L}\trans(\bm{u}-\tilde{\bm{u}})/(2\sigma^2)}}
  {\left(2\pi\sigma^2\right)^{q/2}}\,d\bm{u}
\end{displaymath}
we change the variable variable of integration from $\bm u$ to
\begin{equation}
  \label{eq:vLu}
  \bm{v}=\bm{L}\trans(\bm{u}-\tilde{\bm{u}})/\sigma .
\end{equation}
The determinant of the Jacobian of the transformation (\ref{eq:vLu}) is
\begin{displaymath}
  \left|\frac{d\bm{v}}{d\bm{u}}\right|=\frac{|\bm{L}|}{\sigma^q} .
\end{displaymath}
so, after the change of variable, $I$ becomes a multiple of the
integral of the standard $q$-variate Gaussian density
\begin{equation}
  \label{eq:reducedint}
  I=\int_{\bm{v}}\frac{e^{-\bm{v}\trans\bm{v}/2}}{\left(2\pi\right)^{q/2}}
  \frac{d\bm{v}}{|\bm{L}|}= \frac{1}{|\bm{L}|}
  \int_{\bm{v}}\frac{e^{-\bm{v}\trans\bm{v}/2}}{\left(2\pi\right)^{q/2}}\,d\bm{v}
\end{equation}
Thus, $I=|\bm L|^{-1}$.

Returning to expression (\ref{eq:likelihoodb}), we can now express the
Laplace approximation to the likelihood function or, as more commonly
used as the optimization criterion when determining maximum likelihood
estimates, the log-likelihood,
\begin{equation}
  \label{eq:loglikelihood}
  \ell(\bm{\beta},\bm{\theta},\sigma^2|\bm y)=
  \log L(\bm{\beta},\bm{\theta},\sigma^2|\bm y) .
\end{equation}
(Because the logarithm function is monotonic, the maximizer of the
log-likelihood function also maximizes the likelihood
function. Generally the quadratic approximation to the log-likelihood
is a better approximation than is the quadratic approximation to the
likelihood.)

On the deviance scale (twice the negative log-likelihood) the Laplace
approximation is
\begin{equation}
  \label{eq:LaplaceDev}
  -2\ell(\bm{\beta},\bm{\theta},\sigma^2|\bm y)\approx
  -2\log[k(\sigma^2,\bm y)]+
  \frac{\delta(\tilde{\bm u}|\bm\beta,\bm\theta,\bm y)}{\sigma^2}
  +2\log|\bm L(\bm\beta,\bm\theta,\bm y)| .
\end{equation}
Expression (\ref{eq:LaplaceDev}) will be an exact expression for the
log-likelihood, not just an approximation, whenever the
penalized discrepancy $\delta(\bm{u}|\bm{\beta},\bm{\theta},\bm y)$
is a quadratic function of $\bm{u}$.

\section{Linear mixed models}
\label{sec:lmm}

A linear mixed model can be expressed as
\begin{equation}
  \label{eq:lmmDef}
  \bm y=\bm X\bm{\beta}+\bm{Z}\bm{b}+\bm{\epsilon},\quad
  \bm{\epsilon}\sim\mathcal{N}\left(\bm{0},\sigma^2\bm{I}\right),\quad
  \bm{b}\sim\mathcal{N}\left(\bm{0},\sigma^2\bm{\Sigma}\right),\quad
  \bm{\epsilon}\perp\bm{b}
\end{equation}
where the symbol $\perp$ denotes independence of random variables.
The conditional distribution of $\bm y$ given $\bm b$ is
\begin{equation}
  \label{eq:lmmCondDist}
  f_{\bm y|\bm b}(\bm y|\bm b,\bm\beta,\sigma^2)=
  \left(2\pi\sigma^2\right)^{-n/2}
  e^{-\left\|\bm X\bm\beta+\bm Z\bm b-\bm y\right\|^2/(2\sigma^2)}
\end{equation}
with conditional mean
$\bm\mu_{\bm y|\bm b}(\bm\beta,\bm b)=\bm X\bm\beta+\bm Z\bm
b=\bm\eta_{\bm b}(\bm\beta,\bm b)$.
Comparing (\ref{eq:lmmCondDist}) to (\ref{eq:conddens}) shows that
\begin{align}
  \label{eq:lmmNormFactor}
  k(\sigma^2,\bm y)&=\left(2\pi\sigma^2\right)^{-n/2}\\
  \label{eq:lmmInvLink}
  \bm\mu(\bm\eta)&=\bm\eta\\
  \label{eq:lmmDisc}
  d(\bm{\mu},\bm y)&=\left\|\bm{\mu}-\bm y\right\|^2
\end{align}

The penalized discrepancy for this model is
\begin{multline}
  \label{eq:lmmdelta}
  \delta(\bm u|\bm\beta,\bm\theta,\bm y)\\
  \begin{aligned}
    &=d_{\bm u}(\bm u,\bm\beta,\bm\theta,\bm y)+\bm u\trans\bm u\\
    &=d\left(\bm\mu(\bm\eta_{\bm u}(\bm\beta,\bm\theta,\bm u),\bm y\right)+\bm{u}\trans\bm{u}\\
    &=\left\|\bm\eta_{\bm u}(\bm\beta,\bm\theta,\bm u)-\bm y\right\|^2+\bm{u}\trans\bm{u}\\
    &=\left\|\bm{V}\bm P\trans\bm{u}+\bm X\bm{\beta}-\bm y\right\|^2+\bm{u}\trans\bm{u}\\
    &=\left\|\begin{bmatrix}\bm V\bm P\trans & \bm X & \bm y\end{bmatrix}
      \begin{bmatrix}\bm{u} \\ \bm{\beta} \\ -1 \end{bmatrix}\right\|^2+\bm u\trans\bm u\\
    &=\begin{bmatrix}\bm u\trans & \bm\beta\trans & -1\end{bmatrix}
    \begin{bmatrix}
      \bm P\bm V\trans\bm V\bm P\trans+\bm I & \bm P\bm
      V\trans\bm X & \bm P\bm V\trans\bm y\\
      \bm X\trans\bm V\bm P\trans&\bm X\trans\bm X&\bm X\trans\bm y\\
      \bm y\trans\bm V\bm P\trans&\bm y\trans\bm X&\bm y\trans\bm y
    \end{bmatrix}
    \begin{bmatrix}\bm u \\ \bm\beta \\ -1\end{bmatrix} .
  \end{aligned}
\end{multline}
(To save space we suppressed the dependence of $\bm V(\bm\theta)$ on
$\bm\theta$.)
In (\ref{eq:lmmdelta}) it is obvious that $\delta(\bm
u|\bm\beta,\bm\theta,\bm y)$ is a quadratic function of $\bm u$, which
means that expression (\ref{eq:LaplaceDev}) provides an exact
evaluation of the log-likelihood, not just an approximation.
Furthermore, the Hessian
\begin{equation}
  \label{eq:hessian}
  \nabla_{\bm{u}}^2\delta(\bm{u}|\bm{\beta},\bm{\theta},\bm y)=
  2\left(\bm P\bm V(\bm\theta)\trans\bm V(\bm\theta)\bm P\trans+\bm I\right) ,
\end{equation}
is positive definite and depends only on $\bm\theta$.
The Cholesky factor $\bm L(\bm\beta,\bm\theta,\bm y)$, defined in
(\ref{eq:CholeskyFactor}) and used in the log-likelihood evaluation
(\ref{eq:LaplaceDev}), becomes $\bm L(\bm\theta)$ and is the lower
triangular matrix satisfying
\begin{equation}
  \label{eq:lmmCholeskyFactor}
  \begin{aligned}
    \bm L(\bm\theta)\bm L(\bm\theta)
    &=\bm P\bm V(\bm\theta)\trans\bm V(\bm\theta)\bm P\trans+\bm I\\
    &=\bm P\left(\bm V(\bm\theta)\trans\bm V(\bm\theta)+\bm
      I\right)\bm P\trans .
  \end{aligned}
\end{equation}


Determining the conditional mode, $\tilde{\bm
  u}(\bm\beta,\bm\theta,\bm y)$ as the solution to
\begin{equation}
  \label{eq:conditionalMode}
  \bm L(\bm\theta)\bm L(\bm\theta)\trans
  \tilde{\bm u}(\bm\beta,\bm\theta,\bm y)=
  \bm{V}(\bm\theta)\trans\left(\bm y-\bm X\bm{\beta}\right)
\end{equation}
is straightforward once the Cholesky factor, $\bm L(\bm\theta)$, is
available, thereby providing all the information needed to evaluate
the log-likelihood from (\ref{eq:LaplaceDev}).

However, we can take advantage of the fact that
$\delta(\bm{u}|\bm{\beta},\bm{\theta},\bm y)$ is a quadratic function
of both $\bm{u}$ and $\bm{\beta}$ to minimize $\delta$ with respect to
$\bm{u}$ and $\bm{\beta}$ simultaneously.  Given $\bm{\theta}$ we, in
effect, evaluate a Cholesky factor for
\begin{displaymath}
  \begin{bmatrix}
    \bm P(\bm V(\bm\theta)\trans\bm V(\bm\theta)+\bm I)\bm P\trans & \bm P\bm
    V\trans\bm X & \bm P\bm V(\bm\theta)\trans\bm y\\
    \bm X\trans\bm V(\bm\theta)\bm P\trans&\bm X\trans\bm X&\bm X\trans\bm y\\
    \bm y\trans\bm V(\bm\theta)\bm P\trans&\bm y\trans\bm X&\bm
    y\trans\bm y
  \end{bmatrix} .
\end{displaymath}
Because this factorization will involve combinations of sparse
and dense matrices, we do it in stages, beginning with the evaluation
of the sparse Cholesky factor, $\bm L(\bm\theta)$, from
(\ref{eq:lmmCholeskyFactor}).  Next, solve for the $q\times p$ dense
matrix $\bm R_{VX}(\bm\theta)$ and the $q$-vector $\bm r_{Vy}(\bm\theta)$ in
\begin{align}
  \label{eq:lmmRVX}
  \bm L(\bm\theta)\bm R_{VX}(\bm\theta)&=\bm P\bm V(\bm\theta)\trans\bm X\\
  \label{eq:lmmrVy}
  \bm L(\bm\theta)\bm r_{Vy}(\bm\theta)&=\bm P\bm V(\bm\theta)\trans\bm y
\end{align}
followed by the $p\times p$ upper triangular dense Cholesky factor
$\bm R_X(\bm\theta)$ satisfying
\begin{equation}
  \label{eq:lmmRX}
  \bm R_X(\bm\theta)\trans\bm R_X(\bm\theta)=
  \bm X\trans\bm X-\bm R_{VX}(\bm\theta)\trans\bm R_{VX}(\bm\theta)
\end{equation}
and the $p$-vector $\bm r_{Xy}(\bm\theta)$ satisfying
\begin{equation}
  \label{eq:lmmrXy}
  \bm R_X(\bm\theta)\trans\bm r_{Xy}(\bm\theta)=
  \bm X\trans\bm y-\bm R_{VX}(\bm\theta)\bm r_{Vy}(\bm\theta) .
\end{equation}
Finally, evaluate the scalar
\begin{equation}
  \label{eq:lmmr}
  r(\bm\theta)=\sqrt{\|\bm y\|^2-\|\bm r_{Xy}(\bm\theta)\|^2
    -\|\bm r_{Vy}(\bm\theta)\|^2} .
\end{equation}

Using these factors we can write
\begin{multline}
  \label{eq:deltaReduced}
  \delta(\bm u|\bm\beta,\bm\theta,\bm y)\\
  \begin{aligned}
    &=\left\|
      \begin{bmatrix}
        \bm L(\bm\theta)\trans & \bm R_{VX}(\bm\theta) &\bm r_{Vy}(\bm\theta)\\
        \bm 0 & \bm R_X(\bm\theta)\trans & \bm r_{Xy}(\bm\theta)\\
        \bm 0 &\bm 0 & r(\bm\theta)
      \end{bmatrix}
      \begin{bmatrix}
        \bm u\\
        \bm\beta\\
        -1
      \end{bmatrix}
    \right\|^2\\
    &=r^2(\bm\theta)+\left\|\bm{R}_X(\bm\theta)\bm\beta-\bm r_{Xy}(\bm\theta)\right\|^2+
    \left\|\bm{L}(\bm\theta)\trans\bm u+\bm
      R_{VX}(\bm\theta)\trans\bm\beta-\bm r_{Vy}(\bm\theta)\right\|^2\\
    &=r^2(\bm\theta)+\left\|\bm R_X(\bm\theta)\left(\bm\beta-
        \widehat{\bm\beta}(\bm\theta)\right)\right\|^2+
    \left\|\bm{L}(\bm\theta)\trans\left(\bm u-\widehat{\bm u}(\bm\theta)\right)\right\|^2
  \end{aligned}
\end{multline}
where $\widehat{\bm\beta}(\bm\theta)$, the conditional estimate of $\bm{\beta}$
given $\bm{\theta}$, and $\widehat{\bm u}(\bm\theta)$, the conditional mode of
$\bm{u}$ given $\bm{\theta}$ and $\widehat{\bm{\beta}}(\bm{\theta})$, are
the solutions to
\begin{align}
  \label{eq:conditionalbeta}
  \bm R_X(\bm\theta)\widehat{\bm\beta}(\bm\theta)&=\bm r_{Xy}(\bm\theta)\\
  \label{eq:conditionalu}
  \bm{L}(\bm\theta)\trans\widehat{\bm u}(\bm\theta)
  &=\bm r_{Vy}(\bm\theta)-\bm R_{VX}(\bm\theta)\trans\widehat{\bm\beta}(\bm\theta) .
\end{align}
Furthermore, the minimum of the penalized discrepancy, conditional on
$\bm{\theta}$, is
\begin{equation}
  \label{eq:mindisc}
  \min_{\bm u}\delta(\bm u|\widehat{\bm\beta}(\bm\theta),\bm\theta,
  \bm y)=r^2(\bm\theta) .
\end{equation}


The deviance function,
$-2\ell(\bm{\beta},\bm{\theta},\sigma^2|\bm y)$, evaluated at the
conditional estimate, $\widehat{\bm{\beta}}(\bm{\theta})$, is
\begin{equation}
  \label{eq:reducedDev}
  -2\ell(\widehat{\bm{\beta}}(\bm{\theta}),\bm{\theta},\sigma^2|\bm y)
  =n\log\left(2\pi\sigma^2\right)+\frac{r^2(\bm{\theta})}{\sigma^2}+2\log|\bm{L}(\bm{\theta})| .
\end{equation}
Differentiating
$-2\ell(\widehat{\bm{\beta}}(\bm{\theta}),\bm{\theta},\sigma^2|\bm y)$
as a function of $\sigma^2$ and setting the derivative to zero
provides the conditional estimate
\begin{equation}
  \label{eq:mlsigmasq}
  \widehat{\sigma^2}(\bm{\theta})=\frac{r^2(\bm\theta)}{n} .
\end{equation}
Substituting this estimate into (\ref{eq:reducedDev}) provides the
\emph{profiled deviance} function
\begin{equation}
  \label{eq:profiledDeviance}
  \begin{aligned}
  -2\ell(\widehat{\bm{\beta}}(\bm{\theta}),\bm{\theta},\widehat{\sigma^2}(\bm{\theta})|\bm y)
  &=n\log\left(\frac{2\pi r^2(\bm{\theta})}{n}\right) + n + 2\log|\bm{L}(\bm{\theta})|\\
  &=n\left[1+\log\left(\frac{2\pi}{n}\right)\right]+n\log
  r^2(\bm{\theta})+2\log|\bm{L}(\bm{\theta})| .
  \end{aligned}
\end{equation}
That is, the maximum likelihood estimate (mle) of $\bm{\theta}$ is
\begin{equation}
  \label{eq:lmmThetaMle}
  \widehat{\bm\theta}=\arg\min_{\bm\theta}
  n\left[1+\log\left(\frac{2\pi}{n}\right)\right]+n\log
  r^2(\bm{\theta})+2\log|\bm{L}(\bm{\theta})| .
\end{equation}
The mle's of the other parameters are determined from
$\widehat{\bm\theta}$ using (\ref{eq:mlsigmasq}) and
(\ref{eq:conditionalbeta}).  The conditional modes of the orthogonal
random effects, $\widehat{\bm{u}}(\widehat{\bm\theta})$, evaluated using
(\ref{eq:conditionalu}), and the corresponding conditional modes of the
untransformed random effects,
\begin{equation}
  \label{eq:lmmBLUP}
  \widehat{\bm{b}}(\widehat{\bm\theta})=
  \bm{T}(\widehat{\bm\theta})
  \bm{S}(\widehat{\bm\theta})
  \widehat{\bm{u}}(\widehat{\bm\theta}) ,
\end{equation}
are called the \emph{empirical Best Linear Unbiased Predictors} (eBLUPs) of the
random effects.

The three terms in the objective function being minimized in
(\ref{eq:lmmThetaMle}) are, respectively, a constant,
$n\left[1+\log\left(2\pi/n\right)\right]$, a measure of the fidelity
of the fitted values to the observed data, $n\log r^2(\bm\theta)$,
and a measure of model complexity, $2\log|\bm{L}(\bm\theta)|$.
Thus we can consider maximum likelihood estimation of the parameters in
a linear mixed model to be balancing fidelity to the data against model
complexity by an appropriate choice of $\bm\theta$.

\subsection{REML estimates}
\label{sec:REML}

The maximum likelihood estimate of $\sigma^2$, $\widehat{\sigma^2}=r^2/n$,
is the penalized residual sum of squares divided by the number of
observations.  It has a form like the maximum likelihood estimate of
the variance from a single sample,
$\widehat{\sigma^2}=\sum_{i=1}^n(y_i-\bar{y})^2/n$ or the maximum
likelihood estimate of the variance in a linear regression model with
$p$ coefficients in the predictor,
$\widehat{\sigma^2}=\sum_{i=1}^n(y_i-\hat{y}_i)^2/n$.

Generally these variance estimates are not used because they are
biased downward.  This is, on average they will underestimate the
variance in the model.  Instead we use
$\widehat{\sigma^2}_R=\sum_{i=1}^n(y_i-\bar{y})^2/(n-1)$ for the
variance estimate from a single sample or
$\widehat{\sigma^2}_R=\sum_{i=1}^n(y_i-\hat{y}_i)^2/(n-p)$ for the
variance estimate in a linear regression model.  These estimates are
based on the residuals, $y_i-\hat{y}_i, i=1,\dots,n$ which satisfy $p$
linear constraints and thus are constrained to an $(n-p)$-dimensional
subspace of the $n$-dimensional sample space.  In other words, the
residuals have only $n-p$ degrees of freedom.

In a linear mixed model we often prefer to estimate the variance
components, $\sigma^2$ and $\bm{\Sigma}$, according to the
\emph{residual maximum likelihood} (REML) criterion (sometimes
called the \emph{restricted} maximum likelihood criterion) which
compensates for the estimation of the fixed-effects parameters when
estimating the random effects.

The REML criterion can be expressed as
\begin{equation}
  \label{eq:REMLcrit}
  \begin{aligned}
    L_R(\bm\theta,\sigma^2|\bm y)
    &=\int_{\bm\beta}L(\bm\beta,\bm\theta,\sigma^2|\bm y)\,d\bm\beta\\
    &=\frac{e^{-r^2/(2\sigma^2)}}{|\bm L(\bm\theta)|(2\pi\sigma^2)^{(n-p)/2}}
      \int_{\bm\beta}\frac{
      e^{-(\bm\beta-\widehat{\bm\beta})\trans\bm R_X\trans
          \bm R_X(\bm\beta-\widehat{\bm\beta})/(2\sigma^2)}}{(2\pi\sigma^2)^{p/2}}
        \,d\bm\beta\\
    &=\frac{e^{-r^2/(2\sigma^2)}}{|\bm L(\bm\theta)||\bm R_X(\bm\theta)|(2\pi\sigma^2)^{(n-p)/2}}
  \end{aligned}
\end{equation}
or, on the deviance scale,
\begin{equation}
  \label{eq:REMLcrit}
  \begin{aligned}
    -2\ell_R(\bm{\theta},\sigma^2|\bm y)
    &=(n-p)\log\left(2\pi\sigma^2\right)+\frac{r^2(\bm{\theta})}{\sigma^2}+2|\bm
    L(\bm\theta)|+2|\bm R_X(\bm\theta)|
  \end{aligned}
\end{equation}
from which we can see that the REML estimate of $\sigma^2$ is
\begin{equation}
  \label{eq:REMLsigma}
  \widehat{\sigma}_R(\bm{\theta})=\frac{r^2(\bm\theta)}{n-p}
\end{equation}
and the profiled REML deviance is
\begin{multline}
  \label{eq:REMLsigma}
  -2\ell_R(\bm{\theta},\widehat{\sigma}^2(\bm{\theta})|\bm y)
  =(n-p)\left[1+\log\left(\frac{2\pi}{n-p}\right)\right]
  +(n-p)\log r^2(\bm\theta)\\
  +2\log|\bm L(\bm\theta)|+2\log|\bm R_X(\bm\theta)|  .
\end{multline}

\section{Nonlinear mixed models}
\label{sec:nonlinearmixed}

Like the linear mixed model, the nonlinear mixed model is based on a
multivariate Gaussian distribution of the response,
$\bm y$, given $\bm\mu$.  The model is
\begin{equation}
  \label{eq:nlmm}
  \bm y=\bm\mu(\bm\beta,\bm b)+\bm\epsilon,\quad
  \bm\epsilon\sim\mathcal{N}(\bm{0},\sigma^2\bm I),\quad
  \bm b\sim\mathcal{N}(\bm{0},\sigma^2\bm{\Sigma}(\bm{\theta}),
  \quad\bm\epsilon\perp\bm b
\end{equation}
producing the same discrepancy function and normalizing factor as for the
linear mixed model,
\begin{align}
  \label{eq:nlmmProps}
  d(\bm\mu,\bm y)&=\left\|\bm\mu-\bm y\right\|^2\\
  k(\sigma^2)&=\left(2\pi\sigma^2\right)^{-n/2} .
\end{align}

The mean, $\bm\mu$, however, is no longer equal to the linear
predictor, $\bm\eta$.  Each element of $\bm\mu$ is the value of a
nonlinear model function $g(\bm x,\bm\phi)$ that depends on
covariates, $\bm{x}$, and on a parameter, $\bm\phi$,
of length $s$.  The model function $q(\bm x,\bm\phi)$ can be nonlinear
in some or all of the elements of $\bm\phi$.

When fitting a model the values of the covariates at
each observation are known so we can regard the $i$th element of the
conditional mean, $\mu_i$, as a function
of $\bm\phi_i$ only and write
\begin{equation}
  \label{eq:vectorg}
  \bm\mu=\bm g(\bm\Phi)
\end{equation}
where $\bm\Phi$ is the $n\times s$ matrix
with $i$th row $\bm\phi_i,i=1,\dots,n$ and the vector-valued
function $\bm g$ applies the scalar function $g(\bm x,\bm\phi)$ to the
rows of $\bm\Phi$ and the corresponding covariates $\bm{x}_i,i=1,\dots,n$.

The linear predictor, $\bm\eta$, is
\begin{equation}
  \label{eq:linpredPhi}
  \mathrm{vec}(\bm\Phi)=\bm\eta=\bm X\bm\beta+\bm{Z}\bm b=
  \bm X\bm\beta+\bm{V}(\bm\theta)\bm P\trans\bm u
\end{equation}
where the $\mathrm{vec}$ operator concatenates the columns of
$\bm\phi$ to form a vector of length $m=ns$.  Thus the matrix
$\bm X$ is $ns\times p$ while $\bm{Z}$ and $\bm{V}(\bm\theta)$ are $ns\times q$.

\subsection{Optimizing the penalized discrepancy}
\label{sec:nlmmpenalizedLS}

As for a linear mixed model, the problem of determining
$\tilde{\bm{u}}(\bm\beta,\bm{\theta})$, the optimizer of the
penalized discrepancy function, can be written as a penalized least
squares problem
\begin{equation}
  \label{eq:nlmmdisc}
  \begin{aligned}
    \tilde{\bm{u}}(\bm\beta,\bm{\theta})
    &=\arg\min_{\bm u}\delta(\bm u|\bm\beta,\bm\theta,\bm y)\\
    &=\arg\min_{\bm{u}}\left(\|\bm\mu(\bm\eta_{\bm u}(\bm u,\bm\beta,\bm\theta)-\bm y\|^2
      +\bm u\trans\bm u\right).
  \end{aligned}
\end{equation}
Generally (\ref{eq:nlmmdisc}) is a
penalized nonlinear least squares problem requiring an iterative
solution, not a penalized linear least squares problem like
(\ref{eq:conditionalMode}) with a direct solution.  A
particular type of nonlinear mixed model, described in
\S\ref{sec:nlmmcondlin}, does admit a direct solution to (\ref{eq:nlmmdisc}).

For the general case of an iterative solution to (\ref{eq:nlmmdisc})
we will use parenthesized superscripts to denote the number
of the iteration at which a quantity is evaluated.  Given
$\bm{u}^{(i)}$, the value of the orthogonal random effects at the
$i$th iteration, we evaluate the $n\times q$ matrix
\begin{equation}
  \label{eq:nlmmgrad}
  \begin{aligned}
  \bm{M}^{(i)}&=\left.\frac{\partial\bm\mu}{\partial\bm{u}\trans}\right|_{\bm{u}=\bm{u}^{(i)}}\\
  &=
  \begin{bmatrix}
    \bm I_n&\bm I_n&\dots&\bm I_n
  \end{bmatrix}
  \mathrm{diag}\left(\mathrm{vec}\left.\frac{d\bm\mu}{d\bm\Phi}\right|_{\bm\Phi=\bm\Phi^{(i)}}\right)
  \bm{V}(\bm\theta) .
  \end{aligned}
\end{equation}
(The matrix written $\begin{bmatrix}\bm I_n&\bm I_n&\dots&\bm
  I_n\end{bmatrix}$ contains $s$ copies $\bm I_n$.)  The proposed updated
vector of orthogonal random effects, $\bm{u}^{(i+1)}$, minimizes the
approximate penalized discrepancy based on a linear approximation to
$\bm\mu$ as a function of $\bm u$ evaluated at $\bm u^{(i)}$.  That is,
\begin{equation}
  \label{eq:nlmmuUpdate}
  \begin{aligned}
    \bm{u}^{(i+1)}
  &=\arg\min_{\bm{u}}\left\|\bm y-\bm\mu\left(\bm\beta,\bm{u}^{(i)}\right)
    -\bm{M}^{(i)}\left(\bm{u}-\bm{u}^{(i)}\right)\right\|^2+\bm{u}\trans\bm{u}\\
  &=\arg\min_{\bm{u}}\left\|\bm y-\bm\mu^{(i)}+
    \bm{M}^{(i)}\bm{u}^{(i)}-\bm{M}^{(i)}\bm{u}\right\|^2+\bm{u}\trans\bm{u}\\
  &=\arg\min_{\bm{u}}\left\|
    \begin{bmatrix}
      \bm y-\bm\mu^{(i)}+\bm{M}^{(i)}\bm{u}^{(i)}\\
      \bm{0}
    \end{bmatrix}-
    \begin{bmatrix}
      \bm{M}^{(i)}\\
      \bm I
    \end{bmatrix}\bm{u}\right\|^2 .
  \end{aligned}
\end{equation}
That is, $\bm{u}^{(i+1)}$ is the solution to a linear least squares
problem for which the normal equations are
\begin{equation}
  \label{eq:uip1}
  \left({\bm{M}^{(i)}}\trans\bm{M}^{(i)}+\bm I\right)\bm{u}^{(i+1)}=
  \bm{L}^{(i)}{\bm{L}^{(i)}}\trans\bm{u}^{(i+1)}=
  {\bm{M}^{(i)}}\trans\left(\bm y-\bm\mu^{(i)}+\bm{M}^{(i)}\bm{u}^{(i)}\right) .
\end{equation}

At convergence the Laplace approximation to the deviance is
\begin{equation}
  \label{eq:nlmmLaplace}
  -2\ell(\bm\beta,\bm{\theta},\sigma^2|\bm y)
  =n\log\left(2\pi\sigma^2\right)+\frac{\delta(\tilde{\bm{u}}|\bm{\theta},\bm\beta)}
  {\sigma^2}+2\log|\bm{L}(\bm\beta,\bm\theta)|
\end{equation}
where $\bm{L}(\bm\beta,\bm\theta)$ is the Cholesky
factor of $\bm{M}\trans\bm{M}+\bm I$ evaluated at
$\tilde{\bm{u}}(\bm\beta,\bm\theta)$, $\bm\beta$ and $\bm\theta$.
As for the linear mixed model we can form the conditional estimate of $\sigma^2$
\begin{equation}
  \label{eq:nlmmsigmasq}
  \widehat{\sigma^2}(\bm{\theta},\bm\beta)=
  \frac{\delta(\tilde{\bm{u}}|\bm{\theta},\bm\beta)}{n} .
\end{equation}
Substituting this estimate into (\ref{eq:nlmmLaplace}) produces the
Laplace approximation to the profiled deviance
\begin{equation}
  \label{eq:nlmmprofdevLaplace}
  -2\ell(\bm\beta,\bm{\theta},\widehat{\sigma^2}(\bm\beta,\bm{\theta})|\bm y)
  =n\left[1+\log\left(\frac{2\pi}{n}\right)\right]+
  n\log \delta(\tilde{\bm{u}}|\bm{\theta},\bm\beta)+2\log|\bm{L}| .
\end{equation}

\subsection{Constructing model matrices for nonlinear mixed models}
\label{sec:nlmmmodelmats}

In our previous example involving three measurements at times 1, 2 and
3 on each of five subjects, the conditional mean
$\bm\mu(\bm\beta,\bm b)$ was linear in the parameters $\bm\beta$ and
the in random effects $\bm b$ and also linear with respect to time.
Suppose instead that we felt that the trajectory of each subject's
response with respect to time was more appropriately modelled as
\begin{equation}
  \label{eq:asymOrig}
  {\bm\phi}_1\left(1-e^{-{\bm\phi}_2 x_{i,j}}\right)\quad i=1,\dots,4;\;j=1,\dots,3
\end{equation}
where $x_{i,j}$ is the time of the $j$th observation on the $i$th
subject while $\bm\phi_1$ and $\bm\phi_2$ are subject-specific
parameters representing the asymptotic value for subject $i$ (i.e. the
value predicted for large values of the time, $x$) and the rate
constant for subject $i$, respectively.

The model formula used in the \code{nlmer} function is a three-part
formula in which the left hand side determines the response, the
middle part is the expression of the nonlinear model involving the
parameters $\bm\phi$ and any covariates and the right hand side is a
mixed model formula that can (in fact, must) involve the names of
parameters from the nonlinear model.

In our example, if subject-specific parameters are modelled as
population means, $\bm\beta=[\beta_1,\beta_2]\prime$ plus a
subject-specific random effect for each parameter, allowing for
correlation of the random effects within each subject, the formula
would be written
<<asymorigmodel,echo=FALSE>>=
y ~ A*(1-exp(-rc*time)) ~ (A + rc|subj)
@

The \code{vec} of the $12\times 2$ parameter matrix $\bm\Phi$ is a
vector of length $24$ where the first $12$ elements are values of
\code{A} and the last $12$ elements are values of \code{rc}.  In the
mixed-model formula the names \code{A} and \code{rc} represent
indicator variables for the first $12$ and the last $12$ positions,
respectively.  In the general case of a nonlinear model with $s$
parameters there will be $s$ indicator variables named according to
the model parameters and determining the positions in
$\mathrm{vec}(\bm\Phi)$ that correspond to each parameter.

For the model matrices $\bm X$ and $\bm{Z}$ the implicit intercept
term generated by the standard S language rules for model matrices
would not make sense.  The intercept term is suppressed in the
random-effects terms and is replaced by the sum of the parameter name
indicators in the fixed-effects terms.  Thus the formula shown above
is equivalent to
<<asymorigmodel1,echo=FALSE>>=
y ~ A*(1-exp(-rc*time)) ~ A + rc + (0 + A + rc|subj)
@

The matrix $\bm X$ will be $30\times 2$ with the two columns being
the indicator for \code{A} and the indicator for \code{rc}.

\subsection{Random effects for conditionally linear parameters only}
\label{sec:nlmmcondlin}

There is a special case of a nonlinear mixed model where the Laplace
approximation is the deviance and where the iterative algorithm to
determine $\tilde{\bm{u}}(\bm\beta, \bm{\theta}, \bm y)$ will
converge in one iteration.  Frequently some of the elements of the
parameter vector $\bm{\phi}$ occur linearly in the nonlinear model
$g(\bm{x}, \bm{\phi})$.  These elements are said to be
\emph{conditionally linear} parameters because, conditional on the
values of the other parameters, the model function is a linear
function of these.

If the random effects determine only conditionally linear parameters
then $\bm\mu$ is linear in $\bm{u}$ and
the matrix $\bm{M}$ depends on $\bm\beta$ and $\bm{\theta}$ but
not on $\bm{u}$.  We can rewrite the mean function as
\begin{equation}
  \label{eq:condlinmu}
  \bm\mu(\bm\beta, \bm{u})=\bm\mu_{\bm{0}}(\bm\beta)+\bm{M}\bm{u}
\end{equation}
where $\bm\mu_{\bm{0}}(\bm\beta)=\bm\mu(\bm\beta,\bm{0})
=\bm\mu\left(\bm X\bm\beta\right)$.
The penalized least squares problem (\ref{eq:nlmmuUpdate}) for the
updated $\bm{u}$ can be rewritten as
\begin{equation}
  \label{eq:condlinupdate}
  \begin{aligned}
  \tilde{\bm{u}}\left(\bm\beta,\bm{\theta},\bm y\right)
  = \min_{\bm{u}}\left\|
        \begin{bmatrix}
          \bm y-\bm\mu_{\bm{0}}(\bm\beta)\\
          \bm{0}
        \end{bmatrix}
        - \begin{bmatrix}\bm{M}\\\bm I\end{bmatrix}\bm{u}\right\|^2 .
  \end{aligned}
\end{equation}
That is, $\tilde{\bm{u}}(\bm\beta,\bm{\theta},\bm y)$ is the
solution to
\begin{equation}
  \label{eq:condlinsol}
  \left(\bm{M}\trans\bm{M}+\bm I\right)\tilde{\bm{u}}=
  \bm{M}\trans\left(\bm y-\bm\mu_{\bm{0}}(\bm\beta)\right)
\end{equation}

\section{Generalized linear mixed models}
\label{sec:GLMM}

A generalized linear mixed model differs from a linear mixed model in
the form of the conditional distribution of $\bm y$ given $\bm\beta$,
$\bm b$ and, possibly, $\sigma^2$, which determines the discrepancy
function $d(\bm\mu,\bm y)$, and in the mapping from the linear
predictor, $\bm\eta$, to the conditional mean, $\bm\mu$.  This
mapping between $\bm\eta$ and $\bm\mu$ is assumed to be one-to-one and
to enforce any constraints on the elements of $\bm\mu$, such as the
mean of a Bernoulli or binomial random variable being in the range
$0\le\left\{\bm\mu\right\}_k\le 1, k=1,\dots,n$ or the mean of a
Poisson random variable being positive.  By convention, it is the mapping
from $\bm\mu$ to $\bm\eta=\bm g\left(\bm\mu\right)$ that is called
the \emph{link function}, so the inverse mapping,
$\bm\mu=\bm g^{-1}\left(\bm\eta\right)$, is called
the \emph{inverse link}.

Although we have written the link and the inverse link as functions of
vectors, they are defined in terms of scalar functions, so that
\begin{equation}
  \label{eq:vectorlink}
  \begin{aligned}
    \eta_k&=\left\{\bm\eta\right\}_k=
    \left\{\bm g(\bm\eta)\right\}_k
    =g\left(\left\{\bm\eta\right\}\right)=g(\mu_k)\quad k=1,\dots,n\\
    \mu_k&=\left\{\bm\mu\right\}_k=
    \left\{\bm g^{-1}(\bm\mu)\right\}_k
    =g^{-1}\left(\left\{\bm\mu\right\}\right)\quad k=1,\dots,n .
  \end{aligned}
\end{equation}
where $g(\mu)$ and $g^{-1}(\eta)$ are the scalar link and inverse link
functions, respectively.  Furthermore, the elements of
$\bm y$ are assumed to be conditionally independent, given $\bm\mu$,
and for $k=1,\dots,n$ the distribution of $y_k$ depends only on
$\mu_k$ and, possibly, $sigma^2$.  That is, the discrepancy function
can be written
\begin{equation}
  \label{eq:devianceresid}
  d(\bm\mu,\bm y)=\sum_{k=1}^n r_D^2(\mu_k,y_k)
\end{equation}
where $r_D$ is the \emph{deviance residual} function.  For many models
the discrepancy defines

\subsection{Examples of deviance residual and link functions}
\label{sec:devianceresid}

If the $y_k,k=1,\dots,n$ are binary responses (i.e. each $y_k$ is
either $0$ or $1$) and they are conditionally independent given
$\bm\mu$, then the conditional distribution of $\bm y$
given $\bm\mu$ has probability mass function
\begin{equation}
  \label{eq:Bernoulli}
  f_{\bm y|\bm\mu}(\bm y,\bm\mu)=
  \prod_{k=1}^n\mu_k^{y_k}\left(1-\mu_k\right)^{\left(1-y_k\right)}
\end{equation}
Because the distribution of $y_k$ is completely determined by $\mu_k$
there is no need for a separate scale factor, $\sigma^2$, and
expression (\ref{eq:conddens}) for the conditional density in terms of
the discrepancy can be written
\begin{equation}
  \label{eq:condBernoulli}
  f_{\bm y|\bm\mu}(\bm y|\bm\mu)=k e^{-d(\bm\mu,\bm y)/2} .
\end{equation}
Thus the discrepancy function must be
\begin{equation}
  \label{eq:discrBernoulli}
  d(\bm\mu,\bm y)=
\end{equation}


\bibliography{lme4}
\end{document}
