\documentclass[12pt]{article}
\usepackage{Sweave}
\usepackage{amsmath}
\usepackage{bm}
\usepackage[authoryear,round]{natbib}
\bibliographystyle{plainnat}
\DefineVerbatimEnvironment{Sinput}{Verbatim}
{formatcom={\vspace{-2.5ex}},fontshape=sl,
  fontfamily=courier,fontseries=b, fontsize=\scriptsize}
\DefineVerbatimEnvironment{Soutput}{Verbatim}
{formatcom={\vspace{-2.5ex}},fontfamily=courier,fontseries=b,%
  fontsize=\scriptsize}
%%\VignetteIndexEntry{Implementation Details}
%%\VignetteDepends{lme4}
\newcommand{\trans}{\ensuremath{^\prime}}
\title{Theory and computational methods for mixed models}
\author{Douglas Bates\\Department of Statistics\\%
  University of Wisconsin -- Madison}
\begin{document}
\SweaveOpts{engine=R,eps=FALSE,pdf=TRUE,strip.white=TRUE}
\SweaveOpts{include=FALSE}
\setkeys{Gin}{width=\textwidth}
\newcommand{\code}[1]{\texttt{\small{#1}}}
\newcommand{\package}[1]{\textsf{\small{#1}}}
\maketitle
\begin{abstract}
  The \code{lme4} package provides methods for fitting linear mixed
  models, generalized linear mixed models and nonlinear mixed models.
  In this vignette we describe the formulation of these models and the
  computational approach used to evaluate or approximate the
  log-likelihood of a model/data/parameter value combination.
\end{abstract}

\section{Introduction}
\label{sec:intro}

The \code{lme4} package is used to fit and to analyze linear mixed
models, generalized linear mixed models and nonlinear mixed models.
These models are called \emph{mixed-effects models} or, more simply,
\emph{mixed models} because they incorporate both \emph{fixed-effects}
parameters, which apply to an entire population or to well-defined and
repeatable subsets of a population, and \emph{random effects}, which
apply to the particular experimental units or observational units in
the study.  Such models are also called \emph{multilevel} models
because the random effects represent levels of variation in addition
to the per-observation noise term that is commonly incorporated in
statistical models such as linear regression models, generalized
linear models or nonlinear regression models.

The three types of mixed models -- linear, generalized linear and
nonlinear -- share common characteristics in that the model is
specified in whole or in part by a \emph{mixed model formula} that
describes a \emph{linear predictor} and a variance-covariance
structure for the random effects.  In the next section we describe
these properties of the mixed model formula and a general approach to
evaluating or approximating the log-likelihood at specific parameter
values for such models.

In subsequent sections we describe computational methods for specific
kinds of mixed models.

\section{Mixed-model formulas}
\label{sec:formula}

The right-hand side of a mixed-model formula, as used in the
\code{lme4} package consists of one or more random-effects terms and
zero or more fixed-effects terms separated by the `\code{+}' symbol.
A simple random-effects term is of the form
`\emph{form}\code{|}\emph{factor}' where \emph{form} is a linear model
formula and \emph{factor} is an expression that can be evaluated as a
factor, called the \emph{grouping factor} for the term because it
determines to which group of observations a component of the random
effects applies.

Typically a random-effects term is enclosed by parentheses so the
extent of \emph{form} is clearly defined.

\subsection{Examples of mixed-model formulas}
\label{sec:mixed-model-formula-examples}

The simplest mixed-model formula is of the form
<<formula1,echo=FALSE>>=
y ~ 1 + (1 | Subject)
@
where \code{y} is the response vector and \code{Subject} is a factor
that indicates the experimental unit or observational unit from which
each individual response was collected.  In this formula there is one
random-effect term, \code{(1|Subject)}, and one fixed-effects term,
the constant or intercept term denoted by `\code{1}'.  In the
random-effects term the grouping factor is `\code{Subject}' and the
linear model formula denotes a constant term.  

We denote by $k$ the number of random-effects terms in the formula.
The $i$th grouping factor is written $\bm{f}_i$ and the number of
levels or this factor is $n_i$.  For this formula $k=1$ and $n_1$ is
the number of subjects.

If the model described by this formula is a linear mixed model it
could be written
\begin{displaymath}
  y_{ij}=\mu+b_j\quad i=1,\dots,n_1;\; j = 1,\dots,m_i
\end{displaymath}



  a mixed model formula is similar to a
linear model formula, such as used in the \code{lm} or \code{glm}
functions, except that it contains one or more random-effects terms,
which are terms that contain the vertical bar (\code{|}) symbol.
Before discussing the details of the interpretation of the terms in
the formula we describe how the formula determines the log-likelihood
of the model.

\section{Log-likelihood of mixed models}
\label{sec:log-likelihood}

\subsection{The linear predictor}
\label{sec:linpred}


When evaluated with respect to data consisting of an $n$-dimensional
response vector $\bm{y}$ and corresponding covariates, a mixed model
formula produces the model matrices $\bm{X}$ (of size $m\times p$) and
$\bm{Z}$ (of size $m\times q$) in the \emph{linear predictor}
\begin{equation}
  \label{eq:linpred}
  \bm{X}\bm{\beta}+\bm{Z}\bm{b}
\end{equation}
where $\bm{\beta}$ is the $p$-dimensional vector of fixed-effects
parameters and $\bm{b}$ is the $q$-dimensional vector of random effects.

For all types of mixed models the expected value of the response
vector $\bm{y}$ is a function of $\bm{\beta}$ and $\bm{b}$ that
depends only on the linear predictor.  That is, we can write
\begin{equation}
  \label{eq:expected}
  \mathrm{E}[\bm{y}] = \bm{\mu}(\bm{\beta},\bm{b}) =
  \bm{\mu}\left(\bm{X}\bm{\beta}+\bm{Z}\bm{b}\right)
\end{equation}
The particular way in which the expected value depends on the linear
predictor is different for different types of mixed models.  In the
simplest case, a linear mixed model, the expected value is the linear
predictor; i.e.{}
$\bm{\mu}(\bm{\beta},\bm{b})=\bm{X}\bm{\beta}+\bm{Z}\bm{b}$.

In general we assume we can write the probability density or the
probability mass function of $\bm{y}$ in the form
\begin{equation}
  \label{eq:conddens}
  k(\sigma^2)\exp\left[\frac{d(\bm{\mu},\bm{y})}{-2\sigma^2}\right]
\end{equation}
where $\sigma^2$ is a scale factor in the expression for the
variance-covariance matrix of $\bm{y}$ and
$d(\bm{\mu},\bm{y})$ is the \emph{discrepancy function}.
The normalization constant $k(\sigma^2)$ depends only on the scale
factor $\sigma^2$.

For linear mixed models and nonlinear mixed models the scale factor
$\sigma^2$ is the variance of individual components of $\bm{y}$.
Although we still write this scale factor as $\sigma^2$ for a
generalized linear mixed model it does not need to represent the
variance of any specific quantity.  For some GLMM's, specifically
those based on the binomial or Poisson distributions for $\bm{y}$, the
variance is determined by the mean $\bm{\mu}$ and thus $\sigma^2\equiv 1$.

\subsection{Variance-covariance of the random effects}
\label{sec:ranefvarcov}

The random effects $\bm{b}$ are random variables with a multivariate
normal (or Gaussian) distribution of the form
\begin{equation}
  \label{eq:ranef}
  \bm{b}\sim\mathcal{N}\left(\bm{0},\sigma^2\bm{\Sigma}(\bm{\theta})\right)
\end{equation}
where $\sigma^2$ is the same scale factor as in (\ref{eq:conddens})
and $\bm{\Sigma}$ is a $q\times q$ positive semi-definite relative
variance covariance matrix determined by the parameter $\bm{\theta}$.

In addition to determining the linear predictor, the mixed model
formula determines the form of $\bm{\Sigma}$.

The size $q$ of the symmetric matrix $\bm{\Sigma}$ can be very large
but this matrix is typically quite sparse and the dimension of the
parameter $\bm{\theta}$ that determines $\bm{\Sigma}$ is small.
Because $\bm{\Sigma}$ is positive semi-definite it has a Cholesky
factorization which we write as
\begin{equation}
  \label{eq:TSST}
  \bm{\Sigma}=\bm{T}\bm{S}\bm{S}\bm{T}\trans
\end{equation}
where $\bm{T}$ is unit lower triangular (i.e.{} it is lower triangular
and all its diagonal elements are unity) and $\bm{S}$ is a
non-negative diagonal matrix (i.e.{} it is a diagonal matrix and all
its diagonal elements are non-negative).  This is a simple
modification of the ``LDL'' form of the Cholesky
factorization~\citep{davis06:csparse_book} in which the diagonal
elements of $\bm{S}$ are the square roots of the corresponding
diagonal elements of $\bm{D}$ from the LDL form.

As shown in \S\ref{sec:theta}, the parameter $\bm{\theta}$ determines
$\bm{\Sigma}$ through $\bm{S}$ and $\bm{T}$.  For a fixed value of
$\bm{\theta}$ we can write $\bm{b}$ as
\begin{equation}
  \label{eq:TSu}
  \bm{b}=\bm{T}(\bm{\theta})\bm{S}(\bm{\theta})\bm{u}
\end{equation}
where $\bm{u}$ is a vector of orthogonal random effects with distribution
$\bm{u}\sim\mathcal{N}\left(\bm{0},\sigma^2\bm{I}\right)$.  This
provides the desired distribution
$\bm{b}\sim\mathcal{N}(\bm{0},\sigma^2\bm{\Sigma})$ because
\begin{displaymath}
  \mathrm{E}[\bm{b}]=\mathrm{E}[\bm{T}\bm{S}\bm{u}]=
  \bm{T}\bm{S}\mathrm{E}[\bm{u}]=\bm{0}
\end{displaymath}
and
\begin{displaymath}
  \mathrm{Var}(\bm{b})=\mathrm{E}[\bm{b}\bm{b}\trans]=
  \bm{T}\bm{S}\mathrm{E}[\bm{u}\bm{u}\trans]\bm{S}\bm{T}\trans=
  \sigma^2\bm{T}\bm{S}\bm{S}\bm{T}\trans=\sigma^2\bm{\Sigma}
\end{displaymath}


Because $\bm{T}$ is unit lower triangular its determinant, $|\bm{T}|$, which
is the product of the diagonal elements for a triangular matrix, is
unity.  Similarly $|\bm{S}|$, which is the product of the diagonal
elements of this diagonal matrix, is non-negative.  Together these imply that
\begin{equation}
  \label{eq:Sigmadet}
  |\bm{\Sigma}|^{1/2}=\sqrt{|\bm{T}|^2|\bm{S}|^2}=|\bm{S}| .
\end{equation}
Thus the determinant of the Jacobian of the transformation from
$\bm{u}$ to $\bm{b}$ is
\begin{displaymath}
  \left|\frac{d\bm{b}}{d\bm{u}}\right|=
  \left|\bm{T}\bm{S}\right|=|\bm{\Sigma}|^{1/2}
\end{displaymath}
which allows us to rewrite an integral of the form 
\begin{displaymath}
  \int_{\bm{b}}\frac{f_{\bm{b}}(\bm{b})}{|\bm{\Sigma}|^{1/2}}\,d\bm{b}
\end{displaymath}
as
\begin{displaymath}
  \int_{\bm{u}}f_{\bm{u}}(\bm{u})\,d\bm{u}
\end{displaymath}

Finally, recall that $\bm{\mu}$, the expected value of $\bm{y}$, and the
discrepancy function, $d(\bm{\mu},\bm{y})$, depend on $\bm{b}$ only
through the linear predictor.  Because we can rewrite the linear predictor as
a function of $\bm{\beta}$ and $\bm{u}$
\begin{equation}
  \label{eq:linpredu}
  \bm{X}\bm{\beta}+\bm{Z}\bm{b}=\bm{X}\bm{\beta}+\bm{Z}\bm{T}\bm{S}\bm{u}=
  \bm{X}\bm{\beta}+\bm{V}\bm{u},
\end{equation}
where $\bm{V}=\bm{Z}\bm{T}\bm{S}$, we can also express the discrepancy as a
function of $\bm{\beta}$ and $\bm{u}$.

\subsection{Marginal distribution of $\bm{y}$ }
\label{sec:marginal}

The likelihood of the parameters $\bm{\beta}$,
$\bm{\theta}$ and $\sigma^2$ given $\bm{y}$, written
$L(\bm{\beta},\bm{\theta},\sigma^2|\bm{y})$, is equal
to the marginal density of $\bm{y}$ given the parameters.  This
marginal density is the integral over $\bm{b}$ of the conditional
density of $\bm{y}$ given $\bm{b}$
\begin{displaymath}
  k\left(\sigma^2\right)\exp\left[\frac{d(\bm{\mu},\bm{y})}{-2\sigma^2}\right]
\end{displaymath}
multiplied by the marginal distribution of $\bm{b}$
\begin{displaymath}
  \frac{1}{\left(2\pi\sigma^2\right)^{q/2}|\bm{\Sigma}|^{1/2}}
  \exp\left[\frac{\bm{b}\trans\bm{\Sigma}^{-1}\bm{b}}{-2\sigma^2}\right]
\end{displaymath}
when $\bm{\Sigma}$ is non-singular (i.e.{} when all the diagonal
elements of $\bm{S}$ are positive).

Thus
\begin{equation}
  \label{eq:likelihoodb}
  \begin{aligned}
    L(\bm{\beta},\bm{\theta},\sigma^2|\bm{y})
    =&\int_{\bm{b}}\frac{k(\sigma^2)}{\left(2\pi\sigma^2\right)^{q/2}|\bm{\Sigma}|^{1/2}}
    \exp\left[{\frac{d(\bm{\mu},\bm{y})+
          \bm{b}\trans\bm{\Sigma}^{-1}\bm{b}}{-2\sigma^2}}\right]\,d\bm{b}\\
    =&\int_{\bm{u}}\frac{k(\sigma^2)}{\left(2\pi\sigma^2\right)^{q/2}}
    \exp\left[\frac{d(\bm{\mu},\bm{y})+\bm{u}\trans\bm{u}}{-2\sigma^2}\right]\,d\bm{u}
  \end{aligned}
\end{equation}
Note that the likelihood can be evaluated for a positive semi-definite
$\bm{\Sigma}$ when written as an integral with respect to $\bm{u}$.

\subsection{The Laplace approximation}
\label{sec:Laplace}

The numerator of the exponent in the integrand of
(\ref{eq:likelihoodb}), either
$d(\bm{\mu},\bm{y})+\bm{b}\trans\bm{\Sigma}^{-1}\bm{b}$ as a function
of $\bm{b}$ given the parameters and the data and or
$d(\bm{\mu},\bm{y})+\bm{u}\trans\bm{u}$ as a function of $\bm{u}$, is
called the \emph{penalized discrepancy}.  We write it as
\begin{equation}
  \label{eq:delta}
  \delta(\bm{u}|\bm{\beta},\bm{\theta},\bm{y})=d(\bm{\mu},\bm{y})+\bm{u}\trans\bm{u}
\end{equation}

Given the data $\bm{y}$ and values of the parameters $\bm{\beta}$ and
$\bm{\theta}$ we determine the minimizer of the penalized discrepancy
\begin{equation}
  \label{eq:tildeu}
  \tilde{\bm{u}}(\bm{\beta},\bm{\theta})=
  \arg\min_{\bm{u}}\delta(\bm{u}|\bm{\beta},\bm{\theta},\bm{y})=
  \arg\min_{\bm{u}}\left[d(\bm{\mu},\bm{y})+\bm{u}\trans\bm{u}\right] .
\end{equation}
This value is the \emph{conditional mode} of $\bm{u}$ given
$\bm{\beta}$, $\bm{\theta}$ and $\bm{y}$.  That is, it is the value of
the orthogonal random effects that maximizes the conditional density
given the parameters $\bm{\beta}$ and $\bm{\theta}$ and the data $\bm{y}$.

Near the conditional mode the penalized discrepancy has a quadratic approximation
\begin{equation}
  \label{eq:quadapprox}
  \delta(\bm{u}|\bm{\beta},\bm{\theta},\bm{y})\approx
  \delta(\tilde{\bm{u}}|\bm{\beta},\bm{\theta},\bm{y})+
  \left(\bm{u}-\tilde{\bm{u}}\right)\trans\bm{L}\bm{L}\trans
  \left(\bm{u}-\tilde{\bm{u}}\right)
\end{equation}
where $\bm{L}$ is the Cholesky factor of
$\left.\frac{1}{2}\nabla_{\bm{u}}^2
  \delta(\bm{u}|\bm{\beta},\bm{\theta},\bm{y})\right|_{\bm{u}=\tilde{\bm{u}}}$.

After substituting the quadratic approximation (\ref{eq:quadapprox})
into expression (\ref{eq:likelihoodb}) for
$L(\bm{\beta},\bm{\theta},\sigma^2|\bm{y})$, the only part of the
integrand that depends on $\bm{u}$ will be the quadratic term in the
exponent.  To evaluate an integral of the form
\begin{displaymath}
  I=\int_{\bm{u}}\frac{1}{\left(2\pi\sigma^2\right)^{q/2}}
  \exp\left[\frac{(\bm{u}-\tilde{\bm{u}})\trans\bm{L}\bm{L}\trans(\bm{u}-\tilde{\bm{u}})}
    {-2\sigma^2}\right]\,d\bm{u}
\end{displaymath}
we use a change of variable to
\begin{displaymath}
  \bm{v}=\bm{L}\trans(\bm{u}-\tilde{\bm{u}})/\sigma
\end{displaymath}
for which the determinant of the Jacobian is
\begin{displaymath}
  \left|\frac{d\bm{v}}{d\bm{u}}\right|=\frac{|\bm{L}|}{\sigma^q} .
\end{displaymath}
This change of variable reduces the integral to
\begin{equation}
  \label{eq:reducedint}
  I=\int_{\bm{v}}\frac{1}{\left(2\pi\right)^{q/2}}
  e^{-\bm{v}\trans\bm{v}/2}\frac{d\bm{v}}{|\bm{L}|}=|\bm{L}|^{-1}
\end{equation}
provided that $\bm{L}$ is non-singular.  In general, $\bm{L}$ is lower
triangular with non-negative elements on the diagonal.  As we will see in
\S{\ref{sec:penalizedLS}}, the form of the penalized discrepancy
function ensures that the diagonal elements of this particular
$\bm{L}$ are all positive and, hence, that $\bm{L}$ is non-singular.

Returning to expression (\ref{eq:likelihoodb}), we can now express the
Laplace approximation to the likelihood function or, as more commonly
used as an optimization criterion, the log-likelihood
$\ell(\bm{\beta},\bm{\theta},\sigma^2|\bm{y})=
\log L(\bm{\beta},\bm{\theta},\sigma^2|\bm{y})$.  On the deviance scale
(twice the negative log-likelihood) the approximation is
\begin{equation}
  \label{eq:LaplaceDev}
  -2\ell(\bm{\beta},\bm{\theta},\sigma^2|\bm{y})=-2\log[k(\sigma^2)]+
  \frac{\delta(\tilde{\bm{u}}|\bm{\beta},\bm{\theta},\bm{y})}{\sigma^2}
  +2\log|\bm{L}|
\end{equation}
The Laplace approximation (\ref{eq:LaplaceDev}) will be exact when the
penalized discrepancy $\delta(\bm{u}|\bm{\beta},\bm{\theta},\bm{y})$
is a quadratic function of $\bm{u}$.

\section{Linear mixed models}
\label{sec:lmm}

A linear mixed model can be expressed as
\begin{equation}
  \label{eq:lmmDef}
  \bm{y}=\bm{X}\bm{\beta}+\bm{Z}\bm{b}+\bm{\epsilon},\quad
  \bm{\epsilon}\sim\mathcal{N}\left(\bm{0},\sigma^2\bm{I}\right),\quad
  \bm{b}\sim\mathcal{N}\left(\bm{0},\sigma^2\bm{\Sigma}\right),\quad
  \bm{\epsilon}\perp\bm{b}
\end{equation}
where the symbol $\perp$ denotes independence of random variables.
This model implies that the mean of $\bm{y}$ is the linear predictor, the
discrepancy function is the residual sum of squares and the
normalizing factor is $1/(2\pi\sigma^2)^{n/2}$.  That is,
\begin{align}
  \label{eq:lmmProps}
  \bm{\mu}&=\bm{X}\bm{\beta}+\bm{Z}\bm{b}=\bm{X}\bm{\beta}+\bm{V}\bm{u}\\
  d(\bm{\mu},\bm{y})&=\left\|\bm{\mu}-\bm{y}\right\|^2\\
  k(\sigma^2)&=\frac{1}{\left(2\pi\sigma^2\right)^{n/2}} .
\end{align}
The penalized discrepancy is
\begin{equation}
  \label{eq:lmmdelta}
  \begin{aligned}
    \delta(\bm{u}|\bm{\beta},\bm{\theta},\bm{y})
    &= d(\bm{\mu},\bm{y})+\bm{u}\trans\bm{u}\\
    &=\left\|\bm{\mu}-\bm{y}\right\|^2+\bm{u}\trans\bm{u}\\
    &=\left\|\bm{V}\bm{u}+\bm{X}\bm{\beta}-\bm{y}\right\|^2+\bm{u}\trans\bm{u}\\
    &=
    \begin{bmatrix}
      \bm{u}\trans&\bm{\beta}\trans&1
    \end{bmatrix}
    \begin{bmatrix}
      \bm{V}\trans\bm{V}+\bm{I}&\bm{V}\trans\bm{X}&-\bm{V}\trans\bm{y}\\
      \bm{X}\trans\bm{V}&\bm{X}\trans\bm{X}&-\bm{X}\trans\bm{y}\\
      -\bm{y}\trans\bm{V}&-\bm{y}\trans\bm{X}&\bm{y}\trans\bm{y}
    \end{bmatrix}
    \begin{bmatrix}
      \bm{u}\\
      \bm{\beta}\\
      1
    \end{bmatrix} .
  \end{aligned}
\end{equation}
In this form it is obvious that
$\delta(\bm{u}|\bm{\beta},\bm{\theta},\bm{y})$ is a quadratic function
of $\bm{u}$ and that 
\begin{equation}
  \label{eq:hessian}
  \frac{1}{2}\nabla_{\bm{u}}^2\delta(\bm{u}|\bm{\beta},\bm{\theta},\bm{y})=\bm{V}\trans\bm{V}+\bm{I}
\end{equation}
is positive definite.  This expression for $\nabla_{\bm{u}}^2\delta$
depends on $\bm{\theta}$ (because
$\bm{V}=\bm{Z}\bm{T}(\bm{\theta})\bm{S}(\bm{\theta})$ depends on
$\bm{\theta}$) but not on $\bm{\beta}$ or $\bm{u}$.  Thus the Cholesky
factor $\bm{L}$ required for (\ref{eq:LaplaceDev}) also depends only on
$\bm{\theta}$.

The conditional mode of the orthogonal random effects,
$\tilde{\bm{u}}(\bm{\beta}, \bm{\theta}, \bm{y})$ can be expressed as
the solution to
\begin{equation}
  \label{eq:conditionalMode}
  \left(\bm{V}\trans\bm{V}+\bm{I}\right)\tilde{\bm{u}}=
  \bm{L}\bm{L}\trans\tilde{\bm{u}}=
  \bm{V}\trans\left(\bm{y}-\bm{X}\bm{\beta}\right) .
\end{equation}
and can be easily calculated once the Cholesky factor $\bm{L}$ has
been determined.  

However, we can take advantage of the fact that
$\delta(\bm{u}|\bm{\beta},\bm{\theta},\bm{y})$ is a quadratic function
of both $\bm{u}$ and $\bm{\beta}$ to minimize $\delta$ with respect to
$\bm{u}$ and $\bm{\beta}$ simultaneously.  For a given value of
$\bm{\theta}$ we form the Cholesky factorization
\begin{equation}
  \label{eq:bigChol}
    \begin{bmatrix}
      \bm{V}\trans\bm{V}+\bm{I}&\bm{V}\trans\bm{X}&-\bm{V}\trans\bm{y}\\
      \bm{X}\trans\bm{V}&\bm{X}\trans\bm{X}&-\bm{X}\trans\bm{y}\\
      -\bm{y}\trans\bm{V}&-\bm{y}\trans\bm{X}&\bm{y}\trans\bm{y}
    \end{bmatrix}
    =
    \begin{bmatrix}
      \bm{L}&\bm{0}&\bm{0}\\
      \bm{R}_{VX}\trans&\bm{L}_X&\bm{0}\\
      \bm{r}_{Vy}\trans&\bm{r}_{Xy}\trans&r
    \end{bmatrix}
    \begin{bmatrix}
      \bm{L}\trans&\bm{R}_{VX}&\bm{r}_{Vy}\\
      \bm{0}&\bm{L}_X\trans&\bm{r}_{Xy}\\
      \bm{0}&\bm{0}&r
    \end{bmatrix}
\end{equation}
The somewhat awkward notation in (\ref{eq:bigChol}) distinguishes
those sections of the factor that are lower triangular on the left
($\bm{L}$ and $\bm{L}_X$) from those that are column vectors
($\bm{r}_{Vy}$ and $\bm{r}_{Xy}$) or untransposed matrices
($\bm{R}_{VX}$) on the right.  The lower right element, $r$, is a scalar.

With the factorization (\ref{eq:bigChol}) we can write
\begin{equation}
  \label{eq:deltaReduced}
  \begin{aligned}
    \delta(\bm{u}|\bm{\beta},\bm{\theta},\bm{y})&=
    \left\|
      \begin{bmatrix}
        \bm{L}\trans&\bm{R}_{VX}&\bm{r}_{Vy}\\
        \bm{0}&\bm{L}_X\trans&\bm{r}_{Xy}\\
        \bm{0}&\bm{0}&r
      \end{bmatrix}
      \begin{bmatrix}
        \bm{u}\\
        \bm{\beta}\\
        1
      \end{bmatrix}
    \right\|^2\\
    &=r^2+\left\|\bm{L}_X\trans\bm{\beta}+\bm{r}_{Xy}\right\|^2+
    \left\|\bm{L}\trans\bm{u}+\bm{R}_{VX}\bm{\beta}+\bm{r}_{Vy}\right\|^2\\
    &=r^2+\left\|\bm{L}_X\trans\left(\bm{\beta}-\hat{\bm{\beta}}\right)\right\|^2+
    \left\|\bm{L}\trans\left(\bm{u}-\hat{\bm{u}}\right)\right\|^2
  \end{aligned}
\end{equation}
where $\widehat{\bm{\beta}}$, the conditional estimate of $\bm{\beta}$
given $\bm{\theta}$, and $\widehat{\bm{u}}$, the conditional mode of
$\bm{u}$ given $\bm{\theta}$ and $\widehat{\bm{\beta}}(\bm{\theta})$, are
the solutions to
\begin{align}
  \label{eq:conditionalest}
  \bm{L}_X\trans\widehat{\bm{\beta}}(\bm{\theta})&=-\bm{r}_{Xy}\\
  \bm{L}\trans\widehat{\bm{u}}(\bm{\theta})
  &=-(\bm{R}_{VX}\widehat{\bm{\beta}}+\bm{r}_{Vy}) .
\end{align}

The deviance function,
$-2\ell(\bm{\beta},\bm{\theta},\sigma^2|\bm{y})$ can be reduced to
\begin{equation}
  \label{eq:reducedDev}
  -2\ell(\widehat{\bm{\beta}}(\bm{\theta}),\bm{\theta},\sigma^2|\bm{y})
  =n\log\left(2\pi\sigma^2\right)+\frac{r^2}{\sigma^2}+2\log|\bm{L}| .
\end{equation}
Differentiating
$-2\ell(\widehat{\bm{\beta}}(\bm{\theta}),\bm{\theta},\sigma^2|\bm{y})$
as a function of $\sigma^2$ and setting the derivative to zero
provides the conditional estimate
\begin{equation}
  \label{eq:mlsigmasq}
  \widehat{\sigma^2}(\bm{\theta})=\frac{r^2}{n}
\end{equation}
and substituting this estimate into (\ref{eq:reducedDev}) provides the
profiled deviance function
\begin{equation}
  \label{eq:profiledDeviance}
  \begin{aligned}
  -2\ell(\widehat{\bm{\beta}}(\bm{\theta}),\bm{\theta},\widehat{\sigma^2}(\bm{\theta})|\bm{y})
  &=n\log\left(\frac{2\pi r^2}{n}\right) + n + 2\log|\bm{L}|\\
  &=n\left[1+\log\left(\frac{2\pi}{n}\right)\right]+n\log r^2+2\log|\bm{L}|
  \end{aligned}
\end{equation}

The three terms in the final form of the profiled deviance in
(\ref{eq:profiledDeviance}) are, respectively, a constant,
$n\left[1+\log\left(2\pi/n\right)\right]$, a measure of the fidelity
of the fitted values to the observed data, $n\log r^2$, and a measure
of model complexity, $2\log|\bm{L}|$.  We can view the act of
optimizing the profiled deviance as determining a model that balances
fidelity to the data against model complexity.

\subsection{REML estimates}
\label{sec:REML}

The maximum likelihood estimate of $\sigma^2$, $\widehat{\sigma^2}=r^2/n$,
is the penalized residual sum of squares divided by the number of
observations.  It has a form like the maximum likelihood estimate of
the variance from a single sample,
$\widehat{\sigma^2}=\sum_{i=1}^n(y_i-\bar{y})^2/n$ or the maximum
likelihood estimate of the variance in a linear regression model with
$p$ coefficients in the predictor,
$\widehat{\sigma^2}=\sum_{i=1}^n(y_i-\hat{y}_i)^2/n$.

These variance estimates are not used in practice because they are
biased downward.  This is, on average they will underestimate the
variance in the model.  Instead we use 
$\widehat{\sigma^2}_R=\sum_{i=1}^n(y_i-\bar{y})^2/(n-1)$ for the
variance estimate from a single sample or
$\widehat{\sigma^2}_R=\sum_{i=1}^n(y_i-\hat{y}_i)^2/(n-p)$ for the
variance estimate in a linear regression model.  These estimates are
based on the residuals, $y_i-\hat{y}_i, i=1,\dots,n$ which satisfy $p$
linear constraints and thus are constrained to an $n-p$-dimensional
subspace of the $n$-dimensional sample space.  In other words, the
residuals have only $n-p$ degrees of freedom.

In a mixed-effects model we often prefer to estimate the variance
components, $\sigma^2$ and $\bm{\Sigma}$, according to the
\emph{residual maximum likelihood} (REML) criterion (sometimes
called the \emph{restricted} maximum likelihood criterion) which
compensates for the estimation of the fixed-effects parameters when
estimating the random effects.  

The REML criterion can be expressed as
\begin{equation}
  \label{eq:REMLcrit}
  \begin{aligned}
    L_R(\bm{\theta},\sigma^2|\bm{y})
    &=\int_{\bm{\beta}}L(\bm{\beta},\bm{\theta},\sigma^2|\bm{y})\,d\bm{\beta}\\
    &=\frac{e^{-r^2/(2\sigma^2)}}{|\bm{L}|(2\pi\sigma^2)^{(n-p)/2}}
      \int_{\bm{\beta}}\frac{1}{(2\pi\sigma^2)^{p/2}}
      e^{-(\bm{\beta}-\widehat{\bm{\beta}})\trans\bm{L}_X
          \bm{L}_X\trans(\bm{\beta}-\widehat{\bm{\beta}})/(2\sigma^2)}
        \,d\bm{\beta}\\
    &=\frac{e^{-r^2/(2\sigma^2)}}{|\bm{L}||\bm{L}_X|(2\pi\sigma^2)^{(n-p)/2}}
  \end{aligned}
\end{equation}
or, on the deviance scale,
\begin{equation}
  \label{eq:REMLcrit}
  \begin{aligned}
    -2\ell_R(\bm{\theta},\sigma^2|\bm{y})
    &=(n-p)\log\left(2\pi\sigma^2\right)+\frac{r^2}{\sigma^2}+2|\bm{L}|+2|\bm{L}_X|
  \end{aligned}
\end{equation}
from which we can see that the REML estimate of $\sigma^2$ is
\begin{equation}
  \label{eq:REMLsigma}
  \widehat{\sigma}_R(\bm{\theta})=\frac{r^2}{n-p}
\end{equation}
and the profiled REML deviance is
\begin{multline}
  \label{eq:REMLsigma}
    -2\ell_R(\bm{\theta},\widehat{\sigma}^2(\bm{\theta})|\bm{y})=\\
(n-p)\left[1+\log\left(\frac{2\pi}{n-p}\right)\right]+(n-p)\log r^2+2\log|\bm{L}|+2\log|\bm{L}_X|
\end{multline}

\section{Nonlinear mixed models}
\label{sec:nonlinearmixed}

Like the linear mixed model, the nonlinear mixed model is based on a
multivariate normal (or Gaussian) distribution of the response
$\bm{y}$ given $\bm{\mu}$.  That is,
\begin{equation}
  \label{eq:nlmm}
  \bm{y}=\bm{\mu}(\bm{\beta},\bm{b})+\bm{\epsilon},\quad
  \bm{\epsilon}\sim\mathcal{N}(\bm{0},\sigma^2\bm{I}),\quad
  \bm{b}\sim\mathcal{N}(\bm{0},\sigma^2\bm{\Sigma}(\bm{\theta}),
  \quad\bm{\epsilon}\perp\bm{b}
\end{equation}
and the discrepancy function and normalizing constant are the same as
for the linear mixed model
\begin{align}
  \label{eq:nlmmProps}
  d(\bm{\mu},\bm{y})&=\left\|\bm{\mu}-\bm{y}\right\|^2\\
  k(\sigma^2)&=\frac{1}{\left(2\pi\sigma^2\right)^{n/2}} .
\end{align}

The mean, $\bm{\mu}$, however, is no longer equal to the linear
predictor.  Each element of $\bm{\mu}$ is the value of a nonlinear
model function $g(\bm{x},\bm{\phi})$ that depends on covariates,
$\bm{x}$, and a nonlinear model parameter, $\bm{\phi}$, of length $s$.
When fitting a model the values of the covariates at each observation
are known so we can regard $\bm{\mu}_i$ as a function of $\bm{\phi}_i$ only
and write
\begin{equation}
  \label{eq:vectorg}
  \bm{\mu}=\bm{g}(\bm{\Phi})
\end{equation}
where $\bm{\Phi}$ is the $n\times s$ matrix 
whose $i$th row is $\bm{\phi}_i,i=1,\dots,n$ and the vector-valued
function $\bm{g}$ applies the scalar function $g$ rowwise to
$\bm{\Phi}$ using covariates $\bm{x}_i,i=1,\dots,n$ for the $i$th row.

The linear predictor determines $\bm{\Phi}$ as
\begin{equation}
  \label{eq:linpred}
  \mathrm{vec}(\bm{\Phi})=\bm{X}\bm{\beta}+\bm{Z}\bm{b}=\bm{X}\bm{\beta}+\bm{V}\bm{u}
\end{equation}
where the $\mathrm{vec}$ operator concatenates the columns of
$\bm{\Phi}$ to form a vector of length $m=ns$.  Thus the matrix
$\bm{X}$ is $ns\times p$ while $\bm{Z}$ and $\bm{V}$ are $ns\times q$.

\subsection{Optimizing the penalized discrepancy}
\label{sec:nlmmpenalizedLS}

As for a linear mixed model, the problem of determining
$\tilde{\bm{u}}(\bm{\beta},\bm{\theta})$, the optimizer of the
penalized discrepancy function, can be written as a penalized least
squares problem
\begin{equation}
  \label{eq:nlmmdisc}
  \tilde{\bm{u}}(\bm{\beta},\bm{\theta})=\arg\min_{\bm{u}}
  \delta(\bm{u}|\bm{\beta},\bm{\theta},\bm{y})=\arg\min_{\bm{u}}\left[
    \|\bm{\mu}(\bm{\beta},\bm{u})-\bm{y}\|^2+\bm{u}\trans\bm{u}\right].
\end{equation}
Unlike the case of the linear mixed model this is generally a
penalized nonlinear least squares problem that requires an iterative
solution.

Given $\bm{u}^{(i)}$ (the parenthesized superscripts denote the number
of the iteration at which a quantity is evaluated) we evaluate
\begin{align}
  \label{eq:nlmmgrad}
  \left.\frac{\partial\bm{\mu}}{\partial\bm{u}\trans}\right|_{\bm{u}=\bm{u}^{(i)}}&=\bm{M}^{(i)}\\
  &=
  \begin{bmatrix}
    \bm{I}&\bm{I}&\dots&\bm{I}
  \end{bmatrix}
  \mathrm{diag}\left(\mathrm{vec}\left.\frac{d\bm{\mu}}{d\bm{\Phi}}\right|_{\bm{\Phi}=\bm{\Phi}^{(i)}}\right)
  \bm{V} ,
\end{align}
where the matrix on the left is the horizontal concatenation of $s$
copies of the $n\times n$ identity matrix, then solve for
$\bm{u}^{(i+1)}$ in
\begin{equation}
  \label{eq:uip1}
  \left({\bm{M}^{(i)}}\trans\bm{M}^{(i)}+\bm{I}\right)\bm{u}^{(i+1)}=
  {\bm{M}^{(i)}}\trans\left(\bm{y}-\bm{\mu}^{(i)}+\bm{M}^{(i)}\bm{u}^{(i)}\right)
\end{equation}
using the Cholesky decomposition
\begin{equation}
  \label{eq:nlmmChol}
  \left({\bm{M}^{(i)}}\trans\bm{M}^{(i)}+\bm{I}\right)={\bm{L}^{(i)}}\trans\bm{L}^{(i)} .
\end{equation}

At convergence the Laplace approximation to the deviance is
\begin{equation}
  \label{eq:nlmmLaplace}
  -2\ell(\bm{\beta},\bm{\theta},\sigma^2|\bm{y})
  =n\log\left(2\pi\sigma^2\right)+\frac{\delta(\tilde{\bm{u}}|\bm{\theta},\bm{\beta})}
  {\sigma^2}+2\log|\bm{L}| .
\end{equation}
As in the linear mixed model we can form the conditional estimate of $\sigma^2$
\begin{equation}
  \label{eq:nlmmsigmasq}
  \widehat{\sigma^2}(\bm{\theta},\bm{\beta})=
  \frac{\delta(\tilde{\bm{u}}|\bm{\theta},\bm{\beta})}{n} .
\end{equation}
Substituting this estimate into (\ref{eq:nlmmLaplace}) produces the
Laplace approximation to the profiled deviance
\begin{equation}
  \label{eq:nlmmprofdevLaplace}
  -2\ell(\bm{\beta},\bm{\theta},\widehat{\sigma^2}(\bm{\beta},\bm{\theta})|\bm{y})
  =n\left[1+\log\left(\frac{2\pi}{n}\right)\right]+
  n\log \delta(\tilde{\bm{u}}|\bm{\theta},\bm{\beta})+2\log|\bm{L}| .
\end{equation}

\subsection{Random effects for conditionally linear parameters only}
\label{sec:nlmmcondlin}

There is a special case of a nonlinear mixed model where the Laplace
approximation is the deviance and where the iterative algorithm to
determine $\tilde{\bm{u}}(\bm{\beta}, \bm{\theta}, \bm{y})$ will
converge in one iteration.  Frequently some of the elements of the
parameter vector $\bm{\phi}$ occur linearly in the nonlinear model
$g(\bm{x}, \bm{\phi})$.  These elements are said to be
\emph{conditionally linear} parameters because, conditional on the
values of the other parameters, the model function is a linear
function of these.

If the random effects determine only conditionally linear parameters
then the matrix $\bm{M}$ depends on $\bm{\beta}$ and $\bm{\theta}$ but
not on $\bm{u}$.  We can rewrite the mean function as
\begin{equation}
  \label{eq:condlinmu}
  \bm{\mu}(\bm{\beta}, \bm{u})=\bm{\eta}(\bm{\beta})+\bm{M}\bm{u}
\end{equation}
and minimizing the penalized deviance is equivalent to
\begin{multline}
  \label{eq:condlinupdate}
  \min_{\bm{u}}\left(\|\bm{y}-\bm{\eta}(\bm{\beta})-\bm{M}\bm{u}\|^2
      +\bm{u}\trans\bm{u}\right)\\
      = \min_{\bm{u}}\left\|
        \begin{bmatrix}
          \bm{y}+\bm{M}\bm{u}^{(0)}-\bm{\mu}(\bm{\beta},\bm{u}^{(0)})\\
          \bm{0}
        \end{bmatrix}
        - \begin{bmatrix}\bm{M}\\\bm{I}\end{bmatrix}\bm{u}\right\|^2 .
\end{multline}
That is, $\tilde{\bm{u}}(\bm{\beta},\bm{\theta},\bm{y})$ is the
solution to
\begin{equation}
  \label{eq:condlinsol}
  \left(\bm{M}\trans\bm{M}+\bm{I}\right)\tilde{\bm{u}}=
  \bm{M}\trans\left(\bm{y}+\bm{M}\bm{u}^{(0)}-\bm{\mu}(\bm{\beta},\bm{u}^{(0)})\right)
\end{equation}
\bibliography{lme4}
\end{document}

As shown below for a linear mixed model we can minimize the penalized
discrepancy and calculate $\bm{L}$ directly.  For generalized linear
mixed models or nonlinear mixed models we can minimize the penalized
discrepancy by some form of iterative penalized least squares.
For each of the model types we consider: linear mixed models or
generalized linear mixed models or nonlinear mixed models, there is a
least squares or iterative least squares algorithm to minimize the
discrepancy in a model that does not incorporate random effects.  That
is, we can use least squares to minimize the discrepancy in a linear
model, iteratively reweighted least squares to minimize the
discrepancy in a generalized linear model and iterative least squares
(the Gauss-Newton algorithm) to minimize the discrepancy in a
nonlinear regression model.  To minimize the penalized discrepancy in
the corresponding mixed model we use a penalized least squares
algorithm or an iterative penalized least squares algorithm.

 Just as we
can minimize the discrepancy by least squares for linear models or
iteratively reweighted least squares for generalized linear models or
model, we can minimize the penalized discrepancy by penalized least
squares (
For the
models we will consider it is possible to minimize the penalized discrepancy
