\documentclass[12pt]{article}
\usepackage{Sweave}
\usepackage{amsmath}
\usepackage{bm}
\usepackage[authoryear,round]{natbib}
\bibliographystyle{plainnat}
\DefineVerbatimEnvironment{Sinput}{Verbatim}
{formatcom={\vspace{-2.5ex}},fontshape=sl,
  fontfamily=courier,fontseries=b, fontsize=\scriptsize}
\DefineVerbatimEnvironment{Soutput}{Verbatim}
{formatcom={\vspace{-2.5ex}},fontfamily=courier,fontseries=b,%
  fontsize=\scriptsize}
%%\VignetteIndexEntry{Implementation Details}
%%\VignetteDepends{lme4}
\newcommand{\trans}{\ensuremath{^\mathsf{T}}}
\newcommand{\invtrans}{\ensuremath{^\mathsf{-T}}}
\title{Linear mixed model implementation in lme4}
\author{Douglas Bates\\Department of Statistics\\%
  University of Wisconsin -- Madison}
\begin{document}
\SweaveOpts{engine=R,eps=FALSE,pdf=TRUE,strip.white=TRUE}
\SweaveOpts{prefix=TRUE,prefix.string=figs/Example,include=FALSE}
\setkeys{Gin}{width=\textwidth}

\maketitle
\begin{abstract}
  We present the form of the model and details of the computational
  methods in \code{lmer} for linear mixed models and for generalized
  linear mixed models.  These techniques are illustrated on several
  examples.
\end{abstract}

<<preliminaries,echo=FALSE,print=FALSE>>=
library(lattice)
library(Matrix)
library(lme4)
data("Rail", package = "nlme")
Rail <- data.frame(travel = Rail$travel, Rail = Rail$Rail)
options(width=80, show.signif.stars = FALSE,
        lattice.theme = function() canonical.theme("pdf", color = FALSE))
@

\section{A simple example}
\label{sec:example}

The \code{Rail} data set from the \package{nlme} package is described
in \citet{pinheiro:bates:2000} as consisting of three measurements of
the travel time of a type of sound wave on each of six sample railroad
rails. We can examine the structure of these data with the \code{str}
function
<<strRail>>=
str(Rail)
@

Because there are only three observations on each of the rails a
dotplot (Figure~\ref{fig:Raildotplot}) shows the structure of the data
well.
<<Raildotplot,fig=TRUE,include=FALSE,width=8,height=4>>=
print(dotplot(Rail~travel,Rail,xlab="Travel time (ms)",ylab="Rail number"))
@ 
\begin{figure}[tb]
  \centering
  \includegraphics{figs/Example-Raildotplot}
  \caption{Travel time of sound waves in a sample of six railroad rails.  There were three measurements of the travel time on each rail. The numbering of the rails has been reordered according to increasing mean travel time.}
  \label{fig:Raildotplot}
\end{figure}

In building a model for these data
<<Raildata>>=
Rail
@ 
we wish to characterize a typical travel time, say $\mu$, for the
population of such railroad rails and the deviations, say
$b_i,i=1,\dots,6$ of the individual rails from this population mean.
Because these specific rails are not of interest by themselves as much
as the variation in the population we model the $b_i$, which are
called the ``random effects'' for the rails, as having a normal
(Gaussian) distribution of the form $\mathcal{N}(0,\sigma^2_b)$.  The
$j$th measurement on the $i$th rail is expressed as
\begin{equation}
  \label{eq:1}
  y_{ij}=\mu+b_i+\epsilon_{ij}\quad
  b_i\sim\mathcal{N}(0,\sigma^2_b),\epsilon_{ij}\sim\mathcal{N}(0,\sigma^2)
  \quad  i=1,\dots,6\;j=1,\dots,3
\end{equation}

The parameters of this model are $\mu$, $\sigma^2_b$ and $\sigma^2$.
Technically the $b_i,i=1,\dots,6$ are not parameters but instead are
considered to be unobserved random variables for which we form
``predictions'' instead of ``estimates''.

To express generalizations of models like (\ref{eq:1}) more
conveniently we change the representation to a 

\section{Form of the model}
\label{sec:model}

A linear mixed model for a $n$-dimensional response vector $\bm{y}$ can
be written as
\begin{equation}
  \label{eq:lmm}
  \bm{y} = \bm{X}\bm{\beta} + \bm{Z}\bm{b}+\bm{\epsilon},\quad
  \bm{\epsilon}\sim\mathcal{N},\left(\bm{0},\sigma^2\bm{I}\right),\;
  \bm{b}\sim\mathcal{N}\left(\bm{0},\sigma^2\bm{\Sigma}\right)\;\mathrm{and}\;
  \bm{b}\perp\bm{\epsilon}
\end{equation}
where $\bm{\beta}$ is a $p$-dimensional fixed-effects parameter vector
relative to the $n\times p$ model matrix $\bm{X}$, $\bm{b}$ is a
$q$-dimensional random-effects vector relative to the $n\times q$
model matrix $\bm{Z}$ and $\bm{\Sigma}$ is the $q\times q$ relative
variance-covariance matrix for the random-effects.  The symbol $\perp$
indicates independence of random variables and $\mathcal{N}$ denotes
the multivariate normal (Gaussian) distribution.

We say that matrix $\bm{\Sigma}$ is the relative variance-covariance
matrix of the random effects in the sense that it is the variance of
$\bm{b}$ relative to $\sigma^2$, the scalar variance of the
per-observation noise term $\bm{\epsilon}$.  Although it size, $q$,
can be very large, $\bm{\Sigma}$ is highly structured.  It is
symmetric, positive semi-definite and zero except for the diagonal
elements and certain elements close to the diagonal.

\section{Structure of $\bm{\Sigma}$ and $\bm{Z}$}
\label{sec:role-group-fact}

The columns of $\bm{Z}$ and the rows and columns of $\bm{\Sigma}$ are
associated with the levels of one or more grouping factors in the
data.  For example, a common application of linear mixed models is the
analysis of students' scores on the annual state-wide performance
tests mandated by the No Child Left Behind Act.  A given score is
associated with a student, a teacher, a school and a school district.
These could all be grouping factors in a model.

We write the grouping factors as $\bm{f}_i,i=1,\dots k$.  The number
of levels of the $i$th factor, $\bm{f}_i$, is $n_i$ and the number of
random effects associated with each level is $q_i$.  For example, if
$\bm{f}_1$ is ``student'' then $n_1$ is the number of students in the
study.  If we have a simple additive random effect for each student
then $q_1=1$.  If we have a random effect for both the intercept and
the slope with respect to time for each student then $q_1=2$.  The
$q_i,i=1,\dots,k$ are typically very small whereas the
$n_i,i=1,\dots,k$ can be very large.

In the statistical model we assume that random effects associated with
different grouping factors are independent, which implies that
$\bm{\Sigma}$ is block diagonal with $k$ diagonal blocks of sizes $n_i
q_i\times n_i q_i, i = 1,\dots,k$.  That is
\begin{equation}
  \label{eq:blockDiag}
  \bm{\Sigma}=
  \begin{bmatrix}
    \bm{\Sigma}_1 & \bm{0} & \hdots & \bm{0}\\
    \bm{0} & \bm{\Sigma}_2 & \hdots & \bm{0}\\
    \vdots & \vdots        & \ddots & \vdots \\
    \bm{0} & \bm{0}        & \hdots & \bm{\Sigma}_k
  \end{bmatrix}
\end{equation}
Furthermore, random effects associated
with different levels of the same grouping factor are assumed to be
independent and identically distributed, which implies that
$\bm{\Sigma}_i$ is itself block diagonal in $n_i$ blocks and that each
of these blocks is a copy of a $q_i\times q_i$ matrix
$\tilde{\bm{\Sigma}}_i$.  That is
\begin{equation}
  \label{eq:blockblock}
  \bm{\Sigma}_i=
    \begin{bmatrix}
    \tilde{\bm{\Sigma}}_i & \bm{0}     & \hdots & \bm{0}\\
    \bm{0}     & \tilde{\bm{\Sigma}}_i  & \hdots & \bm{0}\\
    \vdots    & \vdots    & \ddots & \vdots\\
    \bm{0}     & \bm{0}     & \hdots & \tilde{\bm{\Sigma}}_i
  \end{bmatrix}=
  \bm{I}_{n_i}\otimes\tilde{\bm{\Sigma}}_i\quad i=1,\dots,k
\end{equation}
where $\otimes$ denotes the Kronecker product.

The condition that $\bm{\Sigma}$ is positive semi-definite holds if
and only if the $\tilde{\bm{\Sigma}}_i,i=1,\dots,k$  are positive
semi-definite.  To ensure that the $\tilde{\bm{\Sigma}}_i$ are
positive semi-definite, we express them as
\begin{equation}
  \label{eq:STinner}
  \tilde{\bm{\Sigma}}_i=
  \tilde{\bm{S}}_i\tilde{\bm{T}}_i\tilde{\bm{T}}_i\trans\tilde{\bm{S}}_i,\quad
  i=1,\dots,k
\end{equation}
where $\tilde{\bm{T}}_i$ is a $q_i\times q_i$ unit lower-triangular
matrix (i.e. all the elements above the diagonal are zero and all the
diagonal elements are unity) and $\tilde{\bm{S}}_i$ is a $q_i\times
q_i$ diagonal matrix with non-negative elements on the diagonal. (This
is a form of the Cholesky decomposition of positive semi-definite
matrices.  It is similar to the ``LDL'' form but not identical to it.
See Pouramadi (2007) for some discussion of the statistical advantages
of using this form of the decomposition.)

The $n_i q_i\times n_i q_i$ matrices $\bm{S}_i,\bm{T}_i,\,i=1,\dots,k$
and the $q\times q$ matrices $\bm{S}$ and $\bm{T}$ are defined
analogously to (\ref{eq:blockblock}) and (\ref{eq:blockDiag}).  In
particular,
\begin{align}
  \bm{S}_i&=\bm{I}_{n_i}\otimes\tilde{\bm{S}}_i,\quad i=1,\dots,k \\
  \bm{T}_i&=\bm{I}_{n_i}\otimes\tilde{\bm{T}}_i,\quad i=1,\dots,k
\end{align}

Note that when $q_i=1$, $\tilde{\bm{T}}_i=\bm{I}$ and hence
$\bm{T}_i=\bm{I}$ and $\bm{S}_i$ is a multiple of the identity.

The parameter vector $\bm{\theta}_i,i=1,\dots,k$ consists of the $q_i$
diagonal elements of $\tilde{\bm{S}}_i$, which are constrained to be
non-negative, followed by the $q_i(q_i-1)/2$ elements in the strict
lower triangle of $\tilde{\bm{T}}_i$ (in column-major ordering).
These last $q_i(q_i-1)/2$ elements are unconstrained.  The
$\bm{\theta}_i$ are combined as
\begin{displaymath}
  \bm{\theta}=
  \begin{bmatrix}
    \bm{\theta}_1 \\ \bm{\theta}_2 \\ \vdots \\ \bm{\theta}_k
  \end{bmatrix} .
\end{displaymath}
Each of the $q\times q$ matrices $\bm{S}$, $\bm{T}$ and $\bm{\Sigma}$
in the decomposition $\bm{\Sigma}=\bm{S}\bm{T}\bm{T}\trans\bm{S}$ is a
function of $\bm{\theta}$.  

As a unit triangular matrix $\bm{T}$ is non-singular.  That is,
$\bm{T}^{-1}$ exists and is easily calculated from the
$\tilde{\bm{T}}_i^{-1},i=1,\dots,k$.  When $\bm{\theta}$ is not on the
boundary defined by the constraints, $\bm{S}$ is a
diagonal matrix with strictly positive elements on the diagonal,
which implies that $\bm{S}^{-1}$ exists and that
$\bm{\Sigma}$ is non-singular with
$\bm{\Sigma}^{-1}=\bm{S}^{-1}\bm{T}\invtrans\bm{T}^{-1}\bm{S}^{-1}$.

When $\bm{\theta}$ is on the boundary the matrices $\bm{S}$
and $\bm{\Sigma}$ exist but are not invertible.  We say that
$\bm{\Sigma}$ is a \emph{degenerate} variance-covariance matrix in the
sense that one or more linear combinations of the vector $\bm{b}$ are
defined to have zero variance.  That is, the distribution of these
linear combinations is a point mass at 0.

The maximum likelihood estimates of $\bm{\theta}$ (or the restricted
maximum likelihood estimates, defined below) can be on the
boundary.  That is, they can correspond to a degenerate
variance-covariance matrix and we must be careful to allow for this
case.  However, to begin we consider the non-degenerate case.

\section{Methods for non-singular $\bm{\Sigma}$}
\label{sec:non-singular}

When $\bm{\theta}$ is not on the boundary we can define a
standardized random effects vector 
\begin{equation}
  \label{eq:bstar}
  \bm{b}^*=\bm{T}^{-1}\bm{S}^{-1}\bm{b}
\end{equation}
with the properties
\begin{align}
  \mathsf{E}[\bm{b}^*]&=\bm{T}^{-1}\bm{S}^{-1}\mathsf{E}[\bm{b}]\\
  \mathsf{Var}[\bm{b}^*]&=\mathsf{E}[\bm{b}^*{\bm{b}^*}\trans]=\bm{T}^{-1}\bm{S}^{-1}\mathsf{Var}[\bm{b}]\bm{S}^{-1}\bm{T}\invtrans=\sigma^2\bm{T}^{-1}\bm{S}^{-1}\bm{\Sigma}\bm{S}^{-1}\bm{T}\invtrans=\sigma^2\bm{I}.
\end{align}
That is, the unconditional distribution of the $q$ elements of
$\bm{b}^*$ is
$\bm{b}^*\sim\mathcal{N}\left(\bm{0},\sigma^2\bm{I}\right)$, like that
of the $n$ elements of $\bm{\epsilon}$.

Obviously the transformation from $\bm{b}^*$ to $\bm{b}$ is 
\begin{equation}
  \label{eq:bstarb}
  \bm{b}=\bm{S}\bm{T}\bm{b}^*
\end{equation}
and the $n\times q$ model matrix for $\bm{b}^*$ is 
\begin{equation}
  \label{eq:Zstar}
  \bm{Z}^*=\bm{Z}\bm{S}\bm{T}
\end{equation}
so that
\begin{equation}
  \label{eq:Zb}
  \bm{Z}^*\bm{b}^*=\bm{Z}\bm{S}\bm{T}\bm{T}^{-1}\bm{S}^{-1}\bm{b}=\bm{Z}\bm{b} .
\end{equation}

Notice that $\bm{Z}^*$ can be evaluated even when
$\bm{\theta}$ is on the boundary.  Also, if we have a value of
$\bm{b}^*$ in such a case, we can evaluated $\bm{b}$ from $\bm{b}^*$.
 
Given the data $\bm{y}$ and values of $\bm{\theta}$ and $\bm{\beta}$,
the mode of the conditional distribution of $\bm{b}^*$ is the solution
to a penalized least squares problem
\begin{equation}
  \label{eq:bstarmode}
  \begin{split}
    \tilde{\bm{b}^*}(\bm{\theta},\bm{\beta}|\bm{y})&=
    \arg\min_{\bm{b}^*} \left[
    \left\|\bm{y}-\bm{X}\bm{\beta}-\bm{Z}^*\bm{b}^*\right\|^2
    +{\bm{b}^*}\trans\bm{b}^*\right]\\
    &=
    \arg\min_{\bm{b}^*}
    \left\|
      \begin{bmatrix}
        \bm{y}\\
        \bm{0}
      \end{bmatrix} -
      \begin{bmatrix}
        \bm{Z}^* & \bm{X}\\
        \bm{I}   & \bm{0}
      \end{bmatrix}
      \begin{bmatrix}
        \bm{b}^*\\
        \bm{\beta}
      \end{bmatrix}
      \right\|^2
  \end{split}
\end{equation}
In fact, if we optimize penalized least squares expression in
(\ref{eq:bstarmode}) with respect to both $\bm{b}$ and $\bm{\beta}$ we
obtain the conditional estimates
$\hat{\bm{\beta}}(\bm{\theta}|\bm{y})$ and the conditional modes 
$\tilde{\bm{b}^*}(\bm{\theta},\hat{\bm{\beta}}(\bm{\theta})|\bm{y}))$
which we write as $\widehat{\bm{b}^*}(\bm{\theta})$.  That is,
\begin{equation}
  \label{eq:bstarbeta}
  \begin{split}
    \begin{bmatrix}
      \widehat{\bm{b}^*}(\bm{\theta})\\
      \hat{\bm{\beta}}(\bm{\theta})
    \end{bmatrix} & =
    \arg\min_{\bm{b}^*,\bm{\beta}}
    \left\|
      \begin{bmatrix}
        \bm{Z}^* & \bm{X} & -\bm{y}\\
        \bm{I}   & \bm{0} &  \bm{0}
      \end{bmatrix}
      \begin{bmatrix}
        \bm{b}^*\\
        \bm{\beta}\\
        1
      \end{bmatrix}
    \right\|^2\\
     & =
    \arg\min_{\bm{b}^*,\bm{\beta}}
    \begin{bmatrix}
      \bm{b}^*\\
      \bm{\beta}\\
      1
    \end{bmatrix}\trans
    \bm{A}^*(\bm{\theta})
    \begin{bmatrix}
      \bm{b}^*\\
      \bm{\beta}\\
      1
    \end{bmatrix}
  \end{split}
\end{equation}
where the matrix $\bm{A}^*(\bm{\theta})$ is
\begin{equation}
  \label{eq:Astar}
  \begin{split}
    \bm{A}^*(\bm{\theta})&=
    \begin{bmatrix}
      {\bm{Z}^*}\trans\bm{Z}^*+\bm{I} & {\bm{Z}^*}\trans\bm{X} &
      -{\bm{Z}^*}\trans\bm{y}\\
      {\bm{X}}\trans\bm{Z}^* & {\bm{X}}\trans\bm{X} &
      -{\bm{X}}\trans\bm{y}\\
      -{\bm{y}}\trans\bm{Z}^*&-{\bm{y}}\trans\bm{X}&{\bm{y}}\trans\bm{y}
    \end{bmatrix} \\
    &=
    \begin{bmatrix}
      \bm{T}\trans\bm{S} & \bm{0} & \bm{0} \\
      \bm{0} & \bm{I} & \bm{0} \\
      \bm{0} & \bm{0} & 1
    \end{bmatrix}
    \bm{A}
    \begin{bmatrix}
      \bm{S}\bm{T} & \bm{0} & \bm{0} \\
      \bm{0} & \bm{I} & \bm{0} \\
      \bm{0} & \bm{0} & 1
    \end{bmatrix}+
    \begin{bmatrix}
      \bm{I} & \bm{0} & \bm{0} \\
      \bm{0} & \bm{0} & \bm{0} \\
      \bm{0} & \bm{0} & 0
    \end{bmatrix} .
  \end{split}
\end{equation}
and
\begin{equation}
  \label{eq:Amatrix}
  \bm{A}=
  \begin{bmatrix}
    \bm{Z}\trans\bm{Z} & \bm{Z}\trans\bm{X} & -\bm{Z}\trans\bm{y} \\
    \bm{X}\trans\bm{Z} & \bm{X}\trans\bm{X} & -\bm{X}\trans\bm{y} \\    
    -\bm{y}\trans\bm{Z} & -\bm{y}\trans\bm{X} & \bm{y}\trans\bm{y} \\    
  \end{bmatrix} .
\end{equation}
Note that $\bm{A}$ does not depend upon $\bm{\theta}$.  Furthermore,
the nature of the model matrices $\bm{Z}$ and $\bm{X}$ ensures that
the pattern of nonzeros in $\bm{A}^*(\bm{\theta})$ is the same as that
in $\bm{A}$.

Let the $q\times q$ permutation matrix $\bm{P}_Z$ represent a
fill-reducing permutation for $\bm{Z}\trans\bm{Z}$ and $\bm{P}_X$, of
size $p\times p$, represent a fill-reducing permutation for
$\bm{X}\trans\bm{X}$.  These could be determined, for example, using
the \emph{approximate minimal degree} (AMD) algorithm described in
\citet{davis06:csparse_book} and \citet{Davis:1996} and implemented in
both the \texttt{\small Csparse} \citep{Csparse} and the
\texttt{\small CHOLMOD} \citep{Cholmod} libraries of C functions.  (In
many cases $\bm{X}\trans\bm{X}$ is dense, but of small dimension
compared to $\bm{Z}\trans\bm{Z}$, and $\bm{Z}\trans\bm{X}$ is nearly
dense so $\bm{P}_X$ can be $\bm{I}_p$, the $p\times p$ identity
matrix.)

Let the permutation matrix $\bm{P}$ be
\begin{equation}
  \label{eq:fillPerm}
  \bm{P}=
  \begin{bmatrix}
    \bm{P}_Z & \bm{0} & \bm{0} \\
    \bm{0} & \bm{P}_X & \bm{0} \\
    \bm{0} & \bm{0} & 1
  \end{bmatrix} 
\end{equation}
and $\bm{L}(\bm{\theta})$ be the sparse Cholesky decomposition of
$\bm{A}^*(\bm{\theta})$ relative to this permutation.  That
is, $\bm{L}(\bm{\theta})$ is a sparse lower triangular matrix with the
property that
\begin{equation}
  \label{eq:Lmat}
  \bm{L}(\bm{\theta})\bm{L}(\bm{\theta})\trans=
  \bm{P}\bm{A}^*(\bm{\theta})\bm{P}\trans
\end{equation}

For $\bm{L}(\bm{\theta})$ to exist we must ensure that
$\bm{A}^*(\bm{\theta})$ is positive definite.  Examination of
(\ref{eq:bstarbeta}) shows that 
this will be true if
$\bm{X}$ is of full column rank and $\bm{y}$ does not lie in the
column span of $\bm{X}$ (or, in statistical terms, if we can't fit
$\bm{y}$ perfectly using only the fixed effects).

Let $r>0$ be the last element on the diagonal of $\bm{L}$.  Then the
minumum penalized residual sum of squares in (\ref{eq:bstarbeta}) is
$r^2$ and it occurs at $\widehat{\bm{b}^*}(\bm{\theta})$ and
$\hat{\beta}(\bm{\theta})$, the solutions to the sparse triangular system
\begin{equation}
  \label{eq:soln}
  \bm{L}(\bm{\theta})\trans\bm{P}
  \begin{bmatrix}
    \widehat{\bm{b}^*}(\bm{\theta})\\
    \hat{\bm{\beta}}(\bm{\theta})\\
    1
  \end{bmatrix}=
  \begin{bmatrix}
    \bm{0}\\
    \bm{0}\\
    r
  \end{bmatrix}
\end{equation}
(Technically we should not write the $1$ in the solution; it should be
an unknown.  However, for $\bm{L}$ lower triangular with $r$ as the last
element on the diagonal and $\bm{P}$ a permutation that does not move
the last row, the solution for this ``unknown'' will always be $1$.)
Furthermore, $\log|{\bm{Z}^*}\trans\bm{Z}+\bm{I}|$ can be evaluated as
the sum of the logarithms of the first $q$ diagonal elements of
$\bm{L}(\bm{\theta})$.  

The \emph{profiled deviance function}, $d(\bm{\theta})$, which is negative
twice the log-likelihood of model (\ref{eq:lmm}) evaluated at
$\bm{\Sigma}(\bm{\theta})$, $\hat{\bm{\beta}}(\bm{\theta})$ and
$\hat{\sigma}^2(\bm{\theta})$, can be evaluated as
\begin{equation}
  \label{eq:dtheta}
  d(\bm{theta})=
    \log\left|{\bm{Z}^*}\trans\bm{Z}+\bm{I}\right|+
    n\left(1+\log\frac{2\pi r^2}{n}\right) .
\end{equation}

Notice that it is not necessary to solve for
$\hat{\bm{\beta}}(\bm{\theta})$ or $\widehat{\bm{b}^*}(\bm{\theta})$
or $\widehat{\bm{b}}(\bm{\theta})$ to be able to evaluate
$d(\bm{\theta})$.  All that is needed is to update $\bm{A}$ to form
$\bm{A}^*$ from which the sparse Cholesky decomposition
$\bm{L}(\bm{\theta})$ can be calculated.

\section{Methods for singular $\bm{\Sigma}$}
\label{sec:singular}

When $\bm{\theta}$ is on the boundary, corresponding to a singular
$\bm{\Sigma}$, some of the columns of $\bm{Z}^*$ are zero.  However,
the matrix $\bm{A}^*$ is non-singular and elements of $\bm{b}^*$
corresponding to the zeroed columns in $\bm{Z}^*$ approach zero
smoothly as $\bm{\theta}$ approaches the boundary.  Thus
$r(\bm{\theta})$ and $\left|{\bm{Z}^*}\trans\bm{Z}+\bm{I}\right|$ are
well-defined, as is $d(\bm{\theta})$ and the conditional modes
$\widehat{\bm{b}}(\bm{\theta})$.

In other words, (\ref{eq:Astar}) and (\ref{eq:Lmat}) can be used to
define $d(\bm{\theta})$ whether or not $\bm{\theta}$ is on the
boundary.

\bibliography{lme4}
\end{document}
