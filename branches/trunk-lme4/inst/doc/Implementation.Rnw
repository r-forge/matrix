\documentclass[12pt]{article}
\usepackage{Sweave}
\usepackage{amsmath}
\usepackage{bm}
\usepackage[authoryear,round]{natbib}
\bibliographystyle{plainnat}
\DefineVerbatimEnvironment{Sinput}{Verbatim}
{formatcom={\vspace{-2.5ex}},fontshape=sl,
  fontfamily=courier,fontseries=b, fontsize=\scriptsize}
\DefineVerbatimEnvironment{Soutput}{Verbatim}
{formatcom={\vspace{-2.5ex}},fontfamily=courier,fontseries=b,%
  fontsize=\scriptsize}
%%\VignetteIndexEntry{Implementation Details}
%%\VignetteDepends{lme4}
\newcommand{\trans}{\ensuremath{^\mathsf{T}}}
\newcommand{\invtrans}{\ensuremath{^\mathsf{-T}}}
\title{Linear mixed model implementation in lme4}
\author{Douglas Bates\\Department of Statistics\\%
  University of Wisconsin -- Madison}
\begin{document}
\SweaveOpts{engine=R,eps=FALSE,pdf=TRUE,strip.white=TRUE}
\SweaveOpts{prefix=TRUE,prefix.string=figs/Example,include=FALSE}
\setkeys{Gin}{width=\textwidth}

\maketitle
\begin{abstract}
  We present the form of the model and the 
\end{abstract}

<<preliminaries,echo=FALSE,print=FALSE>>=
library(lattice)
library(Matrix)
library(lme4)
#data("Early", package = "mlmRev")
options(width=80, show.signif.stars = FALSE,
        lattice.theme = function() canonical.theme("pdf", color = FALSE))
@
\section{Introduction}
\label{sec:Intro}

A linear mixed model for a $n$-dimensional response vector $\bm{y}$ can
be written as
\begin{equation}
  \label{eq:lmm}
  \bm{y} = \bm{X}\bm{\beta} + \bm{Z}\bm{b}+\bm{\epsilon},\quad
  \bm{\epsilon}\sim\mathcal{N},\left(\bm{0},\sigma^2\bm{I}\right),\;
  \bm{b}\sim\mathcal{N}\left(\bm{0},\sigma^2\bm{\Sigma}\right)\;\mathrm{and}\;
  \bm{b}\perp\bm{\epsilon}
\end{equation}
where $\bm{\beta}$ is a $p$-dimensional fixed-effects parameter vector
relative to the $n\times p$ model matrix $\bm{X}$, $\bm{b}$ is a
$q$-dimensional random-effects vector relative to the $n\times q$
model matrix $\bm{Z}$ and $\bm{\Sigma}$ is the $q\times q$ relative
variance-covariance matrix for the random-effects.  The symbol $\perp$
indicates independence of random variables and $\mathcal{N}$ denotes
the multivariate normal (Gaussian) distribution.

We say that matrix $\bm{\Sigma}$ is the relative variance-covariance
matrix of the random effects in the sense that it is the variance of
$\bm{b}$ relative to $\sigma^2$, the scalar variance of the
per-observation noise term $\bm{\epsilon}$.  Although it size, $q$,
can be very large, $\bm{\Sigma}$ is highly structured.  It is
symmetric, positive semi-definite and zero except for the diagonal
elements and certain elements close to the diagonal.

\section{Structure of $\bm{\Sigma}$ and $\bm{Z}$}
\label{sec:role-group-fact}

The columns of $\bm{Z}$ and the rows and columns of $\bm{\Sigma}$ are
associated with the levels of one or more grouping factors in the
data.  For example, a common application of linear mixed models is the
analysis of students' scores on the annual state-wide performance
tests mandated by the No Child Left Behind Act.  A given score is
associated with a student, a teacher, a school and a school district.
These could all be grouping factors in a model.

We write the grouping factors as $\bm{f}_i,i=1,\dots k$.  The number
of levels of the $i$th factor, $\bm{f}_i$, is $n_i$ and the number of
random effects associated with each level is $q_i$.  For example, if
$\bm{f}_1$ is ``student'' then $n_1$ is the number of students in the
study.  If we have a simple additive random effect for each student
then $q_1=1$.  If we have a random effect for both the intercept and
the slope with respect to time for each student then $q_1=2$.  The
$q_i,i=1,\dots,k$ are typically very small whereas the
$n_i,i=1,\dots,k$ can be very large.

In the statistical model we assume that random effects associated with
different grouping factors are independent, which implies that
$\bm{\Sigma}$ is block diagonal with $k$ diagonal blocks of sizes $n_i
q_i\times n_i q_i, i = 1,\dots,k$.  That is
\begin{equation}
  \label{eq:blockDiag}
  \bm{\Sigma}=
  \begin{bmatrix}
    \bm{\Sigma}_1 & \bm{0} & \hdots & \bm{0}\\
    \bm{0} & \bm{\Sigma}_2 & \hdots & \bm{0}\\
    \vdots & \vdots        & \ddots & \vdots \\
    \bm{0} & \bm{0}        & \hdots & \bm{\Sigma}_k
  \end{bmatrix}
\end{equation}
Furthermore, random effects associated
with different levels of the same grouping factor are assumed to be
independent and identically distributed, which implies that
$\bm{\Sigma}_i$ is itself block diagonal in $n_i$ blocks and that each
of these blocks is a copy of a $q_i\times q_i$ matrix
$\tilde{\bm{\Sigma}}_i$.  That is \begin{equation}
  \label{eq:blockblock}
  \bm{\Sigma}_i=
    \begin{bmatrix}
    \tilde{\bm{\Sigma}}_i & \bm{0}     & \hdots & \bm{0}\\
    \bm{0}     & \tilde{\bm{\Sigma}}_i  & \hdots & \bm{0}\\
    \vdots    & \vdots    & \ddots & \vdots\\
    \bm{0}     & \bm{0}     & \hdots & \tilde{\bm{\Sigma}}_i
  \end{bmatrix}=
  \bm{I}_{n_i}\otimes\tilde{\bm{\Sigma}}_i\quad i=1,\dots,k
\end{equation}
where $\otimes$ denotes the Kronecker product.

The condition that $\bm{\Sigma}$ is positive semi-definite holds if
and only if the $\tilde{\bm{\Sigma}}_i,i=1,\dots,k$  are positive
semi-definite, in which case they can be expressed in the LDL form of
the Cholesky decomposition as
\begin{equation}
  \label{eq:LDLinner}
  \tilde{\bm{\Sigma}}_i=\tilde{\bm{L}}_i\tilde{\bm{D}}_i\tilde{\bm{L}}_i\trans
\end{equation}
where $\tilde{\bm{L}}_i$ is a $q_i\times q_i$ unit lower-triangular
matrix (i.e. all the elements above the diagonal are zero and all the
elements on the diagonal are one) and $\tilde{\bm{D}}_i$ is a
$q_i\times q_i$ diagonal matrix with non-negative elements on the
diagonal. 

The parameter vector $\bm{\theta}_i,i=1,\dots,k$ consists of the $q_i$
diagonal elements of $\tilde{\bm{D}}_i$, which are constrained to be
non-negative, followed by the $q_i(q_i-1)/2$ elements in the strict
lower triangle of $\tilde{\bm{L}}_i$ (in column-major ordering).
These last $q_i(q_i-1)/2$ elements are unconstrained.  The
$\bm{\theta}_i$ are combined as
\begin{displaymath}
  \bm{\theta}=
  \begin{bmatrix}
    \bm{\theta}_1 \\ \bm{\theta}_2 \\ \vdots \\ \bm{\theta}_k
  \end{bmatrix} .
\end{displaymath}
Each of the $q\times q$ matrices $\bm{L}$, $\bm{D}$ and $\bm{\Sigma}$
in the decomposition $\bm{\Sigma}=\bm{L}\bm{D}\bm{L}\trans$ is a
function of $\bm{\theta}$.  

The constraints on $\bm{\theta}$ imply that all the diagonal elements of
$\bm{D}$ are non-negative and we can define $\bm{S}$ to be the
$q\times q$ diagonal matrix whose $j$th diagonal element is the square
root of the $j$th diagonal element of $\bm{D}$, $j=1,\dots,k$.
Trivially, $\bm{D}=\bm{S}\bm{S}$.

As a unit triangular matrix $\bm{L}$ is non-singular.  That is,
$\bm{L}^{-1}$ exists and is easily calculated from the
$\tilde{\bm{L}}_i^{-1},i=1,\dots,k$.  When $\bm{\theta}$ is not on the
boundary defined by the constraints, both $\bm{D}$ and $\bm{S}$ are
diagonal matrices with strictly positive elements on the diagonal,
which implies that $\bm{D}^{-1}$ and $\bm{S}^{-1}$ exist and that
$\bm{\Sigma}$ is non-singular with
$\bm{\Sigma}=\bm{L}\invtrans\bm{D}^{-1}\bm{L}^{-1}$.

When $\bm{\theta}$ is on the boundary the matrices $\bm{D}$, $\bm{S}$
and $\bm{\Sigma}$ exist but none are invertible.  We say that
$\bm{\Sigma}$ is a \emph{degenerate} variance-covariance matrix in the
sense that one or more linear combinations of the vector $\bm{b}$ are
defined to have zero variance.  That is, the distribution of these
linear combinations is a point mass at 0.

The maximum likelihood estimates of $\bm{\theta}$ (or the restricted
maximum likelihood estimates, defined below) can be on the
boundary.  That is, they can correspond to a degenerate
variance-covariance matrix and we must be careful to allow for this
case.  However, to begin we consider the non-degenerate case.


\section{Methods for non-singular $\bm{\Sigma}$}
\label{sec:non-singular}

When $\bm{\theta}$ is not on the boundary we can define the
standardized random effects vector 
\begin{equation}
  \label{eq:bstar}
  \bm{b}^*=\bm{S}^{-1}\bm{L}^{-1}\bm{b}
\end{equation}
with the property
\begin{equation}
  \label{eq:bstardist}
  \bm{b}^*\sim\mathcal{N}\left(\bm{0},\sigma^2\bm{I}\right)
\end{equation}
That is, the unconditional distribution of the $q$ elements of
$\bm{b}^*$ is i.i.d. (independent and identically distributed) normal
with mean $0$ and variance $\sigma^2$, like that of the $n$
elements of $\bm{\epsilon}$.  

Obviously the transformation from $\bm{b}^*$ to $\bm{b}$ is 
\begin{equation}
  \label{eq:bstarb}
  \bm{b}=\bm{L}\bm{S}\bm{b}^*
\end{equation}
and the $n\times q$ model matrix for $\bm{b}^*$ is 
\begin{equation}
  \label{eq:Zstar}
  \bm{Z}^*=\bm{Z}\bm{S}\bm{L} .
\end{equation}
Notice that $\bm{Z}^*$ in (\ref{eq:Zstar}) can be evaluated when
$\bm{\theta}$ is on the boundary and that, if we have a value of
$\bm{b}^*$ in such a case, we can evaluated $\bm{b}$ from
(\ref{eq:bstarb}).
 
Given the data $\bm{y}$ and values of $\bm{\theta}$ and $\bm{\beta}$,
the mode (which is also the mean) of the conditional distribution of
$\bm{b}^*$ is the solution to a penalized least squares problem
\begin{equation}
  \label{eq:bstarmode}
  \begin{split}
    \tilde{\bm{b}^*}(\bm{\theta},\bm{\beta},\bm{y})&=
    \arg\min_{\bm{b}^*} \left[
    \left\|\bm{y}-\bm{X}\bm{\beta}-\bm{Z}^*\bm{b}^*\right\|^2
    +{\bm{b}^*}\trans\bm{b}^*\right]\\
    &=
    \arg\min_{\bm{b}^*}
    \left\|
      \begin{bmatrix}
        \bm{y}\\
        \bm{0}
      \end{bmatrix} -
      \begin{bmatrix}
        \bm{Z}^* & \bm{X}\\
        \bm{I}   & \bm{0}
      \end{bmatrix}
      \begin{bmatrix}
        \bm{b}^*\\
        \bm{\beta}
      \end{bmatrix}
      \right\|^2
  \end{split}
\end{equation}
In fact, if we optimize penalized least squares expression in
(\ref{eq:bstarmode}) with respect to both $\bm{b}$ and $\bm{\beta}$ we
obtain the conditional estimates
$\hat{\bm{\beta}}(\bm{\theta},\bm{y})$ and the conditional modes 
$\tilde{\bm{b}^*}(\bm{\theta},\hat{\bm{\beta}}(\bm{\theta}),\bm{y}))$
which we write as $\widehat{\bm{b}^*}(\bm{\theta})$.  That is,
\begin{equation}
  \label{eq:bstarbeta}
  \begin{split}
    \begin{bmatrix}
      \widehat{\bm{b}^*}(\bm{\theta})\\
      \hat{\bm{\beta}}(\bm{\theta})
    \end{bmatrix} & =
    \arg\min_{\bm{b}^*,\bm{\beta}}
    \left\|
      \begin{bmatrix}
        \bm{Z}^* & \bm{X} & -\bm{y}\\
        \bm{I}   & \bm{0} &  \bm{0}
      \end{bmatrix}
      \begin{bmatrix}
        \bm{b}^*\\
        \bm{\beta}\\
        1
      \end{bmatrix}
    \right\|^2\\
     & =
    \arg\min_{\bm{b}^*,\bm{\beta}}
    \begin{bmatrix}
      \bm{b}^*\\
      \bm{\beta}\\
      1
    \end{bmatrix}\trans
    \bm{A}^*(\bm{\theta})
    \begin{bmatrix}
      \bm{b}^*\\
      \bm{\beta}\\
      1
    \end{bmatrix}
  \end{split}
\end{equation}
where the matrix $\bm{A}^*(\bm{\theta})$ is
\begin{equation}
  \label{eq:Astar}
  \begin{split}
    \bm{A}^*(\bm{\theta})&=
    \begin{bmatrix}
      {\bm{Z}^*}\trans\bm{Z}^*+\bm{I} & {\bm{Z}^*}\trans\bm{X} &
      -{\bm{Z}^*}\trans\bm{y}\\
      {\bm{X}}\trans\bm{Z}^* & {\bm{X}}\trans\bm{X} &
      -{\bm{X}}\trans\bm{y}\\
      -{\bm{y}}\trans\bm{Z}^*&-{\bm{y}}\trans\bm{X}&{\bm{y}}\trans\bm{y}
    \end{bmatrix} \\
    &=
    \begin{bmatrix}
      \bm{L}\trans\bm{S} & \bm{0} & \bm{0} \\
      \bm{0} & \bm{I} & \bm{0} \\
      \bm{0} & \bm{0} & 1
    \end{bmatrix}
    \bm{A}
    \begin{bmatrix}
      \bm{S}\bm{L} & \bm{0} & \bm{0} \\
      \bm{0} & \bm{I} & \bm{0} \\
      \bm{0} & \bm{0} & 1
    \end{bmatrix}+
    \begin{bmatrix}
      \bm{I} & \bm{0} & \bm{0} \\
      \bm{0} & \bm{0} & \bm{0} \\
      \bm{0} & \bm{0} & 0
    \end{bmatrix} .
  \end{split}
\end{equation}
and
\begin{equation}
  \label{eq:Amatrix}
  \bm{A}=
  \begin{bmatrix}
    \bm{Z}\trans\bm{Z} & \bm{Z}\trans\bm{X} & -\bm{Z}\trans\bm{y} \\
    \bm{X}\trans\bm{Z} & \bm{X}\trans\bm{X} & -\bm{X}\trans\bm{y} \\    
    -\bm{y}\trans\bm{Z} & -\bm{y}\trans\bm{X} & \bm{y}\trans\bm{y} \\    
  \end{bmatrix} .
\end{equation}
Note that $\bm{A}$ does not depend upon $\bm{\theta}$.  Furthermore,
the nature of the model matrices $\bm{Z}$ and $\bm{X}$ ensures that
the pattern of nonzeros in $\bm{A}^*(\bm{\theta})$ is the same as that
in $\bm{A}$.

Let the $q\times q$ permutation matrix $\bm{P}_Z$ represent a
fill-reducing permutation for $\bm{Z}\trans\bm{Z}$ and $\bm{P}_X$, of
size $p\times p$, represent a fill-reducing permutation for
$\bm{X}\trans\bm{X}$.  These could be determined, for example, using
the \emph{approximate minimal degree} (AMD) algorithm described in
\citet{davis06:csparse_book} and \citet{Davis:1996} and implemented in
both the \texttt{\small Csparse} \citep{Csparse} and the
\texttt{\small CHOLMOD} \citep{Cholmod} libraries of C functions.  (In
many cases $\bm{X}\trans\bm{X}$ is dense (but of small dimension
compared to $\bm{Z}\trans\bm{Z}$) and $\bm{Z}\trans\bm{X}$ is nearly
dense so $\bm{P}_X$ can be $\bm{I}_p$, the $p\times p$ identity
matrix.)

Let the permutation matrix $\bm{P}$ be
\begin{equation}
  \label{eq:fillPerm}
  \bm{P}=
  \begin{bmatrix}
    \bm{P}_Z & \bm{0} & \bm{0} \\
    \bm{0} & \bm{P}_X & \bm{0} \\
    \bm{0} & \bm{0} & 1
  \end{bmatrix} 
\end{equation}
and $\bm{K}(\bm{\theta})$ be the sparse Cholesky decomposition of
$\bm{A}^*(\bm{\theta})$ relative to this permutation.  That
is, $\bm{K}(\bm{\theta})$ is a sparse lower triangular matrix with the
property that
\begin{equation}
  \label{eq:Kmat}
  \bm{K}(\bm{\theta})\bm{K}(\bm{\theta})\trans=
  \bm{P}\bm{A}^*(\bm{\theta})\bm{P}\trans
\end{equation}

For $\bm{K}(\bm{\theta})$ to exist we must ensure that
$\bm{A}^*(\bm{\theta})$ is positive definite.  This will be true if
$\bm{X}$ is of full column rank and $\bm{y}$ does not lie in the
column span of $\bm{X}$ (or, in statistical terms, if we can't fit
$\bm{y}$ perfectly using only the fixed effects).

Let $r>0$ be the last element on the diagonal of $\bm{K}$.  Then the
minumum penalized residual sum of squares in (\ref{eq:bstarbeta}) is
$r^2$ and it occurs at $\widehat{\bm{b}^*}(\bm{\theta})$ and
$\hat{\beta}(\bm{\theta})$, the solutions to the sparse triangular system
\begin{equation}
  \label{eq:soln}
  \bm{K}(\bm{\theta})\trans\bm{P}
  \begin{bmatrix}
    \widehat{\bm{b}^*}(\bm{\theta})\\
    \hat{\beta}(\bm{\theta})\\
    1
  \end{bmatrix}=
  \begin{bmatrix}
    \bm{0}\\
    \bm{0}\\
    r
  \end{bmatrix}
\end{equation}
(Technically we should not write the $1$ in the solution; it should be
an unknown.  However, for $\bm{K}$ lower triangular with $r$ as the last
element on the diagonal and $\bm{P}$ a permutation that does not move
the last row, the solution for this ``unknown'' will always be $1$.)
Furthermore, $\log|{\bm{Z}^*}\trans\bm{Z}+\bm{I}|$ can be evaluated as
the sum of the logarithms of the first $q$ diagonal elements of
$\bm{K}(\bm{\theta})$.  

The maximum likelihood estimates (mle's) of $\bm{\theta}$ are the
values that minimize the profiled deviance function
\begin{equation}
  \label{eq:thetaMLE}
  \hat{\bm{\theta}}=\arg\min_{\bm{\theta}}\left[
    \log|{\bm{Z}^*}\trans\bm{Z}+\bm{I}|+
    n\left(1+\log\frac{2\pi r^2}{n}\right)\right]
\end{equation}

\bibliography{lme4}
\end{document}
